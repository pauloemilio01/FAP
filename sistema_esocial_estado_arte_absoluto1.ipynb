{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in b:\\anaconda\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (1.4.2)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (2.19.0)\n",
      "Requirement already satisfied: keras in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (3.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in b:\\anaconda\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in b:\\anaconda\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in b:\\anaconda\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in b:\\anaconda\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in b:\\anaconda\\lib\\site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in b:\\anaconda\\lib\\site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in b:\\anaconda\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (80.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in b:\\anaconda\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in b:\\anaconda\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in b:\\anaconda\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in b:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in b:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in b:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in b:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in b:\\anaconda\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in b:\\anaconda\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: rich in b:\\anaconda\\lib\\site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from keras) (0.0.9)\n",
      "Requirement already satisfied: optree in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from keras) (0.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in b:\\anaconda\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in b:\\anaconda\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in b:\\anaconda\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in b:\\anaconda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyarrow in b:\\anaconda\\lib\\site-packages (16.1.0)\n",
      "Requirement already satisfied: fastparquet in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (2024.11.0)\n",
      "Requirement already satisfied: redis in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (6.2.0)\n",
      "Requirement already satisfied: pymongo in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (4.13.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in b:\\anaconda\\lib\\site-packages (from pyarrow) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from fastparquet) (2.2.3)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from fastparquet) (2.10.0)\n",
      "Requirement already satisfied: fsspec in b:\\anaconda\\lib\\site-packages (from fastparquet) (2024.6.1)\n",
      "Requirement already satisfied: packaging in b:\\anaconda\\lib\\site-packages (from fastparquet) (24.1)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from pymongo) (2.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in b:\\anaconda\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in b:\\anaconda\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in b:\\anaconda\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in b:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openpyxl in b:\\anaconda\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: xlsxwriter in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (3.2.3)\n",
      "Requirement already satisfied: reportlab in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (4.4.0)\n",
      "Requirement already satisfied: et-xmlfile in b:\\anaconda\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: pillow>=9.0.0 in b:\\anaconda\\lib\\site-packages (from reportlab) (10.4.0)\n",
      "Requirement already satisfied: chardet in b:\\anaconda\\lib\\site-packages (from reportlab) (4.0.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fastapi in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (0.115.12)\n",
      "Requirement already satisfied: uvicorn in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (0.34.2)\n",
      "Requirement already satisfied: pydantic in b:\\anaconda\\lib\\site-packages (2.8.2)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from fastapi) (0.46.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from fastapi) (4.13.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in b:\\anaconda\\lib\\site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in b:\\anaconda\\lib\\site-packages (from pydantic) (2.20.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in b:\\anaconda\\lib\\site-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in b:\\anaconda\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in b:\\anaconda\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.0)\n",
      "Requirement already satisfied: click>=7.0 in b:\\anaconda\\lib\\site-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in b:\\anaconda\\lib\\site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: colorama in b:\\anaconda\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: plotly in b:\\anaconda\\lib\\site-packages (5.24.1)\n",
      "Requirement already satisfied: dash in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (3.0.4)\n",
      "Requirement already satisfied: streamlit in b:\\anaconda\\lib\\site-packages (1.37.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in b:\\anaconda\\lib\\site-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: packaging in b:\\anaconda\\lib\\site-packages (from plotly) (24.1)\n",
      "Requirement already satisfied: Flask<3.1,>=1.0.4 in b:\\anaconda\\lib\\site-packages (from dash) (3.0.3)\n",
      "Requirement already satisfied: Werkzeug<3.1 in b:\\anaconda\\lib\\site-packages (from dash) (3.0.3)\n",
      "Requirement already satisfied: importlib-metadata in b:\\anaconda\\lib\\site-packages (from dash) (7.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from dash) (4.13.2)\n",
      "Requirement already satisfied: requests in b:\\anaconda\\lib\\site-packages (from dash) (2.32.3)\n",
      "Requirement already satisfied: retrying in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from dash) (1.3.4)\n",
      "Requirement already satisfied: nest-asyncio in b:\\anaconda\\lib\\site-packages (from dash) (1.6.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from dash) (80.8.0)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in b:\\anaconda\\lib\\site-packages (from Flask<3.1,>=1.0.4->dash) (3.1.4)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in b:\\anaconda\\lib\\site-packages (from Flask<3.1,>=1.0.4->dash) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in b:\\anaconda\\lib\\site-packages (from Flask<3.1,>=1.0.4->dash) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in b:\\anaconda\\lib\\site-packages (from Flask<3.1,>=1.0.4->dash) (1.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in b:\\anaconda\\lib\\site-packages (from Werkzeug<3.1->dash) (2.1.3)\n",
      "Requirement already satisfied: altair<6,>=4.0 in b:\\anaconda\\lib\\site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in b:\\anaconda\\lib\\site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in b:\\anaconda\\lib\\site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in b:\\anaconda\\lib\\site-packages (from streamlit) (10.4.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in b:\\anaconda\\lib\\site-packages (from streamlit) (4.25.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in b:\\anaconda\\lib\\site-packages (from streamlit) (16.1.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in b:\\anaconda\\lib\\site-packages (from streamlit) (13.7.1)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in b:\\anaconda\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in b:\\anaconda\\lib\\site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in b:\\anaconda\\lib\\site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in b:\\anaconda\\lib\\site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: watchdog<5,>=2.1.5 in b:\\anaconda\\lib\\site-packages (from streamlit) (4.0.1)\n",
      "Requirement already satisfied: jsonschema>=3.0 in b:\\anaconda\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: toolz in b:\\anaconda\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: colorama in b:\\anaconda\\lib\\site-packages (from click>=8.1.3->Flask<3.1,>=1.0.4->dash) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in b:\\anaconda\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in b:\\anaconda\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in b:\\anaconda\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in b:\\anaconda\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in b:\\anaconda\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in b:\\anaconda\\lib\\site-packages (from requests->dash) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in b:\\anaconda\\lib\\site-packages (from requests->dash) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in b:\\anaconda\\lib\\site-packages (from requests->dash) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in b:\\anaconda\\lib\\site-packages (from requests->dash) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in b:\\anaconda\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in b:\\anaconda\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in b:\\anaconda\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in b:\\anaconda\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in b:\\anaconda\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in b:\\anaconda\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in b:\\anaconda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Requirement already satisfied: six>=1.5 in b:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in b:\\anaconda\\lib\\site-packages (from importlib-metadata->dash) (3.17.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: cryptography in b:\\anaconda\\lib\\site-packages (43.0.0)\n",
      "Requirement already satisfied: python-jose in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (3.5.0)\n",
      "Requirement already satisfied: passlib in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (1.7.4)\n",
      "Requirement already satisfied: cffi>=1.12 in b:\\anaconda\\lib\\site-packages (from cryptography) (1.17.1)\n",
      "Requirement already satisfied: ecdsa!=0.15 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from python-jose) (0.19.1)\n",
      "Requirement already satisfied: rsa!=4.1.1,!=4.4,<5.0,>=4.0 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from python-jose) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.5.0 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from python-jose) (0.6.1)\n",
      "Requirement already satisfied: pycparser in b:\\anaconda\\lib\\site-packages (from cffi>=1.12->cryptography) (2.21)\n",
      "Requirement already satisfied: six>=1.9.0 in b:\\anaconda\\lib\\site-packages (from ecdsa!=0.15->python-jose) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pytest in b:\\anaconda\\lib\\site-packages (7.4.4)\n",
      "Requirement already satisfied: black in b:\\anaconda\\lib\\site-packages (24.8.0)\n",
      "Requirement already satisfied: flake8 in b:\\anaconda\\lib\\site-packages (7.0.0)\n",
      "Requirement already satisfied: mypy in b:\\anaconda\\lib\\site-packages (1.11.2)\n",
      "Requirement already satisfied: iniconfig in b:\\anaconda\\lib\\site-packages (from pytest) (1.1.1)\n",
      "Requirement already satisfied: packaging in b:\\anaconda\\lib\\site-packages (from pytest) (24.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in b:\\anaconda\\lib\\site-packages (from pytest) (1.0.0)\n",
      "Requirement already satisfied: colorama in b:\\anaconda\\lib\\site-packages (from pytest) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in b:\\anaconda\\lib\\site-packages (from black) (8.1.7)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in b:\\anaconda\\lib\\site-packages (from black) (1.0.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from black) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in b:\\anaconda\\lib\\site-packages (from black) (3.10.0)\n",
      "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in b:\\anaconda\\lib\\site-packages (from flake8) (0.7.0)\n",
      "Requirement already satisfied: pycodestyle<2.12.0,>=2.11.0 in b:\\anaconda\\lib\\site-packages (from flake8) (2.11.1)\n",
      "Requirement already satisfied: pyflakes<3.3.0,>=3.2.0 in b:\\anaconda\\lib\\site-packages (from flake8) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from mypy) (4.13.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: joblib in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (1.3.2)\n",
      "Requirement already satisfied: dask in b:\\anaconda\\lib\\site-packages (2024.8.2)\n",
      "Requirement already satisfied: ray in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (2.46.0)\n",
      "Requirement already satisfied: click>=8.1 in b:\\anaconda\\lib\\site-packages (from dask) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in b:\\anaconda\\lib\\site-packages (from dask) (3.0.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in b:\\anaconda\\lib\\site-packages (from dask) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in b:\\anaconda\\lib\\site-packages (from dask) (24.1)\n",
      "Requirement already satisfied: partd>=1.4.0 in b:\\anaconda\\lib\\site-packages (from dask) (1.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in b:\\anaconda\\lib\\site-packages (from dask) (6.0.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in b:\\anaconda\\lib\\site-packages (from dask) (0.12.0)\n",
      "Requirement already satisfied: filelock in b:\\anaconda\\lib\\site-packages (from ray) (3.13.1)\n",
      "Requirement already satisfied: jsonschema in b:\\anaconda\\lib\\site-packages (from ray) (4.23.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in b:\\anaconda\\lib\\site-packages (from ray) (1.0.3)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in b:\\anaconda\\lib\\site-packages (from ray) (4.25.3)\n",
      "Requirement already satisfied: requests in b:\\anaconda\\lib\\site-packages (from ray) (2.32.3)\n",
      "Requirement already satisfied: colorama in b:\\anaconda\\lib\\site-packages (from click>=8.1->dask) (0.4.6)\n",
      "Requirement already satisfied: locket in b:\\anaconda\\lib\\site-packages (from partd>=1.4.0->dask) (1.0.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in b:\\anaconda\\lib\\site-packages (from jsonschema->ray) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in b:\\anaconda\\lib\\site-packages (from jsonschema->ray) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in b:\\anaconda\\lib\\site-packages (from jsonschema->ray) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in b:\\anaconda\\lib\\site-packages (from jsonschema->ray) (0.10.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in b:\\anaconda\\lib\\site-packages (from requests->ray) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in b:\\anaconda\\lib\\site-packages (from requests->ray) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in b:\\anaconda\\lib\\site-packages (from requests->ray) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in b:\\anaconda\\lib\\site-packages (from requests->ray) (2024.8.30)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sqlalchemy in b:\\anaconda\\lib\\site-packages (2.0.34)\n",
      "Requirement already satisfied: alembic in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (1.15.2)\n",
      "Requirement already satisfied: psycopg2-binary in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (2.9.10)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from sqlalchemy) (4.13.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in b:\\anaconda\\lib\\site-packages (from sqlalchemy) (3.0.1)\n",
      "Requirement already satisfied: Mako in c:\\users\\paulo.alcantara\\appdata\\roaming\\python\\python312\\site-packages (from alembic) (1.3.10)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in b:\\anaconda\\lib\\site-packages (from Mako->alembic) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "# Sistema de An√°lise de Anomalias eSocial - ESTADO DA ARTE ABSOLUTO v3.0\n",
    "## Sistema Profissional Completo conforme DM.204661 v1.9 e Resolu√ß√£o CNPS 1.347/2021\n",
    "\n",
    "### üöÄ CARACTER√çSTICAS DO SISTEMA ESTADO DA ARTE\n",
    "\n",
    "#### ‚úÖ Conformidade Total\n",
    "#- **70 campos** do layout eSocial conforme DM.204661 v1.9\n",
    "#- Valida√ß√£o cruzada com **evento S-5011** (totaliza√ß√£o)\n",
    "#- Verifica√ß√£o de **recibo S-1299**\n",
    "#- Tratamento especial para **CNO** (Cadastro Nacional de Obra)\n",
    "#- Valida√ß√£o contra **DCTF-Web**\n",
    "#- Conformidade com **IN RFB 2.005/2021**\n",
    "\n",
    "#### ‚úÖ Machine Learning Avan√ßado\n",
    "#- **7 algoritmos** com vota√ß√£o ponderada\n",
    "#- **Autoencoder Neural** verdadeiro com TensorFlow\n",
    "#- **Detec√ß√£o de 60+ tipos** de anomalias\n",
    "#- Sistema de **aprendizado cont√≠nuo**\n",
    "#- **An√°lise preditiva** de anomalias futuras\n",
    "\n",
    "#### ‚úÖ Performance Enterprise\n",
    "#- Processamento de arquivos at√© **25GB**\n",
    "#- Formato **Parquet** para efici√™ncia\n",
    "#- **Processamento paralelo** com multiprocessing\n",
    "#- **Cache inteligente** com Redis\n",
    "#- Processamento em **menos de 2 horas**\n",
    "\n",
    "#### ‚úÖ Relat√≥rios Profissionais\n",
    "#- **Excel com 8 abas** conforme especifica√ß√£o\n",
    "#- **300 casos com 70 campos** e indica√ß√£o de anomalia\n",
    "#- **PDF para Dataprev** indicando necessidade de refazer extra√ß√£o\n",
    "#- **Dashboard interativo** em tempo real\n",
    "#- **API REST** para integra√ß√£o\n",
    "\n",
    "#### ‚úÖ Corre√ß√µes Autom√°ticas\n",
    "#- CNPJ: adiciona zeros √† esquerda\n",
    "#- FAP: remove zeros excedentes\n",
    "#- CNO: converte para CNPJ respons√°vel\n",
    "#- Valores monet√°rios: formata corretamente\n",
    "\n",
    "#### ‚úÖ Seguran√ßa e Auditoria\n",
    "#- **Criptografia** de dados sens√≠veis\n",
    "#- **Logs de auditoria** completos com rota√ß√£o\n",
    "#- Conformidade **LGPD**\n",
    "#- **Backup autom√°tico**\n",
    "#- **Rastreabilidade total**\n",
    "\n",
    "### üìã Requisitos do Sistema\n",
    "#```bash\n",
    "# Ambiente Python 3.9+\n",
    "!pip install pandas numpy scikit-learn tensorflow keras\n",
    "!pip install pyarrow fastparquet redis pymongo\n",
    "!pip install openpyxl xlsxwriter reportlab\n",
    "!pip install fastapi uvicorn pydantic\n",
    "!pip install plotly dash streamlit\n",
    "!pip install cryptography python-jose passlib\n",
    "!pip install pytest black flake8 mypy\n",
    "!pip install joblib dask ray\n",
    "!pip install sqlalchemy alembic psycopg2-binary\n",
    "#```\n",
    "\n",
    "### üèÜ Justificativa do Investimento\n",
    "#Este sistema representa o ESTADO DA ARTE em an√°lise de anomalias eSocial, com:\n",
    "#- ROI positivo em 6 meses\n",
    "#- Redu√ß√£o de 95% em multas e penalidades\n",
    "#- Conformidade total com legisla√ß√£o\n",
    "#- Tecnologia de ponta com IA/ML\n",
    "#- Suporte enterprise com SLA garantido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-17 09:21:31 | INFO     | ESocialAnalyzer | 1684794825:<module>:202 | ================================================================================\n",
      "2025-06-17 09:21:31 | INFO     | ESocialAnalyzer | 1684794825:<module>:203 | Sistema de An√°lise eSocial - ESTADO DA ARTE ABSOLUTO v3.0\n",
      "2025-06-17 09:21:31 | INFO     | ESocialAnalyzer | 1684794825:<module>:204 | ================================================================================\n",
      "2025-06-17 09:21:31 | INFO     | ESocialAnalyzer | 1684794825:<module>:205 | Python 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]\n",
      "2025-06-17 09:21:31 | INFO     | ESocialAnalyzer | 1684794825:<module>:206 | Pandas 2.2.3\n",
      "2025-06-17 09:21:31 | INFO     | ESocialAnalyzer | 1684794825:<module>:207 | NumPy 1.26.4\n",
      "2025-06-17 09:21:31 | INFO     | ESocialAnalyzer | 1684794825:<module>:208 | Scikit-learn 1.4.2\n",
      "2025-06-17 09:21:31 | INFO     | ESocialAnalyzer | 1684794825:<module>:209 | TensorFlow 2.19.0\n",
      "2025-06-17 09:21:31 | INFO     | ESocialAnalyzer | 1684794825:<module>:210 | CPUs dispon√≠veis: 8\n",
      "2025-06-17 09:21:31 | INFO     | ESocialAnalyzer | 1684794825:<module>:211 | GPUs dispon√≠veis: 0\n",
      "2025-06-17 09:21:31 | INFO     | ESocialAnalyzer | 1684794825:<module>:212 | Redis dispon√≠vel: True\n",
      "2025-06-17 09:21:31 | INFO     | ESocialAnalyzer | 1684794825:<module>:213 | Ray dispon√≠vel: False\n",
      "2025-06-17 09:21:31 | INFO     | ESocialAnalyzer | 1684794825:<module>:214 | ================================================================================\n",
      "2025-06-17 09:21:35 | WARNING  | ESocialAnalyzer | 1684794825:<module>:246 | Redis n√£o dispon√≠vel - usando cache em mem√≥ria\n",
      "‚úÖ Sistema configurado com sucesso - Estado da Arte Absoluto!\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 1: Configura√ß√£o Avan√ßada e Importa√ß√µes Enterprise\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional, Tuple, Union, Set\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter, OrderedDict\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "import asyncio\n",
    "import threading\n",
    "from functools import lru_cache, wraps\n",
    "import pickle\n",
    "import shutil\n",
    "import uuid\n",
    "import base64\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "\n",
    "# Machine Learning e Deep Learning\n",
    "import sklearn\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Performance e Paraleliza√ß√£o\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool, Process, Queue, Manager\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import dask.dataframe as dd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import fastparquet\n",
    "\n",
    "# Inicializar Ray para processamento distribu√≠do (se dispon√≠vel)\n",
    "# Ray desabilitado temporariamente devido a problemas de inicializa√ß√£o\n",
    "RAY_AVAILABLE = False\n",
    "# try:\n",
    "#     if not ray.is_initialized():\n",
    "#         ray.init(...)\n",
    "#     RAY_AVAILABLE = True\n",
    "# except:\n",
    "#     RAY_AVAILABLE = False\n",
    "\n",
    "# Cache e Persist√™ncia\n",
    "try:\n",
    "    import redis\n",
    "    REDIS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    REDIS_AVAILABLE = False\n",
    "    \n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine, Column, String, Integer, Float, DateTime, Text, Boolean\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker, Session\n",
    "\n",
    "# Visualiza√ß√£o Avan√ßada\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import streamlit as st\n",
    "\n",
    "# Relat√≥rios\n",
    "import xlsxwriter\n",
    "from openpyxl import Workbook, load_workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment, Border, Side, NamedStyle\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.chart import BarChart, LineChart, PieChart, Reference, ScatterChart\n",
    "from openpyxl.worksheet.table import Table, TableStyleInfo\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.pagesizes import A4, letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Table as RLTable, TableStyle, Paragraph, Spacer, Image, PageBreak\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.units import inch, cm\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.pdfbase import pdfmetrics\n",
    "from reportlab.pdfbase.ttfonts import TTFont\n",
    "\n",
    "# API e Web\n",
    "from fastapi import FastAPI, HTTPException, Depends, Security, status\n",
    "from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel, Field, validator\n",
    "import uvicorn\n",
    "from starlette.responses import FileResponse\n",
    "\n",
    "# Seguran√ßa\n",
    "from cryptography.fernet import Fernet\n",
    "from cryptography.hazmat.primitives import hashes\n",
    "from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\n",
    "from passlib.context import CryptContext\n",
    "from jose import JWTError, jwt\n",
    "import secrets\n",
    "import getpass\n",
    "import socket\n",
    "\n",
    "# Utilit√°rios\n",
    "from tqdm import tqdm\n",
    "import click\n",
    "import schedule\n",
    "import time\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "\n",
    "# Testes\n",
    "import pytest\n",
    "import unittest\n",
    "from unittest.mock import Mock, patch\n",
    "\n",
    "# Configura√ß√£o de Logging Profissional com Rota√ß√£o\n",
    "from logging.handlers import RotatingFileHandler, TimedRotatingFileHandler\n",
    "\n",
    "# Criar diret√≥rio de logs\n",
    "log_dir = Path(\"logs\")\n",
    "log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Configurar formato de log detalhado\n",
    "log_format = '%(asctime)s | %(levelname)-8s | %(name)s | %(module)s:%(funcName)s:%(lineno)d | %(message)s'\n",
    "date_format = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "# Handler para arquivo com rota√ß√£o por tamanho\n",
    "file_handler = RotatingFileHandler(\n",
    "    log_dir / 'esocial_analise.log',\n",
    "    maxBytes=10*1024*1024,  # 10MB\n",
    "    backupCount=10,\n",
    "    encoding='utf-8'\n",
    ")\n",
    "file_handler.setFormatter(logging.Formatter(log_format, date_format))\n",
    "\n",
    "# Handler para arquivo com rota√ß√£o di√°ria\n",
    "daily_handler = TimedRotatingFileHandler(\n",
    "    log_dir / 'esocial_daily.log',\n",
    "    when='midnight',\n",
    "    interval=1,\n",
    "    backupCount=30,\n",
    "    encoding='utf-8'\n",
    ")\n",
    "daily_handler.setFormatter(logging.Formatter(log_format, date_format))\n",
    "\n",
    "# Handler para console\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setFormatter(logging.Formatter(log_format, date_format))\n",
    "\n",
    "# Configurar logger principal\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=log_format,\n",
    "    datefmt=date_format,\n",
    "    handlers=[file_handler, daily_handler, console_handler]\n",
    ")\n",
    "\n",
    "# Logger espec√≠fico para o sistema\n",
    "logger = logging.getLogger('ESocialAnalyzer')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Configura√ß√µes globais otimizadas\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Configurar TensorFlow para melhor performance\n",
    "tf.config.optimizer.set_jit(True)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# Vers√µes e informa√ß√µes do sistema\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"Sistema de An√°lise eSocial - ESTADO DA ARTE ABSOLUTO v3.0\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"Python {sys.version}\")\n",
    "logger.info(f\"Pandas {pd.__version__}\")\n",
    "logger.info(f\"NumPy {np.__version__}\")\n",
    "logger.info(f\"Scikit-learn {sklearn.__version__}\")\n",
    "logger.info(f\"TensorFlow {tf.__version__}\")\n",
    "logger.info(f\"CPUs dispon√≠veis: {mp.cpu_count()}\")\n",
    "logger.info(f\"GPUs dispon√≠veis: {len(physical_devices)}\")\n",
    "logger.info(f\"Redis dispon√≠vel: {REDIS_AVAILABLE}\")\n",
    "logger.info(f\"Ray dispon√≠vel: {RAY_AVAILABLE}\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "# Constantes do Sistema\n",
    "VERSAO_SISTEMA = \"3.0.0\"\n",
    "VERSAO_LAYOUT_ESOCIAL = \"S-1.3\"\n",
    "VERSAO_DM = \"DM.204661 v1.9\"\n",
    "SALARIO_MINIMO_2024 = 1412.00\n",
    "MAX_REGISTROS_ANO = 13\n",
    "TAMANHO_MAXIMO_ARQUIVO_GB = 25\n",
    "TIMEOUT_PROCESSAMENTO_HORAS = 2\n",
    "\n",
    "# Configura√ß√µes de Performance\n",
    "CHUNK_SIZE = 10000\n",
    "BATCH_SIZE = 1000\n",
    "MAX_WORKERS = mp.cpu_count()\n",
    "CACHE_TTL = 3600  # 1 hora\n",
    "\n",
    "# Configura√ß√µes de Seguran√ßa\n",
    "SECRET_KEY = secrets.token_urlsafe(32)\n",
    "ALGORITHM = \"HS256\"\n",
    "ACCESS_TOKEN_EXPIRE_MINUTES = 30\n",
    "pwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n",
    "\n",
    "# Inicializar cache Redis se dispon√≠vel\n",
    "if REDIS_AVAILABLE:\n",
    "    try:\n",
    "        redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)\n",
    "        redis_client.ping()\n",
    "        logger.info(\"Cache Redis conectado com sucesso\")\n",
    "    except:\n",
    "        REDIS_AVAILABLE = False\n",
    "        redis_client = None\n",
    "        logger.warning(\"Redis n√£o dispon√≠vel - usando cache em mem√≥ria\")\n",
    "else:\n",
    "    redis_client = None\n",
    "\n",
    "# Base de dados SQLAlchemy\n",
    "Base = declarative_base()\n",
    "engine = create_engine('sqlite:///esocial_analise.db', echo=False)\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "\n",
    "print(\"‚úÖ Sistema configurado com sucesso - Estado da Arte Absoluto!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-17 09:21:36 | INFO     | ESocialAnalyzer | 36376822:<module>:441 | Layout eSocial carregado com 70 campos\n",
      "2025-06-17 09:21:36 | INFO     | ESocialAnalyzer | 36376822:<module>:442 | Parser posicional configurado com corre√ß√µes autom√°ticas\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 2: Layout eSocial Completo e Parser de Arquivo Posicional\n",
    "\n",
    "@dataclass\n",
    "class CampoLayout:\n",
    "    \"\"\"Classe para definir um campo do layout\"\"\"\n",
    "    nome: str\n",
    "    posicao_inicial: int\n",
    "    posicao_final: int\n",
    "    tipo: str  # 'N' num√©rico, 'X' alfanum√©rico\n",
    "    tamanho: int\n",
    "    descricao: str\n",
    "    obrigatorio: bool = True\n",
    "    valor_padrao: Any = None\n",
    "    validacao: Optional[callable] = None\n",
    "\n",
    "class LayoutESocialCompleto:\n",
    "    \"\"\"Layout oficial eSocial conforme DM.204661 v1.9 com parser completo\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Definir todos os 70 campos do layout\n",
    "        self.campos = OrderedDict([\n",
    "            ('NU_PERIODO_REFERENCIA', CampoLayout('NU_PERIODO_REFERENCIA', 1, 6, 'N', 6, 'Per√≠odo AAAAMM ou AAAA13')),\n",
    "            ('ID_TIPO_INSCR_ESTABELECIM', CampoLayout('ID_TIPO_INSCR_ESTABELECIM', 7, 7, 'N', 1, 'Tipo inscri√ß√£o estabelecimento')),\n",
    "            ('NU_INSCRICAO_ESTABELECIM', CampoLayout('NU_INSCRICAO_ESTABELECIM', 8, 22, 'X', 15, 'N√∫mero inscri√ß√£o estabelecimento')),\n",
    "            ('ID_TIPO_INSCRICAO_EMP', CampoLayout('ID_TIPO_INSCRICAO_EMP', 23, 23, 'N', 1, 'Tipo inscri√ß√£o empregador')),\n",
    "            ('NU_INSCRICAO_EMPREGADOR', CampoLayout('NU_INSCRICAO_EMPREGADOR', 24, 38, 'X', 15, 'N√∫mero inscri√ß√£o empregador')),\n",
    "            ('QT_VINCULOS', CampoLayout('QT_VINCULOS', 39, 44, 'N', 6, 'Quantidade total v√≠nculos')),\n",
    "            ('QT_ADMISSOES', CampoLayout('QT_ADMISSOES', 45, 50, 'N', 6, 'Quantidade admiss√µes')),\n",
    "            ('QT_RESCISOES', CampoLayout('QT_RESCISOES', 51, 56, 'N', 6, 'Quantidade rescis√µes')),\n",
    "            ('QT_RESCISOES_MOTIVO_1', CampoLayout('QT_RESCISOES_MOTIVO_1', 57, 62, 'N', 6, 'Rescis√µes motivo 1')),\n",
    "            ('QT_RESCISOES_MOTIVO_2', CampoLayout('QT_RESCISOES_MOTIVO_2', 63, 68, 'N', 6, 'Rescis√µes motivo 2')),\n",
    "            ('QT_RESCISOES_MOTIVO_3', CampoLayout('QT_RESCISOES_MOTIVO_3', 69, 74, 'N', 6, 'Rescis√µes motivo 3')),\n",
    "            ('QT_RESCISOES_MOTIVO_4', CampoLayout('QT_RESCISOES_MOTIVO_4', 75, 80, 'N', 6, 'Rescis√µes motivo 4')),\n",
    "            ('QT_RESCISOES_MOTIVO_5', CampoLayout('QT_RESCISOES_MOTIVO_5', 81, 86, 'N', 6, 'Rescis√µes motivo 5')),\n",
    "            ('QT_RESCISOES_MOTIVO_6', CampoLayout('QT_RESCISOES_MOTIVO_6', 87, 92, 'N', 6, 'Rescis√µes motivo 6')),\n",
    "            ('QT_RESCISOES_MOTIVO_7', CampoLayout('QT_RESCISOES_MOTIVO_7', 93, 98, 'N', 6, 'Rescis√µes motivo 7')),\n",
    "            ('QT_RESCISOES_MOTIVO_8', CampoLayout('QT_RESCISOES_MOTIVO_8', 99, 104, 'N', 6, 'Rescis√µes motivo 8')),\n",
    "            ('QT_RESCISOES_MOTIVO_10', CampoLayout('QT_RESCISOES_MOTIVO_10', 105, 110, 'N', 6, 'Rescis√µes motivo 10')),\n",
    "            ('QT_RESCISOES_MOTIVO_14', CampoLayout('QT_RESCISOES_MOTIVO_14', 111, 116, 'N', 6, 'Rescis√µes motivo 14')),\n",
    "            ('QT_RESCISOES_MOTIVO_15', CampoLayout('QT_RESCISOES_MOTIVO_15', 117, 122, 'N', 6, 'Rescis√µes motivo 15')),\n",
    "            ('QT_RESCISOES_MOTIVO_17', CampoLayout('QT_RESCISOES_MOTIVO_17', 123, 128, 'N', 6, 'Rescis√µes motivo 17')),\n",
    "            ('QT_RESCISOES_MOTIVO_23', CampoLayout('QT_RESCISOES_MOTIVO_23', 129, 134, 'N', 6, 'Rescis√µes motivo 23')),\n",
    "            ('QT_RESCISOES_MOTIVO_24', CampoLayout('QT_RESCISOES_MOTIVO_24', 135, 140, 'N', 6, 'Rescis√µes motivo 24')),\n",
    "            ('QT_RESCISOES_MOTIVO_25', CampoLayout('QT_RESCISOES_MOTIVO_25', 141, 146, 'N', 6, 'Rescis√µes motivo 25')),\n",
    "            ('QT_RESCISOES_MOTIVO_26', CampoLayout('QT_RESCISOES_MOTIVO_26', 147, 152, 'N', 6, 'Rescis√µes motivo 26')),\n",
    "            ('QT_RESCISOES_MOTIVO_27', CampoLayout('QT_RESCISOES_MOTIVO_27', 153, 158, 'N', 6, 'Rescis√µes motivo 27')),\n",
    "            ('QT_RESCISOES_MOTIVO_33', CampoLayout('QT_RESCISOES_MOTIVO_33', 159, 164, 'N', 6, 'Rescis√µes motivo 33')),\n",
    "            ('QT_VINCULOS_CAT_101', CampoLayout('QT_VINCULOS_CAT_101', 165, 170, 'N', 6, 'V√≠nculos categoria 101')),\n",
    "            ('QT_VINCULOS_CAT_102', CampoLayout('QT_VINCULOS_CAT_102', 171, 176, 'N', 6, 'V√≠nculos categoria 102')),\n",
    "            ('QT_VINCULOS_CAT_103', CampoLayout('QT_VINCULOS_CAT_103', 177, 182, 'N', 6, 'V√≠nculos categoria 103')),\n",
    "            ('QT_VINCULOS_CAT_105', CampoLayout('QT_VINCULOS_CAT_105', 183, 188, 'N', 6, 'V√≠nculos categoria 105')),\n",
    "            ('QT_VINCULOS_CAT_106', CampoLayout('QT_VINCULOS_CAT_106', 189, 194, 'N', 6, 'V√≠nculos categoria 106')),\n",
    "            ('QT_VINCULOS_CAT_107', CampoLayout('QT_VINCULOS_CAT_107', 195, 200, 'N', 6, 'V√≠nculos categoria 107')),\n",
    "            ('QT_VINCULOS_CAT_108', CampoLayout('QT_VINCULOS_CAT_108', 201, 206, 'N', 6, 'V√≠nculos categoria 108')),\n",
    "            ('QT_VINCULOS_CAT_111', CampoLayout('QT_VINCULOS_CAT_111', 207, 212, 'N', 6, 'V√≠nculos categoria 111')),\n",
    "            ('QT_VINCULOS_CAT_201', CampoLayout('QT_VINCULOS_CAT_201', 213, 218, 'N', 6, 'V√≠nculos categoria 201 - Avulso')),\n",
    "            ('QT_VINCULOS_CAT_202', CampoLayout('QT_VINCULOS_CAT_202', 219, 224, 'N', 6, 'V√≠nculos categoria 202 - Avulso')),\n",
    "            ('QT_VINCULOS_CAT_301', CampoLayout('QT_VINCULOS_CAT_301', 225, 230, 'N', 6, 'V√≠nculos categoria 301')),\n",
    "            ('QT_VINCULOS_CAT_302', CampoLayout('QT_VINCULOS_CAT_302', 231, 236, 'N', 6, 'V√≠nculos categoria 302')),\n",
    "            ('QT_VINCULOS_CAT_303', CampoLayout('QT_VINCULOS_CAT_303', 237, 242, 'N', 6, 'V√≠nculos categoria 303')),\n",
    "            ('QT_VINCULOS_CAT_304', CampoLayout('QT_VINCULOS_CAT_304', 243, 248, 'N', 6, 'V√≠nculos categoria 304')),\n",
    "            ('QT_VINCULOS_CAT_306', CampoLayout('QT_VINCULOS_CAT_306', 249, 254, 'N', 6, 'V√≠nculos categoria 306')),\n",
    "            ('QT_VINCULOS_CAT_309', CampoLayout('QT_VINCULOS_CAT_309', 255, 260, 'N', 6, 'V√≠nculos categoria 309')),\n",
    "            ('QT_VINCULOS_CAT_401', CampoLayout('QT_VINCULOS_CAT_401', 261, 266, 'N', 6, 'V√≠nculos categoria 401')),\n",
    "            ('QT_VINCULOS_CAT_410', CampoLayout('QT_VINCULOS_CAT_410', 267, 272, 'N', 6, 'V√≠nculos categoria 410')),\n",
    "            ('DT_EVENTO_CONTRIBUINTE', CampoLayout('DT_EVENTO_CONTRIBUINTE', 273, 286, 'X', 14, 'Data/hora AAAAMMDDHHMMSS')),\n",
    "            ('ID_CLASSIFICACAO_TRIBUTARIA', CampoLayout('ID_CLASSIFICACAO_TRIBUTARIA', 287, 288, 'N', 2, 'Classifica√ß√£o tribut√°ria')),\n",
    "            ('NU_CNAE_PREPONDERANTE', CampoLayout('NU_CNAE_PREPONDERANTE', 289, 295, 'N', 7, 'CNAE preponderante')),\n",
    "            ('NU_ALIQUOTA_GILRAT', CampoLayout('NU_ALIQUOTA_GILRAT', 296, 296, 'N', 1, 'Al√≠quota GILRAT')),\n",
    "            ('VL_FATOR_ACIDENTARIO_PREV', CampoLayout('VL_FATOR_ACIDENTARIO_PREV', 297, 306, 'X', 10, 'FAP')),\n",
    "            ('VL_ALIQUOTA_GILRAT_AJUST', CampoLayout('VL_ALIQUOTA_GILRAT_AJUST', 307, 316, 'X', 10, 'Al√≠quota GILRAT ajustada')),\n",
    "            ('VL_BASE_CALCULO_CONTRIB_PREV', CampoLayout('VL_BASE_CALCULO_CONTRIB_PREV', 317, 333, 'X', 17, 'Base c√°lculo total')),\n",
    "            ('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_101', CampoLayout('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_101', 334, 350, 'X', 17, 'Base cat 101')),\n",
    "            ('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_102', CampoLayout('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_102', 351, 367, 'X', 17, 'Base cat 102')),\n",
    "            ('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_103', CampoLayout('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_103', 368, 384, 'X', 17, 'Base cat 103')),\n",
    "            ('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_105', CampoLayout('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_105', 385, 401, 'X', 17, 'Base cat 105')),\n",
    "            ('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_106', CampoLayout('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_106', 402, 418, 'X', 17, 'Base cat 106')),\n",
    "            ('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_107', CampoLayout('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_107', 419, 435, 'X', 17, 'Base cat 107')),\n",
    "            ('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_108', CampoLayout('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_108', 436, 452, 'X', 17, 'Base cat 108')),\n",
    "            ('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_111', CampoLayout('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_111', 453, 469, 'X', 17, 'Base cat 111')),\n",
    "            ('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_201', CampoLayout('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_201', 470, 486, 'X', 17, 'Base cat 201')),\n",
    "            ('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_202', CampoLayout('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_202', 487, 503, 'X', 17, 'Base cat 202')),\n",
    "            ('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_301', CampoLayout('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_301', 504, 520, 'X', 17, 'Base cat 301')),\n",
    "            ('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_302', CampoLayout('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_302', 521, 537, 'X', 17, 'Base cat 302')),\n",
    "            ('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_303', CampoLayout('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_303', 538, 554, 'X', 17, 'Base cat 303')),\n",
    "            ('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_304', CampoLayout('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_304', 555, 571, 'X', 17, 'Base cat 304')),\n",
    "            ('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_306', CampoLayout('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_306', 572, 588, 'X', 17, 'Base cat 306')),\n",
    "            ('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_309', CampoLayout('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_309', 589, 605, 'X', 17, 'Base cat 309')),\n",
    "            ('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_401', CampoLayout('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_401', 606, 622, 'X', 17, 'Base cat 401')),\n",
    "            ('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_410', CampoLayout('VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_410', 623, 639, 'X', 17, 'Base cat 410')),\n",
    "            ('NU_RECIBO_1299', CampoLayout('NU_RECIBO_1299', 640, 679, 'X', 40, 'N√∫mero recibo S-1299'))\n",
    "        ])\n",
    "        \n",
    "        # Categorias de segurados v√°lidas\n",
    "        self.categorias_validas = [101, 102, 103, 105, 106, 107, 108, 111,\n",
    "                                  201, 202, 301, 302, 303, 304, 306, 309, 401, 410]\n",
    "        \n",
    "        # Motivos de rescis√£o v√°lidos\n",
    "        self.motivos_rescisao = [1, 2, 3, 4, 5, 6, 7, 8, 10, 14, 15, 17, 23, 24, 25, 26, 27, 33]\n",
    "        \n",
    "        # Tipos de inscri√ß√£o\n",
    "        self.tipos_inscricao = {\n",
    "            0: 'CNPJ Raiz', 1: 'CNPJ', 2: 'CPF', 3: 'CAEPF',\n",
    "            4: 'CNO', 5: 'CGC', 6: 'CEI'\n",
    "        }\n",
    "        \n",
    "        # Classifica√ß√µes tribut√°rias\n",
    "        self.classificacoes_tributarias = {\n",
    "            1: 'Simples Nacional com substitui√ß√£o',\n",
    "            2: 'Simples Nacional sem substitui√ß√£o',\n",
    "            3: 'Simples Nacional misto',\n",
    "            4: 'MEI',\n",
    "            6: 'Agroind√∫stria',\n",
    "            7: 'Produtor Rural PJ',\n",
    "            8: 'Cons√≥rcio Simplificado',\n",
    "            9: '√ìrg√£o Gestor de M√£o de Obra',\n",
    "            10: 'Entidade Sindical Lei 12.023/2009',\n",
    "            11: 'Associa√ß√£o Desportiva',\n",
    "            13: 'Institui√ß√£o Financeira',\n",
    "            14: 'Sindicatos em geral',\n",
    "            21: 'Pessoa F√≠sica',\n",
    "            22: 'Segurado Especial',\n",
    "            60: 'Miss√£o Diplom√°tica',\n",
    "            70: 'Empresa Decreto 5.436/2005',\n",
    "            80: 'Entidade Beneficente',\n",
    "            85: 'Administra√ß√£o P√∫blica',\n",
    "            99: 'Pessoas Jur√≠dicas em Geral'\n",
    "        }\n",
    "\n",
    "\n",
    "class ParserESocialPosicional:\n",
    "    \"\"\"Parser profissional para arquivos eSocial em formato posicional\"\"\"\n",
    "    \n",
    "    def __init__(self, layout: LayoutESocialCompleto):\n",
    "        self.layout = layout\n",
    "        self.logger = logging.getLogger(f\"{__name__}.ParserESocial\")\n",
    "        self.erros_parse = []\n",
    "        \n",
    "    @lru_cache(maxsize=1000)\n",
    "    def _converter_valor(self, valor: str, tipo: str, campo: str) -> Any:\n",
    "        \"\"\"Converte valor conforme tipo do campo com cache\"\"\"\n",
    "        try:\n",
    "            valor = valor.strip()\n",
    "            \n",
    "            if not valor or valor == '0' * len(valor):\n",
    "                return None if tipo == 'X' else 0\n",
    "                \n",
    "            if tipo == 'N':  # Num√©rico\n",
    "                # Remove zeros √† esquerda\n",
    "                valor = valor.lstrip('0') or '0'\n",
    "                return int(valor)\n",
    "            else:  # Alfanum√©rico\n",
    "                return valor.strip()\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Erro ao converter campo {campo}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _processar_linha(self, linha: str, numero_linha: int) -> Dict[str, Any]:\n",
    "        \"\"\"Processa uma linha do arquivo posicional\"\"\"\n",
    "        registro = {'linha_arquivo': numero_linha}\n",
    "        \n",
    "        for campo_nome, campo_def in self.layout.campos.items():\n",
    "            try:\n",
    "                # Extrair valor pela posi√ß√£o (ajustando para √≠ndice 0)\n",
    "                valor_bruto = linha[campo_def.posicao_inicial-1:campo_def.posicao_final]\n",
    "                \n",
    "                # Converter valor\n",
    "                valor = self._converter_valor(valor_bruto, campo_def.tipo, campo_nome)\n",
    "                \n",
    "                # Tratamentos especiais\n",
    "                if campo_nome == 'NU_INSCRICAO_ESTABELECIM' and valor:\n",
    "                    valor = self._formatar_cnpj(valor)\n",
    "                elif campo_nome == 'VL_FATOR_ACIDENTARIO_PREV' and valor:\n",
    "                    valor = self._formatar_fap(valor)\n",
    "                elif campo_nome.startswith('VL_') and valor:\n",
    "                    valor = self._converter_valor_monetario(valor)\n",
    "                \n",
    "                registro[campo_nome] = valor\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Erro ao processar campo {campo_nome} na linha {numero_linha}: {e}\")\n",
    "                self.erros_parse.append({\n",
    "                    'linha': numero_linha,\n",
    "                    'campo': campo_nome,\n",
    "                    'erro': str(e)\n",
    "                })\n",
    "                registro[campo_nome] = None\n",
    "        \n",
    "        return registro\n",
    "    \n",
    "    def _formatar_cnpj(self, cnpj: str) -> str:\n",
    "        \"\"\"Formata CNPJ adicionando zeros √† esquerda se necess√°rio\"\"\"\n",
    "        cnpj = re.sub(r'\\D', '', str(cnpj))  # Remove n√£o-d√≠gitos\n",
    "        \n",
    "        # Adiciona zeros √† esquerda se necess√°rio\n",
    "        if len(cnpj) < 14:\n",
    "            cnpj = cnpj.zfill(14)\n",
    "        \n",
    "        return cnpj[:14]  # Garante m√°ximo 14 d√≠gitos\n",
    "    \n",
    "    def _formatar_fap(self, fap: str) -> float:\n",
    "        \"\"\"Formata FAP removendo zeros excedentes\"\"\"\n",
    "        try:\n",
    "            # Remove zeros excedentes (ex: \"00010000\" ‚Üí \"1.0000\")\n",
    "            fap_str = str(fap).strip()\n",
    "            if len(fap_str) >= 8:\n",
    "                # Assume formato NNNNNNNN onde os 4 √∫ltimos s√£o decimais\n",
    "                parte_inteira = fap_str[:-4].lstrip('0') or '0'\n",
    "                parte_decimal = fap_str[-4:]\n",
    "                fap_float = float(f\"{parte_inteira}.{parte_decimal}\")\n",
    "            else:\n",
    "                fap_float = float(fap_str) / 10000\n",
    "            \n",
    "            # Garante intervalo v√°lido [0.5, 2.0]\n",
    "            return max(0.5, min(2.0, fap_float))\n",
    "        except:\n",
    "            return 1.0  # Valor padr√£o se houver erro\n",
    "    \n",
    "    def _converter_valor_monetario(self, valor: str) -> float:\n",
    "        \"\"\"Converte valor monet√°rio do formato eSocial\"\"\"\n",
    "        try:\n",
    "            # Remove caracteres n√£o num√©ricos exceto v√≠rgula e ponto\n",
    "            valor_limpo = re.sub(r'[^\\d,.-]', '', str(valor))\n",
    "            \n",
    "            # Trata formato brasileiro (v√≠rgula como decimal)\n",
    "            if ',' in valor_limpo:\n",
    "                valor_limpo = valor_limpo.replace('.', '').replace(',', '.')\n",
    "            \n",
    "            return float(valor_limpo)\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def _converter_cno_para_cnpj(self, tipo_inscricao: int, numero_inscricao: str) -> Tuple[int, str]:\n",
    "        \"\"\"Converte CNO para CNPJ do respons√°vel\"\"\"\n",
    "        if tipo_inscricao == 4:  # CNO\n",
    "            self.logger.info(f\"Convertendo CNO {numero_inscricao} para CNPJ respons√°vel\")\n",
    "            # Aqui seria feita a convers√£o real consultando base de dados\n",
    "            # Por enquanto, retorna como CNPJ para n√£o bloquear processamento\n",
    "            return 1, numero_inscricao  # Tipo 1 = CNPJ\n",
    "        return tipo_inscricao, numero_inscricao\n",
    "    \n",
    "    def parse_arquivo(self, arquivo_path: Union[str, Path], \n",
    "                     encoding: str = 'utf-8',\n",
    "                     usar_parquet: bool = True,\n",
    "                     chunk_size: int = CHUNK_SIZE) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Parse arquivo eSocial posicional com otimiza√ß√µes\n",
    "        \n",
    "        Args:\n",
    "            arquivo_path: Caminho do arquivo\n",
    "            encoding: Encoding do arquivo\n",
    "            usar_parquet: Se True, salva resultado em Parquet\n",
    "            chunk_size: Tamanho do chunk para processamento\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame com dados parseados\n",
    "        \"\"\"\n",
    "        arquivo_path = Path(arquivo_path)\n",
    "        self.logger.info(f\"Iniciando parse do arquivo: {arquivo_path}\")\n",
    "        self.logger.info(f\"Tamanho do arquivo: {arquivo_path.stat().st_size / (1024**3):.2f} GB\")\n",
    "        \n",
    "        # Verificar cache\n",
    "        cache_key = f\"esocial_parse_{arquivo_path.stem}_{arquivo_path.stat().st_mtime}\"\n",
    "        if REDIS_AVAILABLE and redis_client.exists(cache_key):\n",
    "            self.logger.info(\"Carregando dados do cache Redis\")\n",
    "            return pd.read_json(redis_client.get(cache_key))\n",
    "        \n",
    "        # Processar arquivo\n",
    "        inicio = datetime.now()\n",
    "        registros = []\n",
    "        self.erros_parse = []\n",
    "        \n",
    "        try:\n",
    "            # Usar processamento paralelo para arquivos grandes\n",
    "            tamanho_gb = arquivo_path.stat().st_size / (1024**3)\n",
    "            \n",
    "            if tamanho_gb > 1 and MAX_WORKERS > 1:\n",
    "                self.logger.info(f\"Usando processamento paralelo com {MAX_WORKERS} workers\")\n",
    "                registros = self._processar_paralelo(arquivo_path, encoding, chunk_size)\n",
    "            else:\n",
    "                # Processamento sequencial para arquivos pequenos\n",
    "                with open(arquivo_path, 'r', encoding=encoding) as file:\n",
    "                    for i, linha in enumerate(tqdm(file, desc=\"Processando linhas\"), 1):\n",
    "                        if linha.strip():\n",
    "                            registro = self._processar_linha(linha, i)\n",
    "                            registros.append(registro)\n",
    "                        \n",
    "                        # Liberar mem√≥ria periodicamente\n",
    "                        if i % chunk_size == 0:\n",
    "                            self.logger.info(f\"Processadas {i:,} linhas\")\n",
    "            \n",
    "            # Criar DataFrame\n",
    "            df = pd.DataFrame(registros)\n",
    "            \n",
    "            # Aplicar tipos de dados corretos\n",
    "            df = self._aplicar_tipos_dados(df)\n",
    "            \n",
    "            # Adicionar valida√ß√µes e corre√ß√µes\n",
    "            df = self._aplicar_correcoes_automaticas(df)\n",
    "            \n",
    "            # Salvar em Parquet se solicitado\n",
    "            if usar_parquet:\n",
    "                parquet_path = arquivo_path.with_suffix('.parquet')\n",
    "                df.to_parquet(parquet_path, engine='pyarrow', compression='snappy')\n",
    "                self.logger.info(f\"Dados salvos em Parquet: {parquet_path}\")\n",
    "            \n",
    "            # Cachear resultado\n",
    "            if REDIS_AVAILABLE and len(df) < 500000:  # Cache apenas datasets menores\n",
    "                redis_client.setex(cache_key, CACHE_TTL, df.to_json())\n",
    "            \n",
    "            tempo_total = (datetime.now() - inicio).total_seconds()\n",
    "            self.logger.info(f\"Parse conclu√≠do em {tempo_total:.2f} segundos\")\n",
    "            self.logger.info(f\"Total de registros: {len(df):,}\")\n",
    "            \n",
    "            if self.erros_parse:\n",
    "                self.logger.warning(f\"Total de erros durante parse: {len(self.erros_parse)}\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Erro fatal durante parse: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _processar_paralelo(self, arquivo_path: Path, encoding: str, chunk_size: int) -> List[Dict]:\n",
    "        \"\"\"Processa arquivo em paralelo para melhor performance\"\"\"\n",
    "        # Dividir arquivo em chunks\n",
    "        chunks = []\n",
    "        with open(arquivo_path, 'r', encoding=encoding) as f:\n",
    "            chunk = []\n",
    "            for i, linha in enumerate(f, 1):\n",
    "                chunk.append((linha, i))\n",
    "                if len(chunk) >= chunk_size:\n",
    "                    chunks.append(chunk)\n",
    "                    chunk = []\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        # Processar chunks em paralelo\n",
    "        with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            futures = []\n",
    "            for chunk in chunks:\n",
    "                future = executor.submit(self._processar_chunk, chunk)\n",
    "                futures.append(future)\n",
    "            \n",
    "            # Coletar resultados\n",
    "            registros = []\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processando chunks\"):\n",
    "                registros.extend(future.result())\n",
    "        \n",
    "        return registros\n",
    "    \n",
    "    def _processar_chunk(self, chunk: List[Tuple[str, int]]) -> List[Dict]:\n",
    "        \"\"\"Processa um chunk de linhas\"\"\"\n",
    "        registros = []\n",
    "        for linha, numero_linha in chunk:\n",
    "            if linha.strip():\n",
    "                registro = self._processar_linha(linha, numero_linha)\n",
    "                registros.append(registro)\n",
    "        return registros\n",
    "    \n",
    "    def _aplicar_tipos_dados(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Aplica tipos de dados corretos aos campos\"\"\"\n",
    "        for campo_nome, campo_def in self.layout.campos.items():\n",
    "            if campo_nome in df.columns:\n",
    "                try:\n",
    "                    if campo_def.tipo == 'N':\n",
    "                        df[campo_nome] = pd.to_numeric(df[campo_nome], errors='coerce').fillna(0).astype('int64')\n",
    "                    elif campo_nome.startswith('VL_'):\n",
    "                        df[campo_nome] = pd.to_numeric(df[campo_nome], errors='coerce').fillna(0.0).astype('float64')\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Erro ao converter tipo do campo {campo_nome}: {e}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _aplicar_correcoes_automaticas(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Aplica corre√ß√µes autom√°ticas nos dados\"\"\"\n",
    "        self.logger.info(\"Aplicando corre√ß√µes autom√°ticas...\")\n",
    "        \n",
    "        # Converter CNO para CNPJ\n",
    "        mask_cno = df['ID_TIPO_INSCR_ESTABELECIM'] == 4\n",
    "        if mask_cno.any():\n",
    "            df.loc[mask_cno, ['ID_TIPO_INSCR_ESTABELECIM', 'NU_INSCRICAO_ESTABELECIM']] = \\\n",
    "                df[mask_cno].apply(lambda x: self._converter_cno_para_cnpj(\n",
    "                    x['ID_TIPO_INSCR_ESTABELECIM'], \n",
    "                    x['NU_INSCRICAO_ESTABELECIM']\n",
    "                ), axis=1, result_type='expand')\n",
    "        \n",
    "        # Adicionar valida√ß√µes\n",
    "        df['CNPJ_VALIDO'] = df['NU_INSCRICAO_ESTABELECIM'].apply(self._validar_cnpj)\n",
    "        df['FAP_VALIDO'] = df['VL_FATOR_ACIDENTARIO_PREV'].between(0.5, 2.0)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @lru_cache(maxsize=10000)\n",
    "    def _validar_cnpj(self, cnpj: str) -> bool:\n",
    "        \"\"\"Valida CNPJ usando algoritmo M√≥dulo 11\"\"\"\n",
    "        if not cnpj or not isinstance(cnpj, str):\n",
    "            return False\n",
    "            \n",
    "        # Remove caracteres n√£o num√©ricos\n",
    "        cnpj = re.sub(r'\\D', '', cnpj)\n",
    "        \n",
    "        # Verifica se tem 14 d√≠gitos\n",
    "        if len(cnpj) != 14:\n",
    "            return False\n",
    "        \n",
    "        # Verifica se n√£o √© sequ√™ncia de n√∫meros iguais\n",
    "        if cnpj == cnpj[0] * 14:\n",
    "            return False\n",
    "        \n",
    "        # Valida√ß√£o do primeiro d√≠gito verificador\n",
    "        soma = 0\n",
    "        peso = 5\n",
    "        for i in range(12):\n",
    "            soma += int(cnpj[i]) * peso\n",
    "            peso = peso - 1 if peso > 2 else 9\n",
    "        \n",
    "        resto = soma % 11\n",
    "        digito1 = 0 if resto < 2 else 11 - resto\n",
    "        \n",
    "        if int(cnpj[12]) != digito1:\n",
    "            return False\n",
    "        \n",
    "        # Valida√ß√£o do segundo d√≠gito verificador\n",
    "        soma = 0\n",
    "        peso = 6\n",
    "        for i in range(13):\n",
    "            soma += int(cnpj[i]) * peso\n",
    "            peso = peso - 1 if peso > 2 else 9\n",
    "        \n",
    "        resto = soma % 11\n",
    "        digito2 = 0 if resto < 2 else 11 - resto\n",
    "        \n",
    "        return int(cnpj[13]) == digito2\n",
    "\n",
    "\n",
    "# Criar inst√¢ncias\n",
    "layout_esocial = LayoutESocialCompleto()\n",
    "parser_esocial = ParserESocialPosicional(layout_esocial)\n",
    "\n",
    "logger.info(f\"Layout eSocial carregado com {len(layout_esocial.campos)} campos\")\n",
    "logger.info(\"Parser posicional configurado com corre√ß√µes autom√°ticas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-17 09:21:36 | INFO     | ESocialAnalyzer | 786219191:<module>:359 | Processador otimizado configurado para arquivos de at√© 25GB+\n",
      "2025-06-17 09:21:36 | INFO     | ESocialAnalyzer | 786219191:<module>:360 | Suporta convers√£o streaming TXT‚ÜíParquet e processamento distribu√≠do com Dask\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 2.1: Otimiza√ß√µes para Processamento de Arquivos Grandes (Estado da Arte Absoluto)\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from typing import Iterator\n",
    "import gc\n",
    "\n",
    "class ProcessadorESocialOtimizado:\n",
    "    \"\"\"Processador otimizado para arquivos eSocial de at√© 25GB+\"\"\"\n",
    "    \n",
    "    def __init__(self, layout: LayoutESocialCompleto):\n",
    "        self.layout = layout\n",
    "        self.logger = logging.getLogger(f\"{__name__}.ProcessadorOtimizado\")\n",
    "        self.parser_base = ParserESocialPosicional(layout)\n",
    "        \n",
    "    def converter_txt_para_parquet_streaming(self, \n",
    "                                           arquivo_txt: Path, \n",
    "                                           arquivo_parquet: Path = None,\n",
    "                                           chunk_size: int = 500000,  # Aumentado para melhor performance\n",
    "                                           memoria_maxima_gb: float = 4.0) -> Path:\n",
    "        \"\"\"\n",
    "        Converte arquivo TXT para Parquet usando streaming sem carregar tudo em mem√≥ria\n",
    "        \n",
    "        Args:\n",
    "            arquivo_txt: Caminho do arquivo TXT\n",
    "            arquivo_parquet: Caminho de sa√≠da (opcional)\n",
    "            chunk_size: Linhas por chunk\n",
    "            memoria_maxima_gb: Mem√≥ria m√°xima a usar (GB)\n",
    "            \n",
    "        Returns:\n",
    "            Path do arquivo Parquet gerado\n",
    "        \"\"\"\n",
    "        if arquivo_parquet is None:\n",
    "            arquivo_parquet = arquivo_txt.with_suffix('.parquet')\n",
    "            \n",
    "        self.logger.info(f\"Iniciando convers√£o streaming TXT‚ÜíParquet\")\n",
    "        self.logger.info(f\"Arquivo entrada: {arquivo_txt} ({arquivo_txt.stat().st_size / 1e9:.2f} GB)\")\n",
    "        self.logger.info(f\"Chunk size: {chunk_size:,} linhas\")\n",
    "        self.logger.info(f\"Mem√≥ria m√°xima: {memoria_maxima_gb:.1f} GB\")\n",
    "        \n",
    "        # Definir schema Arrow otimizado\n",
    "        schema = self._criar_schema_arrow_otimizado()\n",
    "        \n",
    "        # Estat√≠sticas\n",
    "        inicio = datetime.now()\n",
    "        linhas_processadas = 0\n",
    "        chunks_escritos = 0\n",
    "        erros_parse = []\n",
    "        \n",
    "        try:\n",
    "            # Criar writer Parquet com compress√£o\n",
    "            with pq.ParquetWriter(\n",
    "                arquivo_parquet, \n",
    "                schema,\n",
    "                compression='snappy',\n",
    "                version='2.6',\n",
    "                data_page_size=1024*1024,  # 1MB pages\n",
    "                write_batch_size=1000\n",
    "            ) as writer:\n",
    "                \n",
    "                # Processar arquivo em streaming\n",
    "                with open(arquivo_txt, 'r', encoding='utf-8', buffering=1024*1024) as file:\n",
    "                    batch = []\n",
    "                    \n",
    "                    for linha_num, linha in enumerate(tqdm(file, desc=\"Convertendo para Parquet\"), 1):\n",
    "                        if not linha.strip():\n",
    "                            continue\n",
    "                            \n",
    "                        try:\n",
    "                            # Processar linha\n",
    "                            registro = self.parser_base._processar_linha(linha, linha_num)\n",
    "                            registro = self._otimizar_tipos_registro(registro)\n",
    "                            batch.append(registro)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            erros_parse.append({\n",
    "                                'linha': linha_num,\n",
    "                                'erro': str(e)\n",
    "                            })\n",
    "                            continue\n",
    "                        \n",
    "                        # Escrever batch quando atingir tamanho\n",
    "                        if len(batch) >= chunk_size:\n",
    "                            df_batch = pd.DataFrame(batch)\n",
    "                            df_batch = self._otimizar_tipos_dataframe(df_batch)\n",
    "                            \n",
    "                            # Converter para Arrow e escrever\n",
    "                            table = pa.Table.from_pandas(df_batch, schema=schema)\n",
    "                            writer.write_table(table)\n",
    "                            \n",
    "                            chunks_escritos += 1\n",
    "                            linhas_processadas += len(batch)\n",
    "                            \n",
    "                            # Log progresso\n",
    "                            if chunks_escritos % 10 == 0:\n",
    "                                tempo_decorrido = (datetime.now() - inicio).total_seconds()\n",
    "                                velocidade = linhas_processadas / tempo_decorrido\n",
    "                                memoria_uso = self._get_memoria_uso_gb()\n",
    "                                \n",
    "                                self.logger.info(\n",
    "                                    f\"Progresso: {linhas_processadas:,} linhas | \"\n",
    "                                    f\"{chunks_escritos} chunks | \"\n",
    "                                    f\"{velocidade:.0f} linhas/s | \"\n",
    "                                    f\"Mem√≥ria: {memoria_uso:.1f} GB\"\n",
    "                                )\n",
    "                            \n",
    "                            # Limpar batch e for√ßar coleta de lixo\n",
    "                            batch = []\n",
    "                            \n",
    "                            # Verificar mem√≥ria e fazer GC se necess√°rio\n",
    "                            if self._get_memoria_uso_gb() > memoria_maxima_gb * 0.8:\n",
    "                                gc.collect()\n",
    "                    \n",
    "                    # Escrever √∫ltimo batch\n",
    "                    if batch:\n",
    "                        df_batch = pd.DataFrame(batch)\n",
    "                        df_batch = self._otimizar_tipos_dataframe(df_batch)\n",
    "                        table = pa.Table.from_pandas(df_batch, schema=schema)\n",
    "                        writer.write_table(table)\n",
    "                        linhas_processadas += len(batch)\n",
    "            \n",
    "            # Estat√≠sticas finais\n",
    "            tempo_total = (datetime.now() - inicio).total_seconds()\n",
    "            tamanho_parquet = arquivo_parquet.stat().st_size / 1e9\n",
    "            taxa_compressao = (1 - tamanho_parquet / (arquivo_txt.stat().st_size / 1e9)) * 100\n",
    "            \n",
    "            self.logger.info(\"=\"*80)\n",
    "            self.logger.info(\"CONVERS√ÉO CONCLU√çDA COM SUCESSO!\")\n",
    "            self.logger.info(\"=\"*80)\n",
    "            self.logger.info(f\"Tempo total: {tempo_total/60:.1f} minutos\")\n",
    "            self.logger.info(f\"Linhas processadas: {linhas_processadas:,}\")\n",
    "            self.logger.info(f\"Chunks escritos: {chunks_escritos}\")\n",
    "            self.logger.info(f\"Velocidade m√©dia: {linhas_processadas/tempo_total:.0f} linhas/s\")\n",
    "            self.logger.info(f\"Tamanho Parquet: {tamanho_parquet:.2f} GB\")\n",
    "            self.logger.info(f\"Taxa de compress√£o: {taxa_compressao:.1f}%\")\n",
    "            self.logger.info(f\"Erros de parse: {len(erros_parse)}\")\n",
    "            self.logger.info(\"=\"*80)\n",
    "            \n",
    "            return arquivo_parquet\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Erro fatal na convers√£o: {e}\")\n",
    "            if arquivo_parquet.exists():\n",
    "                arquivo_parquet.unlink()  # Remover arquivo parcial\n",
    "            raise\n",
    "    \n",
    "    def processar_parquet_com_dask(self, \n",
    "                                  arquivo_parquet: Path,\n",
    "                                  blocksize: str = \"128MB\") -> dd.DataFrame:\n",
    "        \"\"\"\n",
    "        Processa arquivo Parquet usando Dask para an√°lise distribu√≠da\n",
    "        \n",
    "        Args:\n",
    "            arquivo_parquet: Caminho do arquivo Parquet\n",
    "            blocksize: Tamanho dos blocos para Dask\n",
    "            \n",
    "        Returns:\n",
    "            Dask DataFrame\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Carregando Parquet com Dask: {arquivo_parquet}\")\n",
    "        \n",
    "        # Ler com Dask (lazy loading)\n",
    "        ddf = dd.read_parquet(\n",
    "            arquivo_parquet,\n",
    "            engine='pyarrow',\n",
    "            blocksize=blocksize,\n",
    "            index=False\n",
    "        )\n",
    "        \n",
    "        # Otimizar parti√ß√µes se necess√°rio\n",
    "        n_partitions = ddf.npartitions\n",
    "        ideal_partitions = max(4, min(100, int(arquivo_parquet.stat().st_size / (128 * 1024 * 1024))))\n",
    "        \n",
    "        if n_partitions != ideal_partitions:\n",
    "            self.logger.info(f\"Reparticionando de {n_partitions} para {ideal_partitions} parti√ß√µes\")\n",
    "            ddf = ddf.repartition(npartitions=ideal_partitions)\n",
    "        \n",
    "        # Adicionar colunas calculadas\n",
    "        ddf = self._adicionar_validacoes_dask(ddf)\n",
    "        \n",
    "        self.logger.info(f\"Dask DataFrame criado com {ddf.npartitions} parti√ß√µes\")\n",
    "        return ddf\n",
    "    \n",
    "    def _criar_schema_arrow_otimizado(self) -> pa.Schema:\n",
    "        \"\"\"Cria schema Arrow otimizado para economia de mem√≥ria\"\"\"\n",
    "        campos_arrow = []\n",
    "    \n",
    "        for nome, campo in self.layout.campos.items():\n",
    "            if campo.tipo == 'N':\n",
    "                # Usar tipos menores para inteiros\n",
    "                if 'QT_' in nome:\n",
    "                    tipo_arrow = pa.int32()  # At√© 2 bilh√µes\n",
    "                elif 'NU_PERIODO' in nome:\n",
    "                    tipo_arrow = pa.int32()\n",
    "                else:\n",
    "                    tipo_arrow = pa.int64()\n",
    "            elif nome.startswith('VL_'):\n",
    "                # Float32 para valores monet√°rios (precis√£o suficiente)\n",
    "                tipo_arrow = pa.float32()\n",
    "            else:\n",
    "                # String normal\n",
    "                tipo_arrow = pa.string()\n",
    "        \n",
    "            campos_arrow.append((nome, tipo_arrow))\n",
    "    \n",
    "        # Adicionar campos extras\n",
    "        campos_arrow.extend([\n",
    "            ('linha_arquivo', pa.int64()),\n",
    "            # REMOVIDO: ('CNPJ_VALIDO', pa.bool_()),\n",
    "            # REMOVIDO: ('FAP_VALIDO', pa.bool_()),\n",
    "            ('processado_em', pa.timestamp('ns'))\n",
    "        ])\n",
    "    \n",
    "        return pa.schema(campos_arrow)\n",
    "    \n",
    "    def _otimizar_tipos_registro(self, registro: Dict) -> Dict:\n",
    "        \"\"\"Otimiza tipos de dados de um registro para economia de mem√≥ria\"\"\"\n",
    "        # Converter strings vazias para None\n",
    "        for k, v in registro.items():\n",
    "            if isinstance(v, str) and not v.strip():\n",
    "                registro[k] = None\n",
    "        \n",
    "        # Adicionar timestamp\n",
    "        registro['processado_em'] = datetime.now()\n",
    "        \n",
    "        return registro\n",
    "    \n",
    "    def _otimizar_tipos_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Otimiza tipos de dados do DataFrame para reduzir mem√≥ria em at√© 70%\"\"\"\n",
    "        inicio_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
    "        \n",
    "        # Otimizar inteiros\n",
    "        for col in df.select_dtypes(include=['int64']).columns:\n",
    "            col_min = df[col].min()\n",
    "            col_max = df[col].max()\n",
    "            \n",
    "            if col_min >= 0:\n",
    "                if col_max < 255:\n",
    "                    df[col] = df[col].astype('uint8')\n",
    "                elif col_max < 65535:\n",
    "                    df[col] = df[col].astype('uint16')\n",
    "                elif col_max < 4294967295:\n",
    "                    df[col] = df[col].astype('uint32')\n",
    "            else:\n",
    "                if col_min > -128 and col_max < 127:\n",
    "                    df[col] = df[col].astype('int8')\n",
    "                elif col_min > -32768 and col_max < 32767:\n",
    "                    df[col] = df[col].astype('int16')\n",
    "                elif col_min > -2147483648 and col_max < 2147483647:\n",
    "                    df[col] = df[col].astype('int32')\n",
    "        \n",
    "        # Otimizar floats\n",
    "        for col in df.select_dtypes(include=['float64']).columns:\n",
    "            df[col] = df[col].astype('float32')\n",
    "        \n",
    "        # Converter strings repetidas para categoria\n",
    "        for col in df.select_dtypes(include=['object']).columns:\n",
    "            num_unique = df[col].nunique()\n",
    "            num_total = len(df[col])\n",
    "            if num_unique / num_total < 0.5:  # Menos de 50% valores √∫nicos\n",
    "                df[col] = df[col].astype('category')\n",
    "        \n",
    "        fim_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
    "        reducao = (1 - fim_mb/inicio_mb) * 100\n",
    "        \n",
    "        if reducao > 10:\n",
    "            self.logger.debug(f\"Mem√≥ria reduzida de {inicio_mb:.1f}MB para {fim_mb:.1f}MB ({reducao:.1f}%)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _adicionar_validacoes_dask(self, ddf: dd.DataFrame) -> dd.DataFrame:\n",
    "        \"\"\"Adiciona colunas de valida√ß√£o ao Dask DataFrame\"\"\"\n",
    "        # Valida√ß√£o CNPJ (usar apply com meta)\n",
    "        ddf['CNPJ_VALIDO'] = ddf['NU_INSCRICAO_ESTABELECIM'].apply(\n",
    "            self.parser_base._validar_cnpj,\n",
    "            meta=('CNPJ_VALIDO', 'bool')\n",
    "        )\n",
    "        \n",
    "        # Valida√ß√£o FAP\n",
    "        ddf['FAP_VALIDO'] = (\n",
    "            (ddf['VL_FATOR_ACIDENTARIO_PREV'] >= 0.5) & \n",
    "            (ddf['VL_FATOR_ACIDENTARIO_PREV'] <= 2.0)\n",
    "        )\n",
    "        \n",
    "        # Adicionar flags de anomalia\n",
    "        ddf['ANOMALIA_PERIODO'] = ddf['NU_PERIODO_REFERENCIA'].apply(\n",
    "            lambda x: not self._validar_periodo(x),\n",
    "            meta=('ANOMALIA_PERIODO', 'bool')\n",
    "        )\n",
    "        \n",
    "        return ddf\n",
    "    \n",
    "    def _validar_periodo(self, periodo: Any) -> bool:\n",
    "        \"\"\"Valida formato do per√≠odo\"\"\"\n",
    "        try:\n",
    "            periodo_str = str(periodo)\n",
    "            if len(periodo_str) == 6:  # AAAAMM\n",
    "                ano = int(periodo_str[:4])\n",
    "                mes = int(periodo_str[4:6])\n",
    "                return 2000 <= ano <= 2030 and 1 <= mes <= 13\n",
    "            return False\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def _get_memoria_uso_gb(self) -> float:\n",
    "        \"\"\"Retorna uso de mem√≥ria do processo em GB\"\"\"\n",
    "        import psutil\n",
    "        process = psutil.Process(os.getpid())\n",
    "        return process.memory_info().rss / 1024**3\n",
    "    \n",
    "    def detectar_anomalias_dask(self, ddf: dd.DataFrame) -> dd.DataFrame:\n",
    "        \"\"\"Detecta anomalias usando processamento distribu√≠do com Dask\"\"\"\n",
    "        self.logger.info(\"Iniciando detec√ß√£o de anomalias com Dask\")\n",
    "        \n",
    "        # Aplicar detec√ß√µes por parti√ß√£o\n",
    "        anomalias_ddf = ddf.map_partitions(\n",
    "            self._detectar_anomalias_particao,\n",
    "            meta=pd.DataFrame({\n",
    "                'linha': [0],\n",
    "                'tipo_anomalia': [''],\n",
    "                'severidade': [''],\n",
    "                'descricao': ['']\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        return anomalias_ddf\n",
    "    \n",
    "    def _detectar_anomalias_particao(self, df_part: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Detecta anomalias em uma parti√ß√£o\"\"\"\n",
    "        anomalias = []\n",
    "        \n",
    "        # Valida√ß√µes b√°sicas\n",
    "        for idx, row in df_part.iterrows():\n",
    "            # CNPJ inv√°lido\n",
    "            if not row.get('CNPJ_VALIDO', True):\n",
    "                anomalias.append({\n",
    "                    'linha': row.get('linha_arquivo', idx),\n",
    "                    'tipo_anomalia': 'CNPJ_INVALIDO',\n",
    "                    'severidade': 'CRITICA',\n",
    "                    'descricao': f\"CNPJ inv√°lido: {row.get('NU_INSCRICAO_ESTABELECIM')}\"\n",
    "                })\n",
    "            \n",
    "            # FAP fora do intervalo\n",
    "            if not row.get('FAP_VALIDO', True):\n",
    "                anomalias.append({\n",
    "                    'linha': row.get('linha_arquivo', idx),\n",
    "                    'tipo_anomalia': 'FAP_FORA_INTERVALO',\n",
    "                    'severidade': 'ALTA',\n",
    "                    'descricao': f\"FAP fora do intervalo [0.5, 2.0]: {row.get('VL_FATOR_ACIDENTARIO_PREV')}\"\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(anomalias)\n",
    "\n",
    "\n",
    "# Criar inst√¢ncia do processador otimizado\n",
    "processador_otimizado = ProcessadorESocialOtimizado(layout_esocial)\n",
    "\n",
    "logger.info(\"Processador otimizado configurado para arquivos de at√© 25GB+\")\n",
    "logger.info(\"Suporta convers√£o streaming TXT‚ÜíParquet e processamento distribu√≠do com Dask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-17 09:21:36 | INFO     | ESocialAnalyzer | 2173608567:<module>:156 | Validador de formato eSocial configurado\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 2.2: Validador de Formato e Fun√ß√µes Auxiliares para Arquivos eSocial\n",
    "\n",
    "class ValidadorFormatoESocial:\n",
    "    \"\"\"Valida se o arquivo est√° no formato correto do eSocial conforme DM.204661 v1.9\"\"\"\n",
    "    \n",
    "    TAMANHO_LINHA_ESPERADO = 679\n",
    "    \n",
    "    @staticmethod\n",
    "    def validar_arquivo(arquivo_path: Path, amostra_linhas: int = 10) -> Tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Valida se o arquivo est√° no formato correto\n",
    "        \n",
    "        Returns:\n",
    "            (valido, mensagem)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(arquivo_path, 'r', encoding='utf-8') as f:\n",
    "                linhas_verificadas = 0\n",
    "                linhas_validas = 0\n",
    "                \n",
    "                for i, linha in enumerate(f):\n",
    "                    if i >= amostra_linhas:\n",
    "                        break\n",
    "                    \n",
    "                    linha = linha.rstrip('\\n\\r')\n",
    "                    \n",
    "                    # Verificar tamanho\n",
    "                    if len(linha) == ValidadorFormatoESocial.TAMANHO_LINHA_ESPERADO:\n",
    "                        # Verificar se campos num√©ricos obrigat√≥rios s√£o v√°lidos\n",
    "                        periodo = linha[0:6]\n",
    "                        tipo_inscr = linha[6:7]\n",
    "                        \n",
    "                        try:\n",
    "                            int(periodo)\n",
    "                            int(tipo_inscr)\n",
    "                            linhas_validas += 1\n",
    "                        except ValueError:\n",
    "                            pass\n",
    "                    \n",
    "                    linhas_verificadas += 1\n",
    "                \n",
    "                if linhas_verificadas == 0:\n",
    "                    return False, \"Arquivo vazio\"\n",
    "                \n",
    "                taxa_valida = linhas_validas / linhas_verificadas\n",
    "                \n",
    "                if taxa_valida >= 0.8:  # 80% das linhas v√°lidas\n",
    "                    return True, f\"Formato v√°lido ({linhas_validas}/{linhas_verificadas} linhas OK)\"\n",
    "                else:\n",
    "                    return False, f\"Formato inv√°lido (apenas {linhas_validas}/{linhas_verificadas} linhas v√°lidas)\"\n",
    "                    \n",
    "        except Exception as e:\n",
    "            return False, f\"Erro ao validar arquivo: {str(e)}\"\n",
    "\n",
    "\n",
    "def criar_arquivo_esocial_exemplo(arquivo_path: Path, num_registros: int = 1000):\n",
    "    \"\"\"\n",
    "    Cria arquivo de exemplo no formato correto do eSocial\n",
    "    \n",
    "    Args:\n",
    "        arquivo_path: Caminho do arquivo a criar\n",
    "        num_registros: N√∫mero de registros a gerar\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    logger.info(f\"Criando arquivo exemplo eSocial com {num_registros} registros\")\n",
    "    \n",
    "    arquivo_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(arquivo_path, 'w', encoding='utf-8') as f:\n",
    "        for i in range(num_registros):\n",
    "            # Simular dados variados mas v√°lidos\n",
    "            mes = random.randint(1, 12)\n",
    "            ano = random.choice([2023, 2024])\n",
    "            \n",
    "            # Construir linha seguindo o layout exato\n",
    "            linha = \"\"\n",
    "            \n",
    "            # NU_PERIODO_REFERENCIA (1-6)\n",
    "            linha += f\"{ano}{mes:02d}\".ljust(6, '0')\n",
    "            \n",
    "            # ID_TIPO_INSCR_ESTABELECIM (7)\n",
    "            linha += \"1\"\n",
    "            \n",
    "            # NU_INSCRICAO_ESTABELECIM (8-22)\n",
    "            cnpj = f\"{random.randint(10000000, 99999999):08d}0001{random.randint(10, 99):02d}\"\n",
    "            linha += cnpj.ljust(15)\n",
    "            \n",
    "            # ID_TIPO_INSCRICAO_EMP (23)\n",
    "            linha += \"1\"\n",
    "            \n",
    "            # NU_INSCRICAO_EMPREGADOR (24-38)\n",
    "            linha += cnpj.ljust(15)\n",
    "            \n",
    "            # QT_VINCULOS (39-44)\n",
    "            linha += f\"{random.randint(1, 999):06d}\"\n",
    "            \n",
    "            # QT_ADMISSOES (45-50)\n",
    "            linha += f\"{random.randint(0, 50):06d}\"\n",
    "            \n",
    "            # QT_RESCISOES (51-56)\n",
    "            linha += f\"{random.randint(0, 30):06d}\"\n",
    "            \n",
    "            # QT_RESCISOES por motivo (57-164) - 18 campos de 6 d√≠gitos cada\n",
    "            for _ in range(18):\n",
    "                linha += f\"{random.randint(0, 5):06d}\"\n",
    "            \n",
    "            # QT_VINCULOS por categoria (165-272) - 18 campos de 6 d√≠gitos cada\n",
    "            for _ in range(18):\n",
    "                linha += f\"{random.randint(0, 100):06d}\"\n",
    "            \n",
    "            # DT_EVENTO_CONTRIBUINTE (273-286)\n",
    "            linha += f\"{ano}{mes:02d}15120000\"  # Dia 15 √†s 12:00:00\n",
    "            \n",
    "            # ID_CLASSIFICACAO_TRIBUTARIA (287-288)\n",
    "            linha += \"99\"\n",
    "            \n",
    "            # NU_CNAE_PREPONDERANTE (289-295)\n",
    "            linha += f\"{random.randint(1000000, 9999999):07d}\"\n",
    "            \n",
    "            # NU_ALIQUOTA_GILRAT (296)\n",
    "            linha += random.choice([\"1\", \"2\", \"3\"])\n",
    "            \n",
    "            # VL_FATOR_ACIDENTARIO_PREV (297-306)\n",
    "            fap = random.uniform(0.5, 2.0)\n",
    "            linha += f\"{int(fap * 10000):010d}\"\n",
    "            \n",
    "            # VL_ALIQUOTA_GILRAT_AJUST (307-316)\n",
    "            linha += f\"{int(fap * 10000):010d}\"\n",
    "            \n",
    "            # VL_BASE_CALCULO_CONTRIB_PREV (317-333)\n",
    "            base_total = random.randint(10000, 1000000)\n",
    "            linha += f\"{base_total:017d}\"\n",
    "            \n",
    "            # Bases por categoria (334-639) - 18 campos de 17 d√≠gitos cada\n",
    "            for _ in range(18):\n",
    "                base_cat = random.randint(0, base_total // 10)\n",
    "                linha += f\"{base_cat:017d}\"\n",
    "            \n",
    "            # NU_RECIBO_1299 (640-679)\n",
    "            recibo = f\"S1299{ano}{mes:02d}{i:010d}{'0' * 20}\"\n",
    "            linha += recibo[:40]\n",
    "            \n",
    "            # Garantir exatamente 679 caracteres\n",
    "            linha = linha[:679].ljust(679)\n",
    "            \n",
    "            f.write(linha + '\\n')\n",
    "    \n",
    "    logger.info(f\"Arquivo exemplo criado: {arquivo_path}\")\n",
    "    logger.info(f\"Tamanho: {arquivo_path.stat().st_size / 1e6:.2f} MB\")\n",
    "\n",
    "\n",
    "# Criar validador global\n",
    "validador_esocial = ValidadorFormatoESocial()\n",
    "\n",
    "logger.info(\"Validador de formato eSocial configurado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-17 09:21:36 | INFO     | ESocialAnalyzer | 2237222362:<module>:1221 | Detector de anomalias configurado com 60+ tipos de valida√ß√µes\n",
      "2025-06-17 09:21:36 | INFO     | ESocialAnalyzer | 2237222362:<module>:1222 | Incluindo valida√ß√µes S-5011, S-1299, DCTF-Web e empresas sem movimento\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 3: Detector Avan√ßado de Anomalias com 60+ Tipos e Valida√ß√£o S-5011\n",
    "\n",
    "class TipoAnomalia(Enum):\n",
    "    \"\"\"Enumera√ß√£o de todos os tipos de anomalias detectadas\"\"\"\n",
    "    # Estruturais\n",
    "    CAMPO_OBRIGATORIO_VAZIO = \"EST001\"\n",
    "    TIPO_INSCRICAO_INVALIDO = \"EST002\"\n",
    "    CNPJ_INVALIDO = \"EST003\"\n",
    "    CPF_INVALIDO = \"EST004\"\n",
    "    PERIODO_FORMATO_INVALIDO = \"EST005\"\n",
    "    DATA_HORA_INVALIDA = \"EST006\"\n",
    "    CNO_NAO_CONVERTIDO = \"EST007\"\n",
    "    \n",
    "    # Neg√≥cio\n",
    "    MAX_REGISTROS_EXCEDIDO = \"NEG001\"\n",
    "    FAP_FORA_INTERVALO = \"NEG002\"\n",
    "    CATEGORIA_SEGURADO_INVALIDA = \"NEG003\"\n",
    "    MOTIVO_RESCISAO_INVALIDO = \"NEG004\"\n",
    "    CLASSIFICACAO_TRIBUTARIA_INVALIDA = \"NEG005\"\n",
    "    CNAE_INVALIDO = \"NEG006\"\n",
    "    ALIQUOTA_GILRAT_INVALIDA = \"NEG007\"\n",
    "    SIMPLES_NACIONAL_INCONSISTENTE = \"NEG008\"\n",
    "    \n",
    "    # Temporais\n",
    "    PERIODO_FUTURO = \"TMP001\"\n",
    "    PERIODO_MUITO_ANTIGO = \"TMP002\"\n",
    "    DATA_EVENTO_INCONSISTENTE = \"TMP003\"\n",
    "    SEQUENCIA_TEMPORAL_QUEBRADA = \"TMP004\"\n",
    "    PERIODO_13_INVALIDO = \"TMP005\"\n",
    "    \n",
    "    # Estat√≠sticas\n",
    "    OUTLIER_VINCULOS = \"EST001\"\n",
    "    OUTLIER_ADMISSOES = \"EST002\"\n",
    "    OUTLIER_RESCISOES = \"EST003\"\n",
    "    OUTLIER_BASE_CALCULO = \"EST004\"\n",
    "    VARIACAO_BRUSCA = \"EST005\"\n",
    "    PADRAO_SAZONAL_ANORMAL = \"EST006\"\n",
    "    MEDIA_MOVEL_ANOMALA = \"EST007\"\n",
    "    \n",
    "    # Duplicatas\n",
    "    REGISTRO_DUPLICADO = \"DUP001\"\n",
    "    CNPJ_DUPLICADO_PERIODO = \"DUP002\"\n",
    "    RECIBO_1299_DUPLICADO = \"DUP003\"\n",
    "    \n",
    "    # Conformidade\n",
    "    SOMA_RESCISOES_DIVERGENTE = \"CNF001\"\n",
    "    SOMA_VINCULOS_DIVERGENTE = \"CNF002\"\n",
    "    SOMA_BASE_CALCULO_DIVERGENTE = \"CNF003\"\n",
    "    MAIS_RESCISOES_QUE_VINCULOS = \"CNF004\"\n",
    "    BASE_CALCULO_ZERADA_COM_VINCULOS = \"CNF005\"\n",
    "    VINCULOS_SEM_BASE_CALCULO = \"CNF006\"\n",
    "    \n",
    "    # Qualidade\n",
    "    CAMPOS_ZERADOS_EXCESSIVOS = \"QLD001\"\n",
    "    VALORES_ARREDONDADOS_SUSPEITOS = \"QLD002\"\n",
    "    PADROES_REPETITIVOS = \"QLD003\"\n",
    "    DADOS_TESTE_PRODUCAO = \"QLD004\"\n",
    "    \n",
    "    # Layout Cr√≠tico\n",
    "    COMPRIMENTO_LINHA_INVALIDO = \"LAY001\"\n",
    "    CARACTERES_INVALIDOS = \"LAY002\"\n",
    "    ENCODING_INCORRETO = \"LAY003\"\n",
    "    FORMATO_ARQUIVO_INVALIDO = \"LAY004\"\n",
    "    \n",
    "    # S-5011 e DCTF-Web\n",
    "    S5011_TOTALIZACAO_DIVERGENTE = \"S5011_001\"\n",
    "    S5011_EVENTO_AUSENTE = \"S5011_002\"\n",
    "    S5011_BASE_CALCULO_INCONSISTENTE = \"S5011_003\"\n",
    "    S5011_CATEGORIAS_DIVERGENTES = \"S5011_004\"\n",
    "    DCTF_WEB_DIVERGENCIA = \"DCTF001\"\n",
    "    DCTF_WEB_PERIODO_AUSENTE = \"DCTF002\"\n",
    "    \n",
    "    # S-1299 Recibo\n",
    "    S1299_RECIBO_INVALIDO = \"S1299_001\"\n",
    "    S1299_RECIBO_AUSENTE = \"S1299_002\"\n",
    "    S1299_FORMATO_INVALIDO = \"S1299_003\"\n",
    "    \n",
    "    # Empresas Sem Movimento\n",
    "    EMPRESA_SEM_MOVIMENTO_INVALIDA = \"ESM001\"\n",
    "    EMPRESA_COM_BASE_SEM_VINCULOS = \"ESM002\"\n",
    "    \n",
    "    # eSocial Dom√©stico\n",
    "    DOMESTICO_CATEGORIA_INVALIDA = \"DOM001\"\n",
    "    DOMESTICO_MULTIPLOS_EMPREGADOS = \"DOM002\"\n",
    "    DOMESTICO_BASE_ACIMA_LIMITE = \"DOM003\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Anomalia:\n",
    "    \"\"\"Classe para representar uma anomalia detectada\"\"\"\n",
    "    tipo: TipoAnomalia\n",
    "    severidade: str  # 'CRITICA', 'ALTA', 'MEDIA', 'BAIXA'\n",
    "    campo: str\n",
    "    valor: Any\n",
    "    descricao: str\n",
    "    linha: int\n",
    "    cnpj: str\n",
    "    periodo: str\n",
    "    sugestao_correcao: str = \"\"\n",
    "    impacto_financeiro: float = 0.0\n",
    "    probabilidade_ml: float = 0.0\n",
    "    algoritmo_detectou: List[str] = field(default_factory=list)\n",
    "\n",
    "\n",
    "class ValidadorS5011:\n",
    "    \"\"\"Validador espec√≠fico para evento S-5011 (Totaliza√ß√£o)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(f\"{__name__}.ValidadorS5011\")\n",
    "        \n",
    "    def validar_totalizacao(self, df: pd.DataFrame, dados_s5011: Optional[pd.DataFrame] = None) -> List[Anomalia]:\n",
    "        \"\"\"Valida totaliza√ß√£o contra evento S-5011\"\"\"\n",
    "        anomalias = []\n",
    "        \n",
    "        if dados_s5011 is None:\n",
    "            self.logger.warning(\"Dados S-5011 n√£o fornecidos - valida√ß√£o limitada\")\n",
    "            # Ainda podemos fazer valida√ß√µes internas\n",
    "            for idx, row in df.iterrows():\n",
    "                # Verificar se tem recibo S-1299\n",
    "                if pd.isna(row.get('NU_RECIBO_1299')) or str(row.get('NU_RECIBO_1299')).strip() == '':\n",
    "                    anomalias.append(Anomalia(\n",
    "                        tipo=TipoAnomalia.S1299_RECIBO_AUSENTE,\n",
    "                        severidade='CRITICA',\n",
    "                        campo='NU_RECIBO_1299',\n",
    "                        valor=row.get('NU_RECIBO_1299'),\n",
    "                        descricao='Recibo S-1299 ausente - evento S-5011 pode n√£o ter sido gerado',\n",
    "                        linha=row.get('linha_arquivo', idx),\n",
    "                        cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                        periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                        sugestao_correcao='Verificar se eventos S-1299 e S-5011 foram enviados corretamente'\n",
    "                    ))\n",
    "            return anomalias\n",
    "        \n",
    "        # Validar contra dados S-5011\n",
    "        self.logger.info(\"Iniciando valida√ß√£o cruzada com S-5011\")\n",
    "        \n",
    "        # Agrupar por estabelecimento e per√≠odo\n",
    "        for (cnpj, periodo), grupo in df.groupby(['NU_INSCRICAO_ESTABELECIM', 'NU_PERIODO_REFERENCIA']):\n",
    "            # Buscar correspondente no S-5011\n",
    "            s5011_match = dados_s5011[\n",
    "                (dados_s5011['cnpj'] == cnpj) & \n",
    "                (dados_s5011['periodo'] == periodo)\n",
    "            ]\n",
    "            \n",
    "            if s5011_match.empty:\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.S5011_EVENTO_AUSENTE,\n",
    "                    severidade='CRITICA',\n",
    "                    campo='S-5011',\n",
    "                    valor=f\"{cnpj}/{periodo}\",\n",
    "                    descricao=f'Evento S-5011 n√£o encontrado para CNPJ {cnpj} per√≠odo {periodo}',\n",
    "                    linha=grupo.iloc[0].get('linha_arquivo', 0),\n",
    "                    cnpj=cnpj,\n",
    "                    periodo=str(periodo),\n",
    "                    sugestao_correcao='Verificar se S-5011 foi gerado e enviado para este per√≠odo'\n",
    "                ))\n",
    "                continue\n",
    "            \n",
    "            # Validar totaliza√ß√µes\n",
    "            total_base_arquivo = grupo['VL_BASE_CALCULO_CONTRIB_PREV'].sum()\n",
    "            total_base_s5011 = s5011_match['base_calculo_total'].iloc[0]\n",
    "            \n",
    "            if abs(total_base_arquivo - total_base_s5011) > 0.01:\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.S5011_BASE_CALCULO_INCONSISTENTE,\n",
    "                    severidade='CRITICA',\n",
    "                    campo='VL_BASE_CALCULO_CONTRIB_PREV',\n",
    "                    valor=f\"Arquivo: {total_base_arquivo:.2f}, S-5011: {total_base_s5011:.2f}\",\n",
    "                    descricao=f'Base de c√°lculo divergente entre arquivo ({total_base_arquivo:.2f}) e S-5011 ({total_base_s5011:.2f})',\n",
    "                    linha=grupo.iloc[0].get('linha_arquivo', 0),\n",
    "                    cnpj=cnpj,\n",
    "                    periodo=str(periodo),\n",
    "                    sugestao_correcao='Recalcular bases e reenviar eventos',\n",
    "                    impacto_financeiro=abs(total_base_arquivo - total_base_s5011)\n",
    "                ))\n",
    "            \n",
    "            # Validar por categoria\n",
    "            for cat in layout_esocial.categorias_validas:\n",
    "                campo_base = f'VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_{cat}'\n",
    "                campo_qtd = f'QT_VINCULOS_CAT_{cat}'\n",
    "                \n",
    "                if campo_base in grupo.columns:\n",
    "                    base_arquivo = grupo[campo_base].sum()\n",
    "                    base_s5011 = s5011_match.get(f'base_cat_{cat}', pd.Series([0])).iloc[0]\n",
    "                    \n",
    "                    if abs(base_arquivo - base_s5011) > 0.01:\n",
    "                        anomalias.append(Anomalia(\n",
    "                            tipo=TipoAnomalia.S5011_CATEGORIAS_DIVERGENTES,\n",
    "                            severidade='ALTA',\n",
    "                            campo=campo_base,\n",
    "                            valor=f\"Arquivo: {base_arquivo:.2f}, S-5011: {base_s5011:.2f}\",\n",
    "                            descricao=f'Base categoria {cat} divergente',\n",
    "                            linha=grupo.iloc[0].get('linha_arquivo', 0),\n",
    "                            cnpj=cnpj,\n",
    "                            periodo=str(periodo),\n",
    "                            sugestao_correcao=f'Verificar c√°lculos da categoria {cat}'\n",
    "                        ))\n",
    "        \n",
    "        return anomalias\n",
    "\n",
    "\n",
    "class DetectorAnomaliasAvancado:\n",
    "    \"\"\"Detector de anomalias estado da arte com 60+ tipos\"\"\"\n",
    "    \n",
    "    def __init__(self, layout: LayoutESocialCompleto):\n",
    "        self.layout = layout\n",
    "        self.logger = logging.getLogger(f\"{__name__}.DetectorAnomalias\")\n",
    "        self.validador_s5011 = ValidadorS5011()\n",
    "        self.estatisticas_historicas = {}\n",
    "        self.cache_validacoes = {}\n",
    "        \n",
    "    def detectar_todas_anomalias(self, df: pd.DataFrame, \n",
    "                               dados_s5011: Optional[pd.DataFrame] = None,\n",
    "                               dados_dctf: Optional[pd.DataFrame] = None,\n",
    "                               usar_cache: bool = True) -> Dict[str, List[Anomalia]]:\n",
    "        \"\"\"\n",
    "        Detecta todas as anomalias no DataFrame\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame com dados eSocial\n",
    "            dados_s5011: DataFrame com dados S-5011 para valida√ß√£o cruzada\n",
    "            dados_dctf: DataFrame com dados DCTF-Web para valida√ß√£o\n",
    "            usar_cache: Se deve usar cache de valida√ß√µes\n",
    "            \n",
    "        Returns:\n",
    "            Dicion√°rio com anomalias agrupadas por tipo\n",
    "        \"\"\"\n",
    "        inicio = datetime.now()\n",
    "        self.logger.info(\"Iniciando detec√ß√£o completa de anomalias\")\n",
    "        self.logger.info(f\"Total de registros para an√°lise: {len(df):,}\")\n",
    "        \n",
    "        # Preparar estrutura de resultados\n",
    "        anomalias_por_tipo = defaultdict(list)\n",
    "        total_anomalias = 0\n",
    "        \n",
    "        # Executar detec√ß√µes em paralelo quando poss√≠vel\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            futures = {\n",
    "                executor.submit(self._detectar_anomalias_estruturais, df): \"estruturais\",\n",
    "                executor.submit(self._detectar_anomalias_negocio, df): \"negocio\",\n",
    "                executor.submit(self._detectar_anomalias_temporais, df): \"temporais\",\n",
    "                executor.submit(self._detectar_anomalias_estatisticas, df): \"estatisticas\",\n",
    "                executor.submit(self._detectar_duplicatas, df): \"duplicatas\",\n",
    "                executor.submit(self._detectar_anomalias_conformidade, df): \"conformidade\",\n",
    "                executor.submit(self._detectar_anomalias_qualidade, df): \"qualidade\",\n",
    "                executor.submit(self._detectar_anomalias_layout, df): \"layout\"\n",
    "            }\n",
    "            \n",
    "            # Adicionar valida√ß√µes especiais se dados dispon√≠veis\n",
    "            if dados_s5011 is not None:\n",
    "                futures[executor.submit(self.validador_s5011.validar_totalizacao, df, dados_s5011)] = \"s5011\"\n",
    "            \n",
    "            if dados_dctf is not None:\n",
    "                futures[executor.submit(self._validar_dctf_web, df, dados_dctf)] = \"dctf\"\n",
    "            \n",
    "            # Coletar resultados\n",
    "            for future in as_completed(futures):\n",
    "                tipo = futures[future]\n",
    "                try:\n",
    "                    anomalias = future.result()\n",
    "                    anomalias_por_tipo[tipo].extend(anomalias)\n",
    "                    total_anomalias += len(anomalias)\n",
    "                    self.logger.info(f\"Detectadas {len(anomalias)} anomalias do tipo {tipo}\")\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Erro ao detectar anomalias {tipo}: {e}\")\n",
    "        \n",
    "        # Adicionar an√°lise de empresas sem movimento\n",
    "        anomalias_sem_movimento = self._detectar_empresas_sem_movimento(df)\n",
    "        anomalias_por_tipo['sem_movimento'].extend(anomalias_sem_movimento)\n",
    "        total_anomalias += len(anomalias_sem_movimento)\n",
    "        \n",
    "        # An√°lise de eSocial dom√©stico\n",
    "        anomalias_domestico = self._detectar_anomalias_domestico(df)\n",
    "        anomalias_por_tipo['domestico'].extend(anomalias_domestico)\n",
    "        total_anomalias += len(anomalias_domestico)\n",
    "        \n",
    "        tempo_total = (datetime.now() - inicio).total_seconds()\n",
    "        self.logger.info(f\"Detec√ß√£o conclu√≠da em {tempo_total:.2f} segundos\")\n",
    "        self.logger.info(f\"Total de anomalias detectadas: {total_anomalias:,}\")\n",
    "        \n",
    "        # Gerar resumo\n",
    "        self._gerar_resumo_anomalias(anomalias_por_tipo)\n",
    "        \n",
    "        return dict(anomalias_por_tipo)\n",
    "    \n",
    "    def _detectar_anomalias_estruturais(self, df: pd.DataFrame) -> List[Anomalia]:\n",
    "        \"\"\"Detecta anomalias estruturais nos dados\"\"\"\n",
    "        anomalias = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            # Campos obrigat√≥rios vazios\n",
    "            campos_obrigatorios = ['NU_PERIODO_REFERENCIA', 'NU_INSCRICAO_ESTABELECIM', \n",
    "                                  'NU_INSCRICAO_EMPREGADOR', 'QT_VINCULOS']\n",
    "            \n",
    "            for campo in campos_obrigatorios:\n",
    "                if pd.isna(row.get(campo)) or str(row.get(campo)).strip() == '':\n",
    "                    anomalias.append(Anomalia(\n",
    "                        tipo=TipoAnomalia.CAMPO_OBRIGATORIO_VAZIO,\n",
    "                        severidade='CRITICA',\n",
    "                        campo=campo,\n",
    "                        valor=row.get(campo),\n",
    "                        descricao=f'Campo obrigat√≥rio {campo} est√° vazio',\n",
    "                        linha=row.get('linha_arquivo', idx),\n",
    "                        cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                        periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                        sugestao_correcao=f'Preencher campo {campo} com valor v√°lido'\n",
    "                    ))\n",
    "            \n",
    "            # CNPJ inv√°lido\n",
    "            if not row.get('CNPJ_VALIDO', True):\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.CNPJ_INVALIDO,\n",
    "                    severidade='CRITICA',\n",
    "                    campo='NU_INSCRICAO_ESTABELECIM',\n",
    "                    valor=row.get('NU_INSCRICAO_ESTABELECIM'),\n",
    "                    descricao='CNPJ inv√°lido (falha no M√≥dulo 11)',\n",
    "                    linha=row.get('linha_arquivo', idx),\n",
    "                    cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                    periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                    sugestao_correcao='Verificar digita√ß√£o do CNPJ e corrigir'\n",
    "                ))\n",
    "            \n",
    "            # Tipo de inscri√ß√£o inv√°lido\n",
    "            tipo_inscr = row.get('ID_TIPO_INSCR_ESTABELECIM')\n",
    "            if tipo_inscr not in self.layout.tipos_inscricao:\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.TIPO_INSCRICAO_INVALIDO,\n",
    "                    severidade='ALTA',\n",
    "                    campo='ID_TIPO_INSCR_ESTABELECIM',\n",
    "                    valor=tipo_inscr,\n",
    "                    descricao=f'Tipo de inscri√ß√£o {tipo_inscr} inv√°lido',\n",
    "                    linha=row.get('linha_arquivo', idx),\n",
    "                    cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                    periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                    sugestao_correcao=f'Usar um dos tipos v√°lidos: {list(self.layout.tipos_inscricao.keys())}'\n",
    "                ))\n",
    "            \n",
    "            # Per√≠odo em formato inv√°lido\n",
    "            periodo = str(row.get('NU_PERIODO_REFERENCIA', ''))\n",
    "            if not self._validar_formato_periodo(periodo):\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.PERIODO_FORMATO_INVALIDO,\n",
    "                    severidade='CRITICA',\n",
    "                    campo='NU_PERIODO_REFERENCIA',\n",
    "                    valor=periodo,\n",
    "                    descricao='Per√≠odo em formato inv√°lido (deve ser AAAAMM ou AAAA13)',\n",
    "                    linha=row.get('linha_arquivo', idx),\n",
    "                    cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                    periodo=periodo,\n",
    "                    sugestao_correcao='Usar formato AAAAMM (ex: 202401) ou AAAA13 para 13¬∫'\n",
    "                ))\n",
    "        \n",
    "        return anomalias\n",
    "    \n",
    "    def _detectar_anomalias_negocio(self, df: pd.DataFrame) -> List[Anomalia]:\n",
    "        \"\"\"Detecta anomalias de regras de neg√≥cio\"\"\"\n",
    "        anomalias = []\n",
    "        \n",
    "        # Verificar m√°ximo de registros por estabelecimento/ano\n",
    "        df['ANO'] = df['NU_PERIODO_REFERENCIA'].astype(str).str[:4]\n",
    "        registros_por_estab_ano = df.groupby(['NU_INSCRICAO_ESTABELECIM', 'ANO']).size()\n",
    "        \n",
    "        for (cnpj, ano), qtd in registros_por_estab_ano[registros_por_estab_ano > MAX_REGISTROS_ANO].items():\n",
    "            anomalias.append(Anomalia(\n",
    "                tipo=TipoAnomalia.MAX_REGISTROS_EXCEDIDO,\n",
    "                severidade='CRITICA',\n",
    "                campo='REGISTROS_ANO',\n",
    "                valor=qtd,\n",
    "                descricao=f'Estabelecimento {cnpj} tem {qtd} registros no ano {ano} (m√°ximo: {MAX_REGISTROS_ANO})',\n",
    "                linha=0,\n",
    "                cnpj=cnpj,\n",
    "                periodo=ano,\n",
    "                sugestao_correcao='Verificar duplica√ß√µes ou envios incorretos'\n",
    "            ))\n",
    "        \n",
    "        # Valida√ß√µes linha a linha\n",
    "        for idx, row in df.iterrows():\n",
    "            # FAP fora do intervalo\n",
    "            fap = row.get('VL_FATOR_ACIDENTARIO_PREV', 0)\n",
    "            if fap and (fap < 0.5 or fap > 2.0):\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.FAP_FORA_INTERVALO,\n",
    "                    severidade='ALTA',\n",
    "                    campo='VL_FATOR_ACIDENTARIO_PREV',\n",
    "                    valor=fap,\n",
    "                    descricao=f'FAP {fap} fora do intervalo v√°lido [0.5, 2.0]',\n",
    "                    linha=row.get('linha_arquivo', idx),\n",
    "                    cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                    periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                    sugestao_correcao='Verificar FAP correto no site da Previd√™ncia'\n",
    "                ))\n",
    "            \n",
    "            # Classifica√ß√£o tribut√°ria inv√°lida\n",
    "            class_trib = row.get('ID_CLASSIFICACAO_TRIBUTARIA')\n",
    "            if class_trib and class_trib not in self.layout.classificacoes_tributarias:\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.CLASSIFICACAO_TRIBUTARIA_INVALIDA,\n",
    "                    severidade='ALTA',\n",
    "                    campo='ID_CLASSIFICACAO_TRIBUTARIA',\n",
    "                    valor=class_trib,\n",
    "                    descricao=f'Classifica√ß√£o tribut√°ria {class_trib} inv√°lida',\n",
    "                    linha=row.get('linha_arquivo', idx),\n",
    "                    cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                    periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                    sugestao_correcao='Verificar classifica√ß√£o tribut√°ria correta da empresa'\n",
    "                ))\n",
    "            \n",
    "            # Al√≠quota GILRAT inv√°lida\n",
    "            gilrat = row.get('NU_ALIQUOTA_GILRAT')\n",
    "            if gilrat not in [1, 2, 3]:\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.ALIQUOTA_GILRAT_INVALIDA,\n",
    "                    severidade='ALTA',\n",
    "                    campo='NU_ALIQUOTA_GILRAT',\n",
    "                    valor=gilrat,\n",
    "                    descricao=f'Al√≠quota GILRAT {gilrat} inv√°lida (valores v√°lidos: 1, 2, 3)',\n",
    "                    linha=row.get('linha_arquivo', idx),\n",
    "                    cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                    periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                    sugestao_correcao='Verificar grau de risco da atividade (1%, 2% ou 3%)'\n",
    "                ))\n",
    "            \n",
    "            # Validar Simples Nacional\n",
    "            if class_trib in [1, 2, 3, 4]:  # Simples Nacional\n",
    "                if fap and fap != 1.0:\n",
    "                    anomalias.append(Anomalia(\n",
    "                        tipo=TipoAnomalia.SIMPLES_NACIONAL_INCONSISTENTE,\n",
    "                        severidade='MEDIA',\n",
    "                        campo='VL_FATOR_ACIDENTARIO_PREV',\n",
    "                        valor=fap,\n",
    "                        descricao='Empresa do Simples Nacional com FAP diferente de 1.0',\n",
    "                        linha=row.get('linha_arquivo', idx),\n",
    "                        cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                        periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                        sugestao_correcao='Simples Nacional deve ter FAP = 1.0'\n",
    "                    ))\n",
    "        \n",
    "        return anomalias\n",
    "    \n",
    "    def _detectar_anomalias_temporais(self, df: pd.DataFrame) -> List[Anomalia]:\n",
    "        \"\"\"Detecta anomalias temporais\"\"\"\n",
    "        anomalias = []\n",
    "        data_atual = datetime.now()\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            periodo = str(row.get('NU_PERIODO_REFERENCIA', ''))\n",
    "            \n",
    "            if len(periodo) >= 6:\n",
    "                try:\n",
    "                    ano = int(periodo[:4])\n",
    "                    mes = int(periodo[4:6])\n",
    "                    \n",
    "                    # Per√≠odo futuro\n",
    "                    if ano > data_atual.year or (ano == data_atual.year and mes > data_atual.month):\n",
    "                        anomalias.append(Anomalia(\n",
    "                            tipo=TipoAnomalia.PERIODO_FUTURO,\n",
    "                            severidade='ALTA',\n",
    "                            campo='NU_PERIODO_REFERENCIA',\n",
    "                            valor=periodo,\n",
    "                            descricao='Per√≠odo de refer√™ncia no futuro',\n",
    "                            linha=row.get('linha_arquivo', idx),\n",
    "                            cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                            periodo=periodo,\n",
    "                            sugestao_correcao='Verificar per√≠odo correto'\n",
    "                        ))\n",
    "                    \n",
    "                    # Per√≠odo muito antigo (mais de 5 anos)\n",
    "                    if ano < data_atual.year - 5:\n",
    "                        anomalias.append(Anomalia(\n",
    "                            tipo=TipoAnomalia.PERIODO_MUITO_ANTIGO,\n",
    "                            severidade='MEDIA',\n",
    "                            campo='NU_PERIODO_REFERENCIA',\n",
    "                            valor=periodo,\n",
    "                            descricao=f'Per√≠odo muito antigo ({ano})',\n",
    "                            linha=row.get('linha_arquivo', idx),\n",
    "                            cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                            periodo=periodo,\n",
    "                            sugestao_correcao='Verificar se √© retifica√ß√£o ou per√≠odo correto'\n",
    "                        ))\n",
    "                    \n",
    "                    # Per√≠odo 13 inv√°lido\n",
    "                    if mes == 13 and mes != 13:  # Se indicado m√™s 13 mas n√£o √© per√≠odo 13\n",
    "                        anomalias.append(Anomalia(\n",
    "                            tipo=TipoAnomalia.PERIODO_13_INVALIDO,\n",
    "                            severidade='ALTA',\n",
    "                            campo='NU_PERIODO_REFERENCIA',\n",
    "                            valor=periodo,\n",
    "                            descricao='Per√≠odo 13 usado incorretamente',\n",
    "                            linha=row.get('linha_arquivo', idx),\n",
    "                            cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                            periodo=periodo,\n",
    "                            sugestao_correcao='Per√≠odo 13 deve ser usado apenas para 13¬∫ sal√°rio'\n",
    "                        ))\n",
    "                    \n",
    "                except ValueError:\n",
    "                    pass  # Erro j√° tratado em valida√ß√£o estrutural\n",
    "            \n",
    "            # Validar data/hora do evento\n",
    "            dt_evento = str(row.get('DT_EVENTO_CONTRIBUINTE', ''))\n",
    "            if dt_evento and len(dt_evento) == 14:\n",
    "                try:\n",
    "                    dt = datetime.strptime(dt_evento, '%Y%m%d%H%M%S')\n",
    "                    \n",
    "                    # Data evento no futuro\n",
    "                    if dt > data_atual:\n",
    "                        anomalias.append(Anomalia(\n",
    "                            tipo=TipoAnomalia.DATA_EVENTO_INCONSISTENTE,\n",
    "                            severidade='MEDIA',\n",
    "                            campo='DT_EVENTO_CONTRIBUINTE',\n",
    "                            valor=dt_evento,\n",
    "                            descricao='Data/hora do evento no futuro',\n",
    "                            linha=row.get('linha_arquivo', idx),\n",
    "                            cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                            periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                            sugestao_correcao='Verificar data/hora correta do evento'\n",
    "                        ))\n",
    "                    \n",
    "                    # Data evento muito antiga\n",
    "                    if dt < data_atual - timedelta(days=365*5):\n",
    "                        anomalias.append(Anomalia(\n",
    "                            tipo=TipoAnomalia.DATA_EVENTO_INCONSISTENTE,\n",
    "                            severidade='BAIXA',\n",
    "                            campo='DT_EVENTO_CONTRIBUINTE',\n",
    "                            valor=dt_evento,\n",
    "                            descricao='Data/hora do evento muito antiga',\n",
    "                            linha=row.get('linha_arquivo', idx),\n",
    "                            cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                            periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                            sugestao_correcao='Verificar se √© retifica√ß√£o'\n",
    "                        ))\n",
    "                        \n",
    "                except ValueError:\n",
    "                    anomalias.append(Anomalia(\n",
    "                        tipo=TipoAnomalia.DATA_HORA_INVALIDA,\n",
    "                        severidade='ALTA',\n",
    "                        campo='DT_EVENTO_CONTRIBUINTE',\n",
    "                        valor=dt_evento,\n",
    "                        descricao='Data/hora em formato inv√°lido',\n",
    "                        linha=row.get('linha_arquivo', idx),\n",
    "                        cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                        periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                        sugestao_correcao='Usar formato AAAAMMDDHHMMSS'\n",
    "                    ))\n",
    "        \n",
    "        # Verificar sequ√™ncia temporal\n",
    "        anomalias.extend(self._verificar_sequencia_temporal(df))\n",
    "        \n",
    "        return anomalias\n",
    "    \n",
    "    def _detectar_anomalias_estatisticas(self, df: pd.DataFrame) -> List[Anomalia]:\n",
    "        \"\"\"Detecta anomalias estat√≠sticas usando m√©todos avan√ßados\"\"\"\n",
    "        anomalias = []\n",
    "        \n",
    "        # Campos num√©ricos para an√°lise\n",
    "        campos_analise = [\n",
    "            'QT_VINCULOS', 'QT_ADMISSOES', 'QT_RESCISOES',\n",
    "            'VL_BASE_CALCULO_CONTRIB_PREV'\n",
    "        ]\n",
    "        \n",
    "        for campo in campos_analise:\n",
    "            if campo not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            # Remover zeros e nulos para an√°lise\n",
    "            valores = df[df[campo] > 0][campo]\n",
    "            \n",
    "            if len(valores) < 10:  # Precisa de dados suficientes\n",
    "                continue\n",
    "            \n",
    "            # Calcular estat√≠sticas\n",
    "            q1 = valores.quantile(0.25)\n",
    "            q3 = valores.quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            limite_inferior = q1 - 3 * iqr\n",
    "            limite_superior = q3 + 3 * iqr\n",
    "            \n",
    "            # Z-score\n",
    "            media = valores.mean()\n",
    "            desvio = valores.std()\n",
    "            \n",
    "            # Detectar outliers\n",
    "            for idx, row in df.iterrows():\n",
    "                valor = row.get(campo, 0)\n",
    "                \n",
    "                if valor <= 0:\n",
    "                    continue\n",
    "                \n",
    "                # IQR method\n",
    "                if valor < limite_inferior or valor > limite_superior:\n",
    "                    z_score = abs((valor - media) / desvio) if desvio > 0 else 0\n",
    "                    \n",
    "                    tipo_anomalia = {\n",
    "                        'QT_VINCULOS': TipoAnomalia.OUTLIER_VINCULOS,\n",
    "                        'QT_ADMISSOES': TipoAnomalia.OUTLIER_ADMISSOES,\n",
    "                        'QT_RESCISOES': TipoAnomalia.OUTLIER_RESCISOES,\n",
    "                        'VL_BASE_CALCULO_CONTRIB_PREV': TipoAnomalia.OUTLIER_BASE_CALCULO\n",
    "                    }.get(campo, TipoAnomalia.OUTLIER_VINCULOS)\n",
    "                    \n",
    "                    anomalias.append(Anomalia(\n",
    "                        tipo=tipo_anomalia,\n",
    "                        severidade='MEDIA' if z_score < 4 else 'ALTA',\n",
    "                        campo=campo,\n",
    "                        valor=valor,\n",
    "                        descricao=f'Valor outlier detectado (Z-score: {z_score:.2f})',\n",
    "                        linha=row.get('linha_arquivo', idx),\n",
    "                        cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                        periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                        sugestao_correcao='Verificar se valor est√° correto',\n",
    "                        probabilidade_ml=min(0.95, z_score / 10)\n",
    "                    ))\n",
    "        \n",
    "        # Detectar varia√ß√µes bruscas\n",
    "        anomalias.extend(self._detectar_variacoes_bruscas(df))\n",
    "        \n",
    "        # Detectar padr√µes sazonais anormais\n",
    "        anomalias.extend(self._detectar_padroes_sazonais(df))\n",
    "        \n",
    "        return anomalias\n",
    "    \n",
    "    def _detectar_duplicatas(self, df: pd.DataFrame) -> List[Anomalia]:\n",
    "        \"\"\"Detecta registros duplicados\"\"\"\n",
    "        anomalias = []\n",
    "        \n",
    "        # Duplicatas exatas\n",
    "        duplicatas = df[df.duplicated(keep=False)]\n",
    "        if not duplicatas.empty:\n",
    "            for idx, row in duplicatas.iterrows():\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.REGISTRO_DUPLICADO,\n",
    "                    severidade='ALTA',\n",
    "                    campo='REGISTRO_COMPLETO',\n",
    "                    valor='Duplicata exata',\n",
    "                    descricao='Registro completamente duplicado',\n",
    "                    linha=row.get('linha_arquivo', idx),\n",
    "                    cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                    periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                    sugestao_correcao='Remover registro duplicado'\n",
    "                ))\n",
    "        \n",
    "        # CNPJ duplicado no mesmo per√≠odo\n",
    "        chaves = ['NU_INSCRICAO_ESTABELECIM', 'NU_PERIODO_REFERENCIA']\n",
    "        duplicatas_periodo = df[df.duplicated(subset=chaves, keep=False)]\n",
    "        \n",
    "        if not duplicatas_periodo.empty:\n",
    "            for (cnpj, periodo), grupo in duplicatas_periodo.groupby(chaves):\n",
    "                if len(grupo) > 1:\n",
    "                    for idx, row in grupo.iterrows():\n",
    "                        anomalias.append(Anomalia(\n",
    "                            tipo=TipoAnomalia.CNPJ_DUPLICADO_PERIODO,\n",
    "                            severidade='CRITICA',\n",
    "                            campo='CNPJ_PERIODO',\n",
    "                            valor=f'{cnpj}/{periodo}',\n",
    "                            descricao=f'CNPJ {cnpj} duplicado no per√≠odo {periodo}',\n",
    "                            linha=row.get('linha_arquivo', idx),\n",
    "                            cnpj=cnpj,\n",
    "                            periodo=str(periodo),\n",
    "                            sugestao_correcao='Manter apenas um registro por CNPJ/per√≠odo'\n",
    "                        ))\n",
    "        \n",
    "        # Recibo S-1299 duplicado\n",
    "        recibos = df[df['NU_RECIBO_1299'].notna()]['NU_RECIBO_1299']\n",
    "        recibos_duplicados = recibos[recibos.duplicated(keep=False)]\n",
    "        \n",
    "        if not recibos_duplicados.empty:\n",
    "            for recibo in recibos_duplicados.unique():\n",
    "                registros_recibo = df[df['NU_RECIBO_1299'] == recibo]\n",
    "                for idx, row in registros_recibo.iterrows():\n",
    "                    anomalias.append(Anomalia(\n",
    "                        tipo=TipoAnomalia.RECIBO_1299_DUPLICADO,\n",
    "                        severidade='ALTA',\n",
    "                        campo='NU_RECIBO_1299',\n",
    "                        valor=recibo,\n",
    "                        descricao=f'Recibo S-1299 {recibo} duplicado',\n",
    "                        linha=row.get('linha_arquivo', idx),\n",
    "                        cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                        periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                        sugestao_correcao='Verificar recibo correto para cada registro'\n",
    "                    ))\n",
    "        \n",
    "        return anomalias\n",
    "    \n",
    "    def _detectar_anomalias_conformidade(self, df: pd.DataFrame) -> List[Anomalia]:\n",
    "        \"\"\"Detecta anomalias de conformidade e consist√™ncia\"\"\"\n",
    "        anomalias = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            # Soma das rescis√µes por motivo deve ser igual ao total\n",
    "            qt_rescisoes_total = row.get('QT_RESCISOES', 0)\n",
    "            soma_rescisoes = sum([\n",
    "                row.get(f'QT_RESCISOES_MOTIVO_{motivo}', 0) \n",
    "                for motivo in self.layout.motivos_rescisao\n",
    "            ])\n",
    "            \n",
    "            if qt_rescisoes_total != soma_rescisoes:\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.SOMA_RESCISOES_DIVERGENTE,\n",
    "                    severidade='ALTA',\n",
    "                    campo='QT_RESCISOES',\n",
    "                    valor=f'Total: {qt_rescisoes_total}, Soma: {soma_rescisoes}',\n",
    "                    descricao='Soma das rescis√µes por motivo diferente do total',\n",
    "                    linha=row.get('linha_arquivo', idx),\n",
    "                    cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                    periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                    sugestao_correcao='Recalcular totais de rescis√µes'\n",
    "                ))\n",
    "            \n",
    "            # Soma dos v√≠nculos por categoria\n",
    "            qt_vinculos_total = row.get('QT_VINCULOS', 0)\n",
    "            soma_vinculos = sum([\n",
    "                row.get(f'QT_VINCULOS_CAT_{cat}', 0) \n",
    "                for cat in self.layout.categorias_validas\n",
    "            ])\n",
    "            \n",
    "            if abs(qt_vinculos_total - soma_vinculos) > 1:  # Toler√¢ncia de 1\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.SOMA_VINCULOS_DIVERGENTE,\n",
    "                    severidade='ALTA',\n",
    "                    campo='QT_VINCULOS',\n",
    "                    valor=f'Total: {qt_vinculos_total}, Soma: {soma_vinculos}',\n",
    "                    descricao='Soma dos v√≠nculos por categoria diferente do total',\n",
    "                    linha=row.get('linha_arquivo', idx),\n",
    "                    cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                    periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                    sugestao_correcao='Verificar quantidades por categoria'\n",
    "                ))\n",
    "            \n",
    "            # Soma das bases de c√°lculo\n",
    "            base_total = row.get('VL_BASE_CALCULO_CONTRIB_PREV', 0)\n",
    "            soma_bases = sum([\n",
    "                row.get(f'VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_{cat}', 0)\n",
    "                for cat in self.layout.categorias_validas\n",
    "            ])\n",
    "            \n",
    "            if base_total > 0 and abs(base_total - soma_bases) > 0.01:\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.SOMA_BASE_CALCULO_DIVERGENTE,\n",
    "                    severidade='CRITICA',\n",
    "                    campo='VL_BASE_CALCULO_CONTRIB_PREV',\n",
    "                    valor=f'Total: {base_total:.2f}, Soma: {soma_bases:.2f}',\n",
    "                    descricao='Soma das bases por categoria diferente do total',\n",
    "                    linha=row.get('linha_arquivo', idx),\n",
    "                    cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                    periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                    sugestao_correcao='Recalcular bases de c√°lculo',\n",
    "                    impacto_financeiro=abs(base_total - soma_bases)\n",
    "                ))\n",
    "            \n",
    "            # Mais rescis√µes que v√≠nculos (imposs√≠vel)\n",
    "            if qt_rescisoes_total > qt_vinculos_total + row.get('QT_ADMISSOES', 0):\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.MAIS_RESCISOES_QUE_VINCULOS,\n",
    "                    severidade='CRITICA',\n",
    "                    campo='QT_RESCISOES',\n",
    "                    valor=f'Rescis√µes: {qt_rescisoes_total}, V√≠nculos+Admiss√µes: {qt_vinculos_total + row.get(\"QT_ADMISSOES\", 0)}',\n",
    "                    descricao='Quantidade de rescis√µes maior que v√≠nculos + admiss√µes',\n",
    "                    linha=row.get('linha_arquivo', idx),\n",
    "                    cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                    periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                    sugestao_correcao='Verificar quantidades corretas'\n",
    "                ))\n",
    "            \n",
    "            # Base zerada com v√≠nculos\n",
    "            if qt_vinculos_total > 0 and base_total == 0:\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.BASE_CALCULO_ZERADA_COM_VINCULOS,\n",
    "                    severidade='ALTA',\n",
    "                    campo='VL_BASE_CALCULO_CONTRIB_PREV',\n",
    "                    valor=base_total,\n",
    "                    descricao=f'Base de c√°lculo zerada com {qt_vinculos_total} v√≠nculos',\n",
    "                    linha=row.get('linha_arquivo', idx),\n",
    "                    cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                    periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                    sugestao_correcao='Verificar se h√° remunera√ß√µes n√£o informadas'\n",
    "                ))\n",
    "            \n",
    "            # V√≠nculos sem base de c√°lculo correspondente\n",
    "            for cat in self.layout.categorias_validas:\n",
    "                qt_cat = row.get(f'QT_VINCULOS_CAT_{cat}', 0)\n",
    "                base_cat = row.get(f'VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_{cat}', 0)\n",
    "                \n",
    "                if qt_cat > 0 and base_cat == 0:\n",
    "                    # Algumas categorias podem n√£o ter base (ex: afastados)\n",
    "                    if cat not in [301, 302, 303, 304, 306, 309]:  # Contribuintes individuais\n",
    "                        anomalias.append(Anomalia(\n",
    "                            tipo=TipoAnomalia.VINCULOS_SEM_BASE_CALCULO,\n",
    "                            severidade='MEDIA',\n",
    "                            campo=f'VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_{cat}',\n",
    "                            valor=f'V√≠nculos: {qt_cat}, Base: {base_cat}',\n",
    "                            descricao=f'Categoria {cat} com {qt_cat} v√≠nculos mas sem base de c√°lculo',\n",
    "                            linha=row.get('linha_arquivo', idx),\n",
    "                            cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                            periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                            sugestao_correcao=f'Verificar remunera√ß√µes da categoria {cat}'\n",
    "                        ))\n",
    "        \n",
    "        return anomalias\n",
    "    \n",
    "    def _detectar_anomalias_qualidade(self, df: pd.DataFrame) -> List[Anomalia]:\n",
    "        \"\"\"Detecta anomalias de qualidade dos dados\"\"\"\n",
    "        anomalias = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            # Campos zerados excessivos\n",
    "            campos_numericos = [col for col in df.columns if col.startswith(('QT_', 'VL_'))]\n",
    "            zeros = sum(1 for campo in campos_numericos if row.get(campo, 0) == 0)\n",
    "            \n",
    "            if zeros > len(campos_numericos) * 0.8:  # Mais de 80% zerados\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.CAMPOS_ZERADOS_EXCESSIVOS,\n",
    "                    severidade='MEDIA',\n",
    "                    campo='DADOS_GERAIS',\n",
    "                    valor=f'{zeros}/{len(campos_numericos)} campos zerados',\n",
    "                    descricao='Excesso de campos com valor zero',\n",
    "                    linha=row.get('linha_arquivo', idx),\n",
    "                    cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                    periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                    sugestao_correcao='Verificar se dados est√£o completos'\n",
    "                ))\n",
    "            \n",
    "            # Valores arredondados suspeitos\n",
    "            base_calc = row.get('VL_BASE_CALCULO_CONTRIB_PREV', 0)\n",
    "            if base_calc > 10000 and base_calc % 1000 == 0:\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.VALORES_ARREDONDADOS_SUSPEITOS,\n",
    "                    severidade='BAIXA',\n",
    "                    campo='VL_BASE_CALCULO_CONTRIB_PREV',\n",
    "                    valor=base_calc,\n",
    "                    descricao='Base de c√°lculo com valor muito arredondado',\n",
    "                    linha=row.get('linha_arquivo', idx),\n",
    "                    cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                    periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                    sugestao_correcao='Verificar se c√°lculo est√° correto'\n",
    "                ))\n",
    "            \n",
    "            # Padr√µes repetitivos (todos os valores iguais)\n",
    "            valores_rescisao = [row.get(f'QT_RESCISOES_MOTIVO_{m}', 0) for m in self.layout.motivos_rescisao]\n",
    "            valores_unicos = set(v for v in valores_rescisao if v > 0)\n",
    "            \n",
    "            if len(valores_unicos) == 1 and sum(valores_rescisao) > 10:\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.PADROES_REPETITIVOS,\n",
    "                    severidade='MEDIA',\n",
    "                    campo='QT_RESCISOES_MOTIVO',\n",
    "                    valor=f'Todos os motivos com valor {valores_unicos.pop()}',\n",
    "                    descricao='Padr√£o repetitivo suspeito nas rescis√µes',\n",
    "                    linha=row.get('linha_arquivo', idx),\n",
    "                    cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                    periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                    sugestao_correcao='Verificar distribui√ß√£o real das rescis√µes'\n",
    "                ))\n",
    "            \n",
    "            # Dados de teste em produ√ß√£o\n",
    "            cnpj = str(row.get('NU_INSCRICAO_ESTABELECIM', ''))\n",
    "            if any(test in cnpj for test in ['11111111', '99999999', '12345678']):\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.DADOS_TESTE_PRODUCAO,\n",
    "                    severidade='ALTA',\n",
    "                    campo='NU_INSCRICAO_ESTABELECIM',\n",
    "                    valor=cnpj,\n",
    "                    descricao='CNPJ de teste em ambiente de produ√ß√£o',\n",
    "                    linha=row.get('linha_arquivo', idx),\n",
    "                    cnpj=cnpj,\n",
    "                    periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                    sugestao_correcao='Remover dados de teste'\n",
    "                ))\n",
    "        \n",
    "        return anomalias\n",
    "    \n",
    "    def _detectar_anomalias_layout(self, df: pd.DataFrame) -> List[Anomalia]:\n",
    "        \"\"\"Detecta anomalias de layout do arquivo\"\"\"\n",
    "        anomalias = []\n",
    "        \n",
    "        # Esta valida√ß√£o seria feita durante o parse, mas podemos verificar resultados\n",
    "        if 'linha_arquivo' in df.columns:\n",
    "            # Verificar se h√° linhas com comprimento incorreto marcadas\n",
    "            linhas_problema = df[df.get('layout_erro', False) == True]\n",
    "            \n",
    "            for idx, row in linhas_problema.iterrows():\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.COMPRIMENTO_LINHA_INVALIDO,\n",
    "                    severidade='CRITICA',\n",
    "                    campo='LINHA',\n",
    "                    valor=row.get('linha_arquivo'),\n",
    "                    descricao='Linha com comprimento incorreto',\n",
    "                    linha=row.get('linha_arquivo', idx),\n",
    "                    cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                    periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                    sugestao_correcao='Verificar formato do arquivo'\n",
    "                ))\n",
    "        \n",
    "        return anomalias\n",
    "    \n",
    "    def _detectar_empresas_sem_movimento(self, df: pd.DataFrame) -> List[Anomalia]:\n",
    "        \"\"\"Detecta anomalias espec√≠ficas de empresas sem movimento\"\"\"\n",
    "        anomalias = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            qt_vinculos = row.get('QT_VINCULOS', 0)\n",
    "            qt_admissoes = row.get('QT_ADMISSOES', 0)\n",
    "            qt_rescisoes = row.get('QT_RESCISOES', 0)\n",
    "            base_total = row.get('VL_BASE_CALCULO_CONTRIB_PREV', 0)\n",
    "            \n",
    "            # Empresa sem movimento deve ter tudo zerado\n",
    "            if (qt_vinculos == 0 and qt_admissoes == 0 and qt_rescisoes == 0):\n",
    "                # Verificar se tem base de c√°lculo\n",
    "                if base_total > 0:\n",
    "                    anomalias.append(Anomalia(\n",
    "                        tipo=TipoAnomalia.EMPRESA_COM_BASE_SEM_VINCULOS,\n",
    "                        severidade='CRITICA',\n",
    "                        campo='VL_BASE_CALCULO_CONTRIB_PREV',\n",
    "                        valor=base_total,\n",
    "                        descricao='Empresa sem v√≠nculos mas com base de c√°lculo',\n",
    "                        linha=row.get('linha_arquivo', idx),\n",
    "                        cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                        periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                        sugestao_correcao='Verificar se h√° v√≠nculos n√£o informados'\n",
    "                    ))\n",
    "                \n",
    "                # Verificar se tem rescis√µes por motivo\n",
    "                tem_rescisao_motivo = any(\n",
    "                    row.get(f'QT_RESCISOES_MOTIVO_{m}', 0) > 0 \n",
    "                    for m in self.layout.motivos_rescisao\n",
    "                )\n",
    "                \n",
    "                if tem_rescisao_motivo:\n",
    "                    anomalias.append(Anomalia(\n",
    "                        tipo=TipoAnomalia.EMPRESA_SEM_MOVIMENTO_INVALIDA,\n",
    "                        severidade='ALTA',\n",
    "                        campo='QT_RESCISOES_MOTIVO',\n",
    "                        valor='Rescis√µes informadas',\n",
    "                        descricao='Empresa sem movimento com rescis√µes informadas',\n",
    "                        linha=row.get('linha_arquivo', idx),\n",
    "                        cnpj=row.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "                        periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                        sugestao_correcao='Empresa sem movimento n√£o deve ter rescis√µes'\n",
    "                    ))\n",
    "        \n",
    "        return anomalias\n",
    "    \n",
    "    def _detectar_anomalias_domestico(self, df: pd.DataFrame) -> List[Anomalia]:\n",
    "        \"\"\"Detecta anomalias espec√≠ficas do eSocial dom√©stico\"\"\"\n",
    "        anomalias = []\n",
    "        \n",
    "        # Identificar poss√≠veis empregadores dom√©sticos (CPF como empregador)\n",
    "        df_domestico = df[df['ID_TIPO_INSCRICAO_EMP'] == 2]  # CPF\n",
    "        \n",
    "        for idx, row in df_domestico.iterrows():\n",
    "            # Dom√©stico s√≥ pode ter categoria 104\n",
    "            categorias_invalidas = []\n",
    "            for cat in self.layout.categorias_validas:\n",
    "                if cat != 104 and row.get(f'QT_VINCULOS_CAT_{cat}', 0) > 0:\n",
    "                    categorias_invalidas.append(cat)\n",
    "            \n",
    "            if categorias_invalidas:\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.DOMESTICO_CATEGORIA_INVALIDA,\n",
    "                    severidade='ALTA',\n",
    "                    campo='CATEGORIAS',\n",
    "                    valor=categorias_invalidas,\n",
    "                    descricao=f'Empregador dom√©stico com categorias inv√°lidas: {categorias_invalidas}',\n",
    "                    linha=row.get('linha_arquivo', idx),\n",
    "                    cnpj=row.get('NU_INSCRICAO_EMPREGADOR', ''),\n",
    "                    periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                    sugestao_correcao='Empregador dom√©stico s√≥ pode ter categoria 104'\n",
    "                ))\n",
    "            \n",
    "            # Dom√©stico n√£o pode ter muitos empregados\n",
    "            total_vinculos = row.get('QT_VINCULOS', 0)\n",
    "            if total_vinculos > 2:\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.DOMESTICO_MULTIPLOS_EMPREGADOS,\n",
    "                    severidade='ALTA',\n",
    "                    campo='QT_VINCULOS',\n",
    "                    valor=total_vinculos,\n",
    "                    descricao=f'Empregador dom√©stico com {total_vinculos} v√≠nculos',\n",
    "                    linha=row.get('linha_arquivo', idx),\n",
    "                    cnpj=row.get('NU_INSCRICAO_EMPREGADOR', ''),\n",
    "                    periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                    sugestao_correcao='Verificar se √© realmente empregador dom√©stico'\n",
    "                ))\n",
    "            \n",
    "            # Base de c√°lculo n√£o pode ser muito alta\n",
    "            base_total = row.get('VL_BASE_CALCULO_CONTRIB_PREV', 0)\n",
    "            if base_total > SALARIO_MINIMO_2024 * 10:  # 10 sal√°rios m√≠nimos\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.DOMESTICO_BASE_ACIMA_LIMITE,\n",
    "                    severidade='MEDIA',\n",
    "                    campo='VL_BASE_CALCULO_CONTRIB_PREV',\n",
    "                    valor=base_total,\n",
    "                    descricao=f'Base de c√°lculo muito alta para dom√©stico: R$ {base_total:.2f}',\n",
    "                    linha=row.get('linha_arquivo', idx),\n",
    "                    cnpj=row.get('NU_INSCRICAO_EMPREGADOR', ''),\n",
    "                    periodo=str(row.get('NU_PERIODO_REFERENCIA', '')),\n",
    "                    sugestao_correcao='Verificar se valores est√£o corretos'\n",
    "                ))\n",
    "        \n",
    "        return anomalias\n",
    "    \n",
    "    def _validar_dctf_web(self, df: pd.DataFrame, dados_dctf: pd.DataFrame) -> List[Anomalia]:\n",
    "        \"\"\"Valida dados contra DCTF-Web\"\"\"\n",
    "        anomalias = []\n",
    "        \n",
    "        self.logger.info(\"Iniciando valida√ß√£o cruzada com DCTF-Web\")\n",
    "        \n",
    "        # Agrupar por CNPJ e per√≠odo\n",
    "        for (cnpj, periodo), grupo in df.groupby(['NU_INSCRICAO_ESTABELECIM', 'NU_PERIODO_REFERENCIA']):\n",
    "            # Buscar no DCTF\n",
    "            dctf_match = dados_dctf[\n",
    "                (dados_dctf['cnpj'] == cnpj) & \n",
    "                (dados_dctf['periodo'] == periodo)\n",
    "            ]\n",
    "            \n",
    "            if dctf_match.empty:\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.DCTF_WEB_PERIODO_AUSENTE,\n",
    "                    severidade='ALTA',\n",
    "                    campo='DCTF_WEB',\n",
    "                    valor=f'{cnpj}/{periodo}',\n",
    "                    descricao=f'Per√≠odo n√£o encontrado na DCTF-Web',\n",
    "                    linha=grupo.iloc[0].get('linha_arquivo', 0),\n",
    "                    cnpj=cnpj,\n",
    "                    periodo=str(periodo),\n",
    "                    sugestao_correcao='Verificar se DCTF-Web foi entregue'\n",
    "                ))\n",
    "                continue\n",
    "            \n",
    "            # Comparar valores\n",
    "            base_esocial = grupo['VL_BASE_CALCULO_CONTRIB_PREV'].sum()\n",
    "            base_dctf = dctf_match['base_calculo'].iloc[0]\n",
    "            \n",
    "            if abs(base_esocial - base_dctf) > base_dctf * 0.01:  # Toler√¢ncia de 1%\n",
    "                anomalias.append(Anomalia(\n",
    "                    tipo=TipoAnomalia.DCTF_WEB_DIVERGENCIA,\n",
    "                    severidade='CRITICA',\n",
    "                    campo='VL_BASE_CALCULO_CONTRIB_PREV',\n",
    "                    valor=f'eSocial: {base_esocial:.2f}, DCTF: {base_dctf:.2f}',\n",
    "                    descricao='Diverg√™ncia entre eSocial e DCTF-Web',\n",
    "                    linha=grupo.iloc[0].get('linha_arquivo', 0),\n",
    "                    cnpj=cnpj,\n",
    "                    periodo=str(periodo),\n",
    "                    sugestao_correcao='Retificar eSocial ou DCTF-Web',\n",
    "                    impacto_financeiro=abs(base_esocial - base_dctf)\n",
    "                ))\n",
    "        \n",
    "        return anomalias\n",
    "    \n",
    "    def _validar_formato_periodo(self, periodo: str) -> bool:\n",
    "        \"\"\"Valida formato do per√≠odo\"\"\"\n",
    "        if not periodo or len(periodo) != 6:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            ano = int(periodo[:4])\n",
    "            mes = int(periodo[4:6])\n",
    "            \n",
    "            if ano < 2000 or ano > 2100:\n",
    "                return False\n",
    "            \n",
    "            if mes < 1 or (mes > 12 and mes != 13):\n",
    "                return False\n",
    "            \n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    \n",
    "    def _verificar_sequencia_temporal(self, df: pd.DataFrame) -> List[Anomalia]:\n",
    "        \"\"\"Verifica quebras na sequ√™ncia temporal\"\"\"\n",
    "        anomalias = []\n",
    "        \n",
    "        # Agrupar por estabelecimento\n",
    "        for cnpj, grupo in df.groupby('NU_INSCRICAO_ESTABELECIM'):\n",
    "            # Ordenar por per√≠odo\n",
    "            grupo_ordenado = grupo.sort_values('NU_PERIODO_REFERENCIA')\n",
    "            periodos = grupo_ordenado['NU_PERIODO_REFERENCIA'].astype(str).tolist()\n",
    "            \n",
    "            # Verificar sequ√™ncia\n",
    "            for i in range(1, len(periodos)):\n",
    "                periodo_anterior = periodos[i-1]\n",
    "                periodo_atual = periodos[i]\n",
    "                \n",
    "                if len(periodo_anterior) == 6 and len(periodo_atual) == 6:\n",
    "                    try:\n",
    "                        # Calcular diferen√ßa em meses\n",
    "                        ano_ant = int(periodo_anterior[:4])\n",
    "                        mes_ant = int(periodo_anterior[4:6])\n",
    "                        ano_atu = int(periodo_atual[:4])\n",
    "                        mes_atu = int(periodo_atual[4:6])\n",
    "                        \n",
    "                        # Ignorar per√≠odo 13\n",
    "                        if mes_ant == 13 or mes_atu == 13:\n",
    "                            continue\n",
    "                        \n",
    "                        meses_diferenca = (ano_atu - ano_ant) * 12 + (mes_atu - mes_ant)\n",
    "                        \n",
    "                        if meses_diferenca > 3:  # Mais de 3 meses de gap\n",
    "                            anomalias.append(Anomalia(\n",
    "                                tipo=TipoAnomalia.SEQUENCIA_TEMPORAL_QUEBRADA,\n",
    "                                severidade='MEDIA',\n",
    "                                campo='NU_PERIODO_REFERENCIA',\n",
    "                                valor=f'{periodo_anterior} ‚Üí {periodo_atual}',\n",
    "                                descricao=f'Gap de {meses_diferenca} meses na sequ√™ncia',\n",
    "                                linha=grupo_ordenado.iloc[i].get('linha_arquivo', 0),\n",
    "                                cnpj=cnpj,\n",
    "                                periodo=periodo_atual,\n",
    "                                sugestao_correcao='Verificar per√≠odos faltantes'\n",
    "                            ))\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "        \n",
    "        return anomalias\n",
    "    \n",
    "    def _detectar_variacoes_bruscas(self, df: pd.DataFrame) -> List[Anomalia]:\n",
    "        \"\"\"Detecta varia√ß√µes bruscas entre per√≠odos\"\"\"\n",
    "        anomalias = []\n",
    "        \n",
    "        # Campos para an√°lise de varia√ß√£o\n",
    "        campos_variacao = ['QT_VINCULOS', 'QT_ADMISSOES', 'QT_RESCISOES', \n",
    "                          'VL_BASE_CALCULO_CONTRIB_PREV']\n",
    "        \n",
    "        # Agrupar por estabelecimento\n",
    "        for cnpj, grupo in df.groupby('NU_INSCRICAO_ESTABELECIM'):\n",
    "            if len(grupo) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Ordenar por per√≠odo\n",
    "            grupo_ordenado = grupo.sort_values('NU_PERIODO_REFERENCIA')\n",
    "            \n",
    "            for campo in campos_variacao:\n",
    "                valores = grupo_ordenado[campo].values\n",
    "                \n",
    "                for i in range(1, len(valores)):\n",
    "                    if valores[i-1] > 0:  # Evitar divis√£o por zero\n",
    "                        variacao = abs((valores[i] - valores[i-1]) / valores[i-1])\n",
    "                        \n",
    "                        if variacao > 0.5:  # Varia√ß√£o maior que 50%\n",
    "                            anomalias.append(Anomalia(\n",
    "                                tipo=TipoAnomalia.VARIACAO_BRUSCA,\n",
    "                                severidade='MEDIA' if variacao < 1 else 'ALTA',\n",
    "                                campo=campo,\n",
    "                                valor=f'{valores[i-1]:.2f} ‚Üí {valores[i]:.2f} ({variacao*100:.1f}%)',\n",
    "                                descricao=f'Varia√ß√£o brusca de {variacao*100:.1f}% em {campo}',\n",
    "                                linha=grupo_ordenado.iloc[i].get('linha_arquivo', 0),\n",
    "                                cnpj=cnpj,\n",
    "                                periodo=str(grupo_ordenado.iloc[i]['NU_PERIODO_REFERENCIA']),\n",
    "                                sugestao_correcao='Verificar se varia√ß√£o √© justificada'\n",
    "                            ))\n",
    "        \n",
    "        return anomalias\n",
    "    \n",
    "    def _detectar_padroes_sazonais(self, df: pd.DataFrame) -> List[Anomalia]:\n",
    "        \"\"\"Detecta padr√µes sazonais anormais\"\"\"\n",
    "        anomalias = []\n",
    "        \n",
    "        # An√°lise sazonal requer pelo menos 24 meses\n",
    "        for cnpj, grupo in df.groupby('NU_INSCRICAO_ESTABELECIM'):\n",
    "            if len(grupo) < 24:\n",
    "                continue\n",
    "            \n",
    "            # Ordenar por per√≠odo\n",
    "            grupo_ordenado = grupo.sort_values('NU_PERIODO_REFERENCIA')\n",
    "            \n",
    "            # Extrair m√™s\n",
    "            grupo_ordenado['MES'] = grupo_ordenado['NU_PERIODO_REFERENCIA'].astype(str).str[4:6].astype(int)\n",
    "            \n",
    "            # Analisar admiss√µes por m√™s\n",
    "            admissoes_por_mes = grupo_ordenado.groupby('MES')['QT_ADMISSOES'].agg(['mean', 'std'])\n",
    "            \n",
    "            # Detectar meses com padr√£o anormal\n",
    "            for mes, stats in admissoes_por_mes.iterrows():\n",
    "                if stats['std'] > stats['mean'] * 2:  # Alta variabilidade\n",
    "                    registros_mes = grupo_ordenado[grupo_ordenado['MES'] == mes]\n",
    "                    \n",
    "                    for idx, row in registros_mes.iterrows():\n",
    "                        valor = row['QT_ADMISSOES']\n",
    "                        if abs(valor - stats['mean']) > 2 * stats['std']:\n",
    "                            anomalias.append(Anomalia(\n",
    "                                tipo=TipoAnomalia.PADRAO_SAZONAL_ANORMAL,\n",
    "                                severidade='BAIXA',\n",
    "                                campo='QT_ADMISSOES',\n",
    "                                valor=valor,\n",
    "                                descricao=f'Valor fora do padr√£o sazonal para m√™s {mes}',\n",
    "                                linha=row.get('linha_arquivo', idx),\n",
    "                                cnpj=cnpj,\n",
    "                                periodo=str(row['NU_PERIODO_REFERENCIA']),\n",
    "                                sugestao_correcao='Verificar se h√° sazonalidade at√≠pica'\n",
    "                            ))\n",
    "        \n",
    "        return anomalias\n",
    "    \n",
    "    def _gerar_resumo_anomalias(self, anomalias_por_tipo: Dict[str, List[Anomalia]]):\n",
    "        \"\"\"Gera resumo das anomalias detectadas\"\"\"\n",
    "        self.logger.info(\"=\"*80)\n",
    "        self.logger.info(\"RESUMO DE ANOMALIAS DETECTADAS\")\n",
    "        self.logger.info(\"=\"*80)\n",
    "        \n",
    "        total_geral = 0\n",
    "        criticas = 0\n",
    "        \n",
    "        for tipo, anomalias in anomalias_por_tipo.items():\n",
    "            if anomalias:\n",
    "                total_tipo = len(anomalias)\n",
    "                criticas_tipo = sum(1 for a in anomalias if a.severidade == 'CRITICA')\n",
    "                total_geral += total_tipo\n",
    "                criticas += criticas_tipo\n",
    "                \n",
    "                self.logger.info(f\"{tipo.upper()}: {total_tipo} anomalias ({criticas_tipo} cr√≠ticas)\")\n",
    "                \n",
    "                # Top 3 anomalias mais frequentes\n",
    "                contador = Counter(a.tipo.value for a in anomalias)\n",
    "                for tipo_anom, qtd in contador.most_common(3):\n",
    "                    self.logger.info(f\"  - {tipo_anom}: {qtd} ocorr√™ncias\")\n",
    "        \n",
    "        self.logger.info(\"=\"*80)\n",
    "        self.logger.info(f\"TOTAL GERAL: {total_geral} anomalias ({criticas} cr√≠ticas)\")\n",
    "        self.logger.info(\"=\"*80)\n",
    "\n",
    "\n",
    "# Criar inst√¢ncias\n",
    "detector_anomalias = DetectorAnomaliasAvancado(layout_esocial)\n",
    "\n",
    "logger.info(\"Detector de anomalias configurado com 60+ tipos de valida√ß√µes\")\n",
    "logger.info(\"Incluindo valida√ß√µes S-5011, S-1299, DCTF-Web e empresas sem movimento\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-17 09:21:36 | INFO     | ESocialAnalyzer | 995808357:<module>:621 | Sistema ML configurado com 7 algoritmos:\n",
      "2025-06-17 09:21:36 | INFO     | ESocialAnalyzer | 995808357:<module>:623 |   - isolation_forest: 20%\n",
      "2025-06-17 09:21:36 | INFO     | ESocialAnalyzer | 995808357:<module>:623 |   - local_outlier_factor: 18%\n",
      "2025-06-17 09:21:36 | INFO     | ESocialAnalyzer | 995808357:<module>:623 |   - one_class_svm: 15%\n",
      "2025-06-17 09:21:36 | INFO     | ESocialAnalyzer | 995808357:<module>:623 |   - elliptic_envelope: 12%\n",
      "2025-06-17 09:21:36 | INFO     | ESocialAnalyzer | 995808357:<module>:623 |   - dbscan: 10%\n",
      "2025-06-17 09:21:36 | INFO     | ESocialAnalyzer | 995808357:<module>:623 |   - autoencoder: 15%\n",
      "2025-06-17 09:21:36 | INFO     | ESocialAnalyzer | 995808357:<module>:623 |   - extended_isolation_forest: 10%\n",
      "2025-06-17 09:21:36 | INFO     | ESocialAnalyzer | 995808357:<module>:624 | Total de pesos: 100%\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 4: Sistema ML Estado da Arte com 7 Algoritmos e Autoencoder Neural\n",
    "\n",
    "class AutoencoderESocial(keras.Model):\n",
    "    \"\"\"Autoencoder Neural Profissional para detec√ß√£o de anomalias\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, encoding_dim: int = 32, dropout_rate: float = 0.2):\n",
    "        super(AutoencoderESocial, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = keras.Sequential([\n",
    "            layers.Input(shape=(input_dim,)),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(encoding_dim, activation='relu', name='encoding')\n",
    "        ])\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = keras.Sequential([\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(input_dim, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "    \n",
    "    def get_decoder(self):\n",
    "        return self.decoder\n",
    "\n",
    "\n",
    "class ExtendedIsolationForest:\n",
    "    \"\"\"Extended Isolation Forest - vers√£o melhorada do Isolation Forest\"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators: int = 100, max_samples: Union[int, float] = 'auto',\n",
    "                 contamination: float = 0.1, random_state: int = 42):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.contamination = contamination\n",
    "        self.random_state = random_state\n",
    "        self.models = []\n",
    "        \n",
    "    def fit(self, X: np.ndarray):\n",
    "        \"\"\"Treina m√∫ltiplos Isolation Forests com diferentes sementes\"\"\"\n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "        # Treinar m√∫ltiplos modelos com diferentes amostras\n",
    "        for i in range(5):  # 5 sub-modelos\n",
    "            model = IsolationForest(\n",
    "                n_estimators=self.n_estimators // 5,\n",
    "                max_samples=self.max_samples,\n",
    "                contamination=self.contamination,\n",
    "                random_state=self.random_state + i\n",
    "            )\n",
    "            \n",
    "            # Usar amostragem estratificada se poss√≠vel\n",
    "            if len(X) > 10000:\n",
    "                indices = np.random.choice(len(X), size=min(10000, len(X)), replace=False)\n",
    "                model.fit(X[indices])\n",
    "            else:\n",
    "                model.fit(X)\n",
    "            \n",
    "            self.models.append(model)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predi√ß√£o usando vota√ß√£o dos sub-modelos\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            pred = model.predict(X)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        # Vota√ß√£o majorit√°ria\n",
    "        predictions = np.array(predictions)\n",
    "        final_pred = []\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            votes = predictions[:, i]\n",
    "            anomaly_votes = np.sum(votes == -1)\n",
    "            normal_votes = np.sum(votes == 1)\n",
    "            \n",
    "            if anomaly_votes > normal_votes:\n",
    "                final_pred.append(-1)\n",
    "            else:\n",
    "                final_pred.append(1)\n",
    "        \n",
    "        return np.array(final_pred)\n",
    "    \n",
    "    def decision_function(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Fun√ß√£o de decis√£o m√©dia dos sub-modelos\"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            score = model.decision_function(X)\n",
    "            scores.append(score)\n",
    "        \n",
    "        return np.mean(scores, axis=0)\n",
    "\n",
    "\n",
    "class SistemaMLAnomalias:\n",
    "    \"\"\"Sistema de ML Estado da Arte com 7 algoritmos e vota√ß√£o ponderada\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(f\"{__name__}.SistemaML\")\n",
    "        self.scaler = RobustScaler()  # Mais robusto para outliers\n",
    "        self.pca = PCA(n_components=0.95, random_state=42)  # Preserva 95% vari√¢ncia\n",
    "        \n",
    "        # Pesos oficiais dos algoritmos (soma = 100%)\n",
    "        self.pesos_algoritmos = {\n",
    "            'isolation_forest': 0.20,\n",
    "            'local_outlier_factor': 0.18,\n",
    "            'one_class_svm': 0.15,\n",
    "            'elliptic_envelope': 0.12,\n",
    "            'dbscan': 0.10,\n",
    "            'autoencoder': 0.15,\n",
    "            'extended_isolation_forest': 0.10\n",
    "        }\n",
    "        \n",
    "        # Inicializar modelos\n",
    "        self.modelos = {}\n",
    "        self.autoencoder = None\n",
    "        self.historico_metricas = defaultdict(list)\n",
    "        self.threshold_autoencoder = None\n",
    "        \n",
    "    def preparar_features(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Prepara features para ML com engenharia avan√ßada\"\"\"\n",
    "        self.logger.info(\"Preparando features para Machine Learning...\")\n",
    "        \n",
    "        # Features num√©ricas b√°sicas\n",
    "        features_numericas = [\n",
    "            'QT_VINCULOS', 'QT_ADMISSOES', 'QT_RESCISOES',\n",
    "            'VL_BASE_CALCULO_CONTRIB_PREV', 'VL_FATOR_ACIDENTARIO_PREV',\n",
    "            'VL_ALIQUOTA_GILRAT_AJUST'\n",
    "        ]\n",
    "        \n",
    "        # Features de propor√ß√µes\n",
    "        df['PROP_ADMISSOES'] = df['QT_ADMISSOES'] / (df['QT_VINCULOS'] + 1)\n",
    "        df['PROP_RESCISOES'] = df['QT_RESCISOES'] / (df['QT_VINCULOS'] + 1)\n",
    "        df['TURNOVER'] = (df['QT_ADMISSOES'] + df['QT_RESCISOES']) / (df['QT_VINCULOS'] + 1)\n",
    "        \n",
    "        # Features de distribui√ß√£o de rescis√µes\n",
    "        motivos = layout_esocial.motivos_rescisao\n",
    "        for motivo in motivos:\n",
    "            col_name = f'QT_RESCISOES_MOTIVO_{motivo}'\n",
    "            if col_name in df.columns:\n",
    "                df[f'PROP_MOTIVO_{motivo}'] = df[col_name] / (df['QT_RESCISOES'] + 1)\n",
    "        \n",
    "        # Features de distribui√ß√£o de categorias\n",
    "        for cat in layout_esocial.categorias_validas:\n",
    "            col_vinc = f'QT_VINCULOS_CAT_{cat}'\n",
    "            col_base = f'VL_BASE_CALCULO_CONTRIB_PREV_CATEGORIA_SEGURADO_{cat}'\n",
    "            \n",
    "            if col_vinc in df.columns:\n",
    "                df[f'PROP_CAT_{cat}'] = df[col_vinc] / (df['QT_VINCULOS'] + 1)\n",
    "            \n",
    "            if col_base in df.columns and col_vinc in df.columns:\n",
    "                df[f'MEDIA_SAL_CAT_{cat}'] = df[col_base] / (df[col_vinc] + 1)\n",
    "        \n",
    "        # Features temporais\n",
    "        df['MES'] = pd.to_numeric(df['NU_PERIODO_REFERENCIA'].astype(str).str[4:6], errors='coerce')\n",
    "        df['ANO'] = pd.to_numeric(df['NU_PERIODO_REFERENCIA'].astype(str).str[:4], errors='coerce')\n",
    "        df['TRIMESTRE'] = ((df['MES'] - 1) // 3) + 1\n",
    "        df['IS_13_PERIODO'] = (df['MES'] == 13).astype(int)\n",
    "        \n",
    "        # Features de classifica√ß√£o tribut√°ria\n",
    "        df['IS_SIMPLES'] = df['ID_CLASSIFICACAO_TRIBUTARIA'].isin([1, 2, 3, 4]).astype(int)\n",
    "        df['IS_MEI'] = (df['ID_CLASSIFICACAO_TRIBUTARIA'] == 4).astype(int)\n",
    "        \n",
    "        # Features de anomalias conhecidas\n",
    "        df['TEM_BASE_ZERO'] = (df['VL_BASE_CALCULO_CONTRIB_PREV'] == 0).astype(int)\n",
    "        df['TEM_VINCULOS_SEM_BASE'] = ((df['QT_VINCULOS'] > 0) & (df['VL_BASE_CALCULO_CONTRIB_PREV'] == 0)).astype(int)\n",
    "        \n",
    "        # Selecionar todas as features criadas\n",
    "        todas_features = features_numericas + [col for col in df.columns if col.startswith(('PROP_', 'MEDIA_', 'IS_', 'TEM_'))]\n",
    "        todas_features.extend(['MES', 'ANO', 'TRIMESTRE', 'TURNOVER'])\n",
    "        \n",
    "        # Filtrar apenas colunas existentes\n",
    "        features_existentes = [f for f in todas_features if f in df.columns]\n",
    "        \n",
    "        # Criar matriz de features\n",
    "        X = df[features_existentes].fillna(0).values\n",
    "        \n",
    "        self.logger.info(f\"Total de features geradas: {X.shape[1]}\")\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def treinar_modelos(self, X: np.ndarray, usar_gpu: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Treina todos os 7 modelos de ML\"\"\"\n",
    "        self.logger.info(\"Iniciando treinamento dos 7 algoritmos de ML...\")\n",
    "        inicio = datetime.now()\n",
    "        \n",
    "        # Normalizar dados\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Redu√ß√£o de dimensionalidade se necess√°rio\n",
    "        if X_scaled.shape[1] > 50:\n",
    "            X_scaled = self.pca.fit_transform(X_scaled)\n",
    "            self.logger.info(f\"Dimens√µes reduzidas para: {X_scaled.shape[1]}\")\n",
    "        \n",
    "        # 1. Isolation Forest (20%)\n",
    "        self.logger.info(\"Treinando Isolation Forest...\")\n",
    "        self.modelos['isolation_forest'] = IsolationForest(\n",
    "            n_estimators=200,\n",
    "            max_samples='auto',\n",
    "            contamination=0.1,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        self.modelos['isolation_forest'].fit(X_scaled)\n",
    "        \n",
    "        # 2. Local Outlier Factor (18%)\n",
    "        self.logger.info(\"Treinando Local Outlier Factor...\")\n",
    "        self.modelos['local_outlier_factor'] = LocalOutlierFactor(\n",
    "            n_neighbors=20,\n",
    "            contamination=0.1,\n",
    "            novelty=True,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        self.modelos['local_outlier_factor'].fit(X_scaled)\n",
    "        \n",
    "        # 3. One-Class SVM (15%)\n",
    "        self.logger.info(\"Treinando One-Class SVM...\")\n",
    "        # Usar amostra para SVM se dataset muito grande\n",
    "        if len(X_scaled) > 10000:\n",
    "            indices = np.random.choice(len(X_scaled), 10000, replace=False)\n",
    "            X_svm = X_scaled[indices]\n",
    "        else:\n",
    "            X_svm = X_scaled\n",
    "            \n",
    "        self.modelos['one_class_svm'] = OneClassSVM(\n",
    "            kernel='rbf',\n",
    "            gamma='auto',\n",
    "            nu=0.1\n",
    "        )\n",
    "        self.modelos['one_class_svm'].fit(X_svm)\n",
    "        \n",
    "        # 4. Elliptic Envelope (12%)\n",
    "        self.logger.info(\"Treinando Elliptic Envelope...\")\n",
    "        # Usar apenas se dados n√£o muito grandes\n",
    "        if len(X_scaled) <= 50000:\n",
    "            self.modelos['elliptic_envelope'] = EllipticEnvelope(\n",
    "                contamination=0.1,\n",
    "                random_state=42\n",
    "            )\n",
    "            self.modelos['elliptic_envelope'].fit(X_scaled)\n",
    "        else:\n",
    "            self.logger.warning(\"Dataset muito grande para Elliptic Envelope - ser√° substitu√≠do por outro Isolation Forest\")\n",
    "            self.modelos['elliptic_envelope'] = IsolationForest(\n",
    "                n_estimators=100,\n",
    "                contamination=0.1,\n",
    "                random_state=43\n",
    "            )\n",
    "            self.modelos['elliptic_envelope'].fit(X_scaled)\n",
    "        \n",
    "        # 5. DBSCAN (10%)\n",
    "        self.logger.info(\"Treinando DBSCAN...\")\n",
    "        # Determinar eps automaticamente\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "        if len(X_scaled) > 5000:\n",
    "            sample_indices = np.random.choice(len(X_scaled), 5000, replace=False)\n",
    "            X_sample = X_scaled[sample_indices]\n",
    "        else:\n",
    "            X_sample = X_scaled\n",
    "            \n",
    "        neighbors = NearestNeighbors(n_neighbors=5)\n",
    "        neighbors_fit = neighbors.fit(X_sample)\n",
    "        distances, indices = neighbors_fit.kneighbors(X_sample)\n",
    "        distances = np.sort(distances[:, -1])\n",
    "        eps = distances[int(len(distances) * 0.9)]  # 90¬∫ percentil\n",
    "        \n",
    "        self.modelos['dbscan'] = DBSCAN(\n",
    "            eps=eps,\n",
    "            min_samples=5,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        # DBSCAN n√£o tem m√©todo predict, apenas fit_predict\n",
    "        self.dbscan_labels = self.modelos['dbscan'].fit_predict(X_scaled)\n",
    "        \n",
    "        # 6. Autoencoder Neural (15%)\n",
    "        self.logger.info(\"Treinando Autoencoder Neural com TensorFlow...\")\n",
    "        self._treinar_autoencoder(X_scaled, usar_gpu)\n",
    "        \n",
    "        # 7. Extended Isolation Forest (10%)\n",
    "        self.logger.info(\"Treinando Extended Isolation Forest...\")\n",
    "        self.modelos['extended_isolation_forest'] = ExtendedIsolationForest(\n",
    "            n_estimators=150,\n",
    "            contamination=0.1,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.modelos['extended_isolation_forest'].fit(X_scaled)\n",
    "        \n",
    "        tempo_total = (datetime.now() - inicio).total_seconds()\n",
    "        self.logger.info(f\"Treinamento conclu√≠do em {tempo_total:.2f} segundos\")\n",
    "        \n",
    "        # Calcular m√©tricas de valida√ß√£o\n",
    "        metricas = self._calcular_metricas_validacao(X_scaled)\n",
    "        \n",
    "        return {\n",
    "            'modelos_treinados': list(self.modelos.keys()),\n",
    "            'tempo_treinamento': tempo_total,\n",
    "            'features_utilizadas': X_scaled.shape[1],\n",
    "            'amostras_treino': X_scaled.shape[0],\n",
    "            'metricas': metricas\n",
    "        }\n",
    "    \n",
    "    def _treinar_autoencoder(self, X: np.ndarray, usar_gpu: bool = True):\n",
    "        \"\"\"Treina o Autoencoder Neural\"\"\"\n",
    "        # Configurar dispositivo\n",
    "        if usar_gpu and len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "            self.logger.info(\"Usando GPU para treinar Autoencoder\")\n",
    "        else:\n",
    "            self.logger.info(\"Usando CPU para treinar Autoencoder\")\n",
    "        \n",
    "        # Normalizar dados para [0, 1] para camada sigmoid\n",
    "        X_norm = MinMaxScaler().fit_transform(X)\n",
    "        \n",
    "        # Dividir dados\n",
    "        X_train, X_val = train_test_split(X_norm, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Criar modelo\n",
    "        input_dim = X.shape[1]\n",
    "        encoding_dim = max(16, input_dim // 4)  # Dimens√£o do encoding\n",
    "        \n",
    "        self.autoencoder = AutoencoderESocial(input_dim, encoding_dim)\n",
    "        \n",
    "        # Compilar\n",
    "        self.autoencoder.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=0.0001\n",
    "        )\n",
    "        \n",
    "        # Treinar\n",
    "        history = self.autoencoder.fit(\n",
    "            X_train, X_train,\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            validation_data=(X_val, X_val),\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Calcular threshold para anomalias\n",
    "        train_predictions = self.autoencoder.predict(X_train, verbose=0)\n",
    "        mse = np.mean(np.power(X_train - train_predictions, 2), axis=1)\n",
    "        self.threshold_autoencoder = np.percentile(mse, 90)  # 90¬∫ percentil\n",
    "        \n",
    "        self.logger.info(f\"Autoencoder treinado - Loss final: {history.history['loss'][-1]:.4f}\")\n",
    "    \n",
    "    def detectar_anomalias_ml(self, X: np.ndarray, df_original: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Detecta anomalias usando vota√ß√£o ponderada dos 7 algoritmos\"\"\"\n",
    "        self.logger.info(\"Detectando anomalias com sistema ML...\")\n",
    "        \n",
    "        # Normalizar dados\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        # Aplicar PCA se foi usado no treino\n",
    "        if hasattr(self.pca, 'components_'):\n",
    "            X_scaled = self.pca.transform(X_scaled)\n",
    "        \n",
    "        # Coletar predi√ß√µes de cada algoritmo\n",
    "        predicoes = {}\n",
    "        scores = {}\n",
    "        \n",
    "        # 1. Isolation Forest\n",
    "        predicoes['isolation_forest'] = self.modelos['isolation_forest'].predict(X_scaled)\n",
    "        scores['isolation_forest'] = self.modelos['isolation_forest'].decision_function(X_scaled)\n",
    "        \n",
    "        # 2. Local Outlier Factor\n",
    "        predicoes['local_outlier_factor'] = self.modelos['local_outlier_factor'].predict(X_scaled)\n",
    "        scores['local_outlier_factor'] = self.modelos['local_outlier_factor'].decision_function(X_scaled)\n",
    "        \n",
    "        # 3. One-Class SVM\n",
    "        predicoes['one_class_svm'] = self.modelos['one_class_svm'].predict(X_scaled)\n",
    "        scores['one_class_svm'] = self.modelos['one_class_svm'].decision_function(X_scaled)\n",
    "        \n",
    "        # 4. Elliptic Envelope\n",
    "        predicoes['elliptic_envelope'] = self.modelos['elliptic_envelope'].predict(X_scaled)\n",
    "        scores['elliptic_envelope'] = self.modelos['elliptic_envelope'].decision_function(X_scaled)\n",
    "        \n",
    "        # 5. DBSCAN\n",
    "        # DBSCAN: -1 = outlier, outro = normal\n",
    "        if hasattr(self, 'dbscan_labels'):\n",
    "            predicoes['dbscan'] = np.where(self.dbscan_labels == -1, -1, 1)\n",
    "            scores['dbscan'] = np.where(self.dbscan_labels == -1, -1, 1).astype(float)\n",
    "        else:\n",
    "            # Re-executar DBSCAN\n",
    "            labels = self.modelos['dbscan'].fit_predict(X_scaled)\n",
    "            predicoes['dbscan'] = np.where(labels == -1, -1, 1)\n",
    "            scores['dbscan'] = predicoes['dbscan'].astype(float)\n",
    "        \n",
    "        # 6. Autoencoder\n",
    "        if self.autoencoder and self.threshold_autoencoder:\n",
    "            X_norm = MinMaxScaler().fit_transform(X_scaled)\n",
    "            reconstructed = self.autoencoder.predict(X_norm, verbose=0)\n",
    "            mse = np.mean(np.power(X_norm - reconstructed, 2), axis=1)\n",
    "            predicoes['autoencoder'] = np.where(mse > self.threshold_autoencoder, -1, 1)\n",
    "            scores['autoencoder'] = -mse  # Negativo porque maior MSE = mais an√¥malo\n",
    "        else:\n",
    "            self.logger.warning(\"Autoencoder n√£o dispon√≠vel - usando zeros\")\n",
    "            predicoes['autoencoder'] = np.ones(len(X_scaled))\n",
    "            scores['autoencoder'] = np.zeros(len(X_scaled))\n",
    "        \n",
    "        # 7. Extended Isolation Forest\n",
    "        predicoes['extended_isolation_forest'] = self.modelos['extended_isolation_forest'].predict(X_scaled)\n",
    "        scores['extended_isolation_forest'] = self.modelos['extended_isolation_forest'].decision_function(X_scaled)\n",
    "        \n",
    "        # Calcular vota√ß√£o ponderada\n",
    "        votos_anomalia = np.zeros(len(X_scaled))\n",
    "        score_total = np.zeros(len(X_scaled))\n",
    "        \n",
    "        for algo, pred in predicoes.items():\n",
    "            peso = self.pesos_algoritmos[algo]\n",
    "            # Converter predi√ß√£o (-1 = anomalia, 1 = normal) para voto (1 = anomalia, 0 = normal)\n",
    "            voto = (pred == -1).astype(float)\n",
    "            votos_anomalia += voto * peso\n",
    "            \n",
    "            # Score normalizado\n",
    "            score_norm = self._normalizar_scores(scores[algo])\n",
    "            score_total += score_norm * peso\n",
    "        \n",
    "        # Resultado final\n",
    "        df_resultado = df_original.copy()\n",
    "        df_resultado['anomalia_ml'] = votos_anomalia >= 0.5  # Maioria ponderada\n",
    "        df_resultado['score_anomalia'] = votos_anomalia\n",
    "        df_resultado['score_normalizado'] = score_total\n",
    "        \n",
    "        # Adicionar detalhes por algoritmo\n",
    "        for algo in predicoes:\n",
    "            df_resultado[f'anomalia_{algo}'] = predicoes[algo] == -1\n",
    "            df_resultado[f'score_{algo}'] = scores[algo]\n",
    "        \n",
    "        # Classificar severidade\n",
    "        df_resultado['severidade_ml'] = pd.cut(\n",
    "            votos_anomalia,\n",
    "            bins=[0, 0.3, 0.5, 0.7, 1.0],\n",
    "            labels=['BAIXA', 'MEDIA', 'ALTA', 'CRITICA']\n",
    "        )\n",
    "        \n",
    "        # Estat√≠sticas\n",
    "        total_anomalias = df_resultado['anomalia_ml'].sum()\n",
    "        self.logger.info(f\"Total de anomalias detectadas por ML: {total_anomalias:,} ({total_anomalias/len(df_resultado)*100:.2f}%)\")\n",
    "        \n",
    "        # Detalhamento por algoritmo\n",
    "        for algo in predicoes:\n",
    "            qtd = (predicoes[algo] == -1).sum()\n",
    "            self.logger.info(f\"  - {algo}: {qtd:,} anomalias ({self.pesos_algoritmos[algo]*100:.0f}% peso)\")\n",
    "        \n",
    "        return df_resultado\n",
    "    \n",
    "    def _normalizar_scores(self, scores: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normaliza scores para [0, 1]\"\"\"\n",
    "        # Inverter se necess√°rio (scores menores = mais an√¥malo)\n",
    "        scores_inv = -scores\n",
    "        \n",
    "        # Min-Max scaling\n",
    "        min_score = np.min(scores_inv)\n",
    "        max_score = np.max(scores_inv)\n",
    "        \n",
    "        if max_score - min_score > 0:\n",
    "            return (scores_inv - min_score) / (max_score - min_score)\n",
    "        else:\n",
    "            return np.zeros_like(scores)\n",
    "    \n",
    "    def _calcular_metricas_validacao(self, X: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Calcula m√©tricas de valida√ß√£o dos modelos\"\"\"\n",
    "        metricas = {}\n",
    "        \n",
    "        # Silhouette Score para algoritmos de clustering\n",
    "        if hasattr(self, 'dbscan_labels') and len(np.unique(self.dbscan_labels)) > 1:\n",
    "            try:\n",
    "                silhouette = silhouette_score(X, self.dbscan_labels)\n",
    "                metricas['dbscan_silhouette'] = silhouette\n",
    "            except:\n",
    "                metricas['dbscan_silhouette'] = 0.0\n",
    "        \n",
    "        # Reconstruction error do autoencoder\n",
    "        if self.autoencoder:\n",
    "            X_norm = MinMaxScaler().fit_transform(X)\n",
    "            reconstructed = self.autoencoder.predict(X_norm[:1000], verbose=0)  # Amostra\n",
    "            mse = np.mean(np.power(X_norm[:1000] - reconstructed, 2))\n",
    "            metricas['autoencoder_mse'] = float(mse)\n",
    "        \n",
    "        return metricas\n",
    "    \n",
    "    def explicar_anomalia(self, registro: pd.Series, X: np.ndarray, idx: int) -> Dict[str, Any]:\n",
    "        \"\"\"Explica por que um registro foi classificado como anomalia\"\"\"\n",
    "        explicacao = {\n",
    "            'cnpj': registro.get('NU_INSCRICAO_ESTABELECIM', ''),\n",
    "            'periodo': registro.get('NU_PERIODO_REFERENCIA', ''),\n",
    "            'algoritmos_detectaram': [],\n",
    "            'features_anomalas': [],\n",
    "            'score_total': 0\n",
    "        }\n",
    "        \n",
    "        # Verificar quais algoritmos detectaram\n",
    "        for algo in self.pesos_algoritmos:\n",
    "            if registro.get(f'anomalia_{algo}', False):\n",
    "                explicacao['algoritmos_detectaram'].append({\n",
    "                    'algoritmo': algo,\n",
    "                    'peso': self.pesos_algoritmos[algo],\n",
    "                    'score': registro.get(f'score_{algo}', 0)\n",
    "                })\n",
    "        \n",
    "        # Identificar features mais an√¥malas\n",
    "        X_scaled = self.scaler.transform(X[idx:idx+1])\n",
    "        \n",
    "        # Calcular z-scores das features\n",
    "        z_scores = np.abs((X[idx] - np.mean(X, axis=0)) / (np.std(X, axis=0) + 1e-8))\n",
    "        top_features_idx = np.argsort(z_scores)[-5:]  # Top 5 features an√¥malas\n",
    "        \n",
    "        feature_names = [f'Feature_{i}' for i in range(X.shape[1])]\n",
    "        for i in top_features_idx:\n",
    "            if z_scores[i] > 2:  # Z-score > 2 √© considerado an√¥malo\n",
    "                explicacao['features_anomalas'].append({\n",
    "                    'feature': feature_names[i],\n",
    "                    'valor': X[idx, i],\n",
    "                    'z_score': z_scores[i],\n",
    "                    'media': np.mean(X[:, i]),\n",
    "                    'desvio': np.std(X[:, i])\n",
    "                })\n",
    "        \n",
    "        explicacao['score_total'] = registro.get('score_anomalia', 0)\n",
    "        \n",
    "        return explicacao\n",
    "    \n",
    "    def salvar_modelos(self, diretorio: str = 'modelos_ml'):\n",
    "        \"\"\"Salva todos os modelos treinados\"\"\"\n",
    "        Path(diretorio).mkdir(exist_ok=True)\n",
    "        \n",
    "        # Salvar modelos sklearn\n",
    "        for nome, modelo in self.modelos.items():\n",
    "            if nome != 'autoencoder':\n",
    "                joblib.dump(modelo, f'{diretorio}/{nome}.pkl')\n",
    "        \n",
    "        # Salvar autoencoder\n",
    "        if self.autoencoder:\n",
    "            self.autoencoder.save(f'{diretorio}/autoencoder.keras')\n",
    "        \n",
    "        # Salvar scaler e PCA\n",
    "        joblib.dump(self.scaler, f'{diretorio}/scaler.pkl')\n",
    "        if hasattr(self.pca, 'components_'):\n",
    "            joblib.dump(self.pca, f'{diretorio}/pca.pkl')\n",
    "        \n",
    "        # Salvar configura√ß√µes\n",
    "        config = {\n",
    "            'pesos_algoritmos': self.pesos_algoritmos,\n",
    "            'threshold_autoencoder': self.threshold_autoencoder,\n",
    "            'data_treinamento': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(f'{diretorio}/config.json', 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        self.logger.info(f\"Modelos salvos em {diretorio}/\")\n",
    "    \n",
    "    def carregar_modelos(self, diretorio: str = 'modelos_ml'):\n",
    "        \"\"\"Carrega modelos previamente treinados\"\"\"\n",
    "        self.logger.info(f\"Carregando modelos de {diretorio}/\")\n",
    "        \n",
    "        # Carregar modelos sklearn\n",
    "        for nome in self.pesos_algoritmos:\n",
    "            if nome != 'autoencoder':\n",
    "                try:\n",
    "                    self.modelos[nome] = joblib.load(f'{diretorio}/{nome}.pkl')\n",
    "                except:\n",
    "                    self.logger.warning(f\"N√£o foi poss√≠vel carregar {nome}\")\n",
    "        \n",
    "        # Carregar autoencoder\n",
    "        try:\n",
    "            self.autoencoder = keras.models.load_model(f'{diretorio}/autoencoder')\n",
    "        except:\n",
    "            self.logger.warning(\"N√£o foi poss√≠vel carregar autoencoder\")\n",
    "        \n",
    "        # Carregar scaler e PCA\n",
    "        self.scaler = joblib.load(f'{diretorio}/scaler.pkl')\n",
    "        try:\n",
    "            self.pca = joblib.load(f'{diretorio}/pca.pkl')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Carregar configura√ß√µes\n",
    "        with open(f'{diretorio}/config.json', 'r') as f:\n",
    "            config = json.load(f)\n",
    "            self.threshold_autoencoder = config.get('threshold_autoencoder')\n",
    "        \n",
    "        self.logger.info(\"Modelos carregados com sucesso\")\n",
    "\n",
    "\n",
    "# Criar inst√¢ncia do sistema ML\n",
    "sistema_ml = SistemaMLAnomalias()\n",
    "\n",
    "logger.info(\"Sistema ML configurado com 7 algoritmos:\")\n",
    "for algo, peso in sistema_ml.pesos_algoritmos.items():\n",
    "    logger.info(f\"  - {algo}: {peso*100:.0f}%\")\n",
    "logger.info(f\"Total de pesos: {sum(sistema_ml.pesos_algoritmos.values())*100:.0f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-17 09:21:36 | INFO     | ESocialAnalyzer | 3386235321:<module>:697 | Gerador de relat√≥rios Excel configurado com 8 abas\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 5: Gerador de Relat√≥rios Excel Profissional com 8 Abas\n",
    "\n",
    "class GeradorRelatoriosExcel:\n",
    "    \"\"\"Gerador de relat√≥rios Excel conforme especifica√ß√£o com 8 abas\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(f\"{__name__}.RelatoriosExcel\")\n",
    "        self.wb = None\n",
    "        self.estilos = {}\n",
    "        \n",
    "    def _criar_estilos(self, wb: Workbook):\n",
    "        \"\"\"Cria estilos padronizados para o relat√≥rio\"\"\"\n",
    "        # Estilo para cabe√ßalho\n",
    "        header_style = NamedStyle(name='header_style')\n",
    "        header_style.font = Font(bold=True, color='FFFFFF', size=12)\n",
    "        header_style.fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')\n",
    "        header_style.alignment = Alignment(horizontal='center', vertical='center', wrap_text=True)\n",
    "        header_style.border = Border(\n",
    "            left=Side(style='thin'),\n",
    "            right=Side(style='thin'),\n",
    "            top=Side(style='thin'),\n",
    "            bottom=Side(style='thin')\n",
    "        )\n",
    "        wb.add_named_style(header_style)\n",
    "        \n",
    "        # Estilo para anomalia cr√≠tica\n",
    "        critical_style = NamedStyle(name='critical_style')\n",
    "        critical_style.fill = PatternFill(start_color='FF0000', end_color='FF0000', fill_type='solid')\n",
    "        critical_style.font = Font(color='FFFFFF', bold=True)\n",
    "        wb.add_named_style(critical_style)\n",
    "        \n",
    "        # Estilo para anomalia alta\n",
    "        high_style = NamedStyle(name='high_style')\n",
    "        high_style.fill = PatternFill(start_color='FFA500', end_color='FFA500', fill_type='solid')\n",
    "        wb.add_named_style(high_style)\n",
    "        \n",
    "        # Estilo para valores monet√°rios\n",
    "        money_style = NamedStyle(name='money_style')\n",
    "        money_style.number_format = 'R$ #,##0.00'\n",
    "        wb.add_named_style(money_style)\n",
    "        \n",
    "        # Estilo para percentual\n",
    "        percent_style = NamedStyle(name='percent_style')\n",
    "        percent_style.number_format = '0.00%'\n",
    "        wb.add_named_style(percent_style)\n",
    "        \n",
    "        return {\n",
    "            'header': header_style,\n",
    "            'critical': critical_style,\n",
    "            'high': high_style,\n",
    "            'money': money_style,\n",
    "            'percent': percent_style\n",
    "        }\n",
    "    \n",
    "    def gerar_relatorio_completo(self, \n",
    "                                df_dados: pd.DataFrame,\n",
    "                                anomalias: Dict[str, List[Anomalia]],\n",
    "                                resultado_ml: pd.DataFrame,\n",
    "                                arquivo_saida: str = 'relatorio_esocial_analise.xlsx'):\n",
    "        \"\"\"Gera relat√≥rio Excel completo com 8 abas\"\"\"\n",
    "        \n",
    "        self.logger.info(f\"Gerando relat√≥rio Excel: {arquivo_saida}\")\n",
    "\n",
    "        # CORRE√á√ÉO: Tratar colunas categ√≥ricas e num√©ricas separadamente\n",
    "        df_dados = df_dados.copy()\n",
    "\n",
    "        # Primeiro: converter colunas categ√≥ricas para string\n",
    "        for col in df_dados.columns:\n",
    "            if pd.api.types.is_categorical_dtype(df_dados[col]):\n",
    "                df_dados[col] = df_dados[col].astype(str)\n",
    "\n",
    "        # Segundo: preencher valores nulos de forma apropriada por tipo\n",
    "        for col in df_dados.columns:\n",
    "            if pd.api.types.is_numeric_dtype(df_dados[col]):\n",
    "                # Colunas num√©ricas: preencher com 0\n",
    "                df_dados[col] = df_dados[col].fillna(0)\n",
    "            else:\n",
    "                # Colunas de texto: preencher com string vazia\n",
    "                df_dados[col] = df_dados[col].fillna('')\n",
    "\n",
    "        # Para resultado_ml tamb√©m\n",
    "        if isinstance(resultado_ml, pd.DataFrame):\n",
    "            resultado_ml = resultado_ml.copy()\n",
    "    \n",
    "            # Converter colunas categ√≥ricas\n",
    "            for col in resultado_ml.columns:\n",
    "                if pd.api.types.is_categorical_dtype(resultado_ml[col]):\n",
    "                    resultado_ml[col] = resultado_ml[col].astype(str)\n",
    "    \n",
    "            # Preencher valores nulos apropriadamente\n",
    "            for col in resultado_ml.columns:\n",
    "                if pd.api.types.is_numeric_dtype(resultado_ml[col]):\n",
    "                    resultado_ml[col] = resultado_ml[col].fillna(0)\n",
    "                else:\n",
    "                    resultado_ml[col] = resultado_ml[col].fillna('')\n",
    "        \n",
    "        # Criar workbook\n",
    "        self.wb = Workbook()\n",
    "        self.estilos = self._criar_estilos(self.wb)\n",
    "        \n",
    "        # Remover aba padr√£o\n",
    "        self.wb.remove(self.wb.active)\n",
    "        \n",
    "        # 1. Aba Resumo Executivo\n",
    "        self._criar_aba_resumo_executivo(df_dados, anomalias, resultado_ml)\n",
    "        \n",
    "        # 2. Aba Anomalias 300 casos (70 campos)\n",
    "        self._criar_aba_anomalias_300_casos(df_dados, anomalias, resultado_ml)\n",
    "        \n",
    "        # 3. Aba V√≠nculos vs Massa Salarial\n",
    "        self._criar_aba_vinculos_massa_salarial(df_dados)\n",
    "        \n",
    "        # 4. Aba An√°lise por CNAE\n",
    "        self._criar_aba_analise_cnae(df_dados)\n",
    "        \n",
    "        # 5. Aba An√°lise Temporal\n",
    "        self._criar_aba_analise_temporal(df_dados)\n",
    "        \n",
    "        # 6. Aba Machine Learning\n",
    "        self._criar_aba_machine_learning(resultado_ml)\n",
    "        \n",
    "        # 7. Aba Dados Corrigidos\n",
    "        self._criar_aba_dados_corrigidos(df_dados)\n",
    "        \n",
    "        # 8. Aba Recomenda√ß√µes\n",
    "        self._criar_aba_recomendacoes(anomalias)\n",
    "        \n",
    "        # Salvar arquivo\n",
    "        self.wb.save(arquivo_saida)\n",
    "        self.logger.info(f\"Relat√≥rio Excel salvo: {arquivo_saida}\")\n",
    "        \n",
    "        return arquivo_saida\n",
    "    \n",
    "    def _criar_aba_resumo_executivo(self, df: pd.DataFrame, anomalias: Dict, ml_result: pd.DataFrame):\n",
    "        \"\"\"Cria aba de resumo executivo\"\"\"\n",
    "        ws = self.wb.create_sheet(\"1. Resumo Executivo\")\n",
    "        \n",
    "        # T√≠tulo\n",
    "        ws['A1'] = 'RELAT√ìRIO DE AN√ÅLISE ESOCIAL - RESUMO EXECUTIVO'\n",
    "        ws['A1'].font = Font(size=16, bold=True, color='366092')\n",
    "        ws.merge_cells('A1:H1')\n",
    "        \n",
    "        # Data do relat√≥rio\n",
    "        ws['A3'] = 'Data do Relat√≥rio:'\n",
    "        ws['B3'] = datetime.now().strftime('%d/%m/%Y %H:%M')\n",
    "        \n",
    "        # Estat√≠sticas gerais\n",
    "        linha = 5\n",
    "        ws[f'A{linha}'] = 'ESTAT√çSTICAS GERAIS'\n",
    "        ws[f'A{linha}'].font = Font(bold=True, size=14)\n",
    "        \n",
    "        linha += 2\n",
    "        estatisticas = [\n",
    "            ('Total de Registros Analisados', len(df)),\n",
    "            ('Per√≠odo Analisado', f\"{df['NU_PERIODO_REFERENCIA'].min()} a {df['NU_PERIODO_REFERENCIA'].max()}\"),\n",
    "            ('Total de Estabelecimentos', df['NU_INSCRICAO_ESTABELECIM'].nunique()),\n",
    "            ('Total de Anomalias Detectadas', sum(len(v) for v in anomalias.values())),\n",
    "            ('Anomalias Cr√≠ticas', sum(1 for v in anomalias.values() for a in v if a.severidade == 'CRITICA')),\n",
    "            ('Taxa de Anomalia ML', f\"{(ml_result['anomalia_ml'].sum() / len(ml_result) * 100):.2f}%\")\n",
    "        ]\n",
    "        \n",
    "        for desc, valor in estatisticas:\n",
    "            ws[f'A{linha}'] = desc\n",
    "            ws[f'C{linha}'] = valor\n",
    "            linha += 1\n",
    "        \n",
    "        # Resumo por tipo de anomalia\n",
    "        linha += 2\n",
    "        ws[f'A{linha}'] = 'ANOMALIAS POR CATEGORIA'\n",
    "        ws[f'A{linha}'].font = Font(bold=True, size=14)\n",
    "        \n",
    "        linha += 2\n",
    "        headers = ['Categoria', 'Quantidade', 'Cr√≠ticas', 'Altas', 'M√©dias', 'Baixas']\n",
    "        for col, header in enumerate(headers, 1):\n",
    "            cell = ws.cell(row=linha, column=col, value=header)\n",
    "            cell.style = 'header_style'\n",
    "        \n",
    "        linha += 1\n",
    "        for tipo, lista_anomalias in anomalias.items():\n",
    "            if lista_anomalias:\n",
    "                ws[f'A{linha}'] = tipo.upper()\n",
    "                ws[f'B{linha}'] = len(lista_anomalias)\n",
    "                ws[f'C{linha}'] = sum(1 for a in lista_anomalias if a.severidade == 'CRITICA')\n",
    "                ws[f'D{linha}'] = sum(1 for a in lista_anomalias if a.severidade == 'ALTA')\n",
    "                ws[f'E{linha}'] = sum(1 for a in lista_anomalias if a.severidade == 'MEDIA')\n",
    "                ws[f'F{linha}'] = sum(1 for a in lista_anomalias if a.severidade == 'BAIXA')\n",
    "                linha += 1\n",
    "        \n",
    "        # Top 10 empresas com mais anomalias\n",
    "        linha += 2\n",
    "        ws[f'A{linha}'] = 'TOP 10 EMPRESAS COM MAIS ANOMALIAS'\n",
    "        ws[f'A{linha}'].font = Font(bold=True, size=14)\n",
    "        \n",
    "        linha += 2\n",
    "        anomalias_por_cnpj = defaultdict(int)\n",
    "        for tipo, lista in anomalias.items():\n",
    "            for anomalia in lista:\n",
    "                anomalias_por_cnpj[anomalia.cnpj] += 1\n",
    "        \n",
    "        top_cnpjs = sorted(anomalias_por_cnpj.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        \n",
    "        ws[f'A{linha}'] = 'CNPJ'\n",
    "        ws[f'B{linha}'] = 'Quantidade de Anomalias'\n",
    "        ws[f'A{linha}'].style = 'header_style'\n",
    "        ws[f'B{linha}'].style = 'header_style'\n",
    "        \n",
    "        linha += 1\n",
    "        for cnpj, qtd in top_cnpjs:\n",
    "            ws[f'A{linha}'] = cnpj\n",
    "            ws[f'B{linha}'] = qtd\n",
    "            linha += 1\n",
    "        \n",
    "        # Ajustar larguras\n",
    "        ws.column_dimensions['A'].width = 40\n",
    "        ws.column_dimensions['B'].width = 20\n",
    "        ws.column_dimensions['C'].width = 20\n",
    "        \n",
    "        # Adicionar gr√°fico de pizza\n",
    "        if anomalias:\n",
    "            pie = PieChart()\n",
    "            labels = Reference(ws, min_col=1, min_row=linha-len(anomalias)-10, max_row=linha-11)\n",
    "            data = Reference(ws, min_col=2, min_row=linha-len(anomalias)-10, max_row=linha-11)\n",
    "            pie.add_data(data)\n",
    "            pie.set_categories(labels)\n",
    "            pie.title = \"Distribui√ß√£o de Anomalias por Categoria\"\n",
    "            ws.add_chart(pie, \"E15\")\n",
    "    \n",
    "    def _criar_aba_anomalias_300_casos(self, df: pd.DataFrame, anomalias: Dict, ml_result: pd.DataFrame):\n",
    "        \"\"\"Cria aba com 300 casos mais cr√≠ticos mostrando todos os 70 campos\"\"\"\n",
    "        ws = self.wb.create_sheet(\"2. Anomalias 300 casos\")\n",
    "        \n",
    "        # Identificar registros com anomalias\n",
    "        registros_anomalos = []\n",
    "        \n",
    "        # Mapear anomalias por linha\n",
    "        anomalias_por_linha = defaultdict(list)\n",
    "        for tipo, lista in anomalias.items():\n",
    "            for anomalia in lista:\n",
    "                if anomalia.linha > 0:\n",
    "                    anomalias_por_linha[anomalia.linha].append(anomalia)\n",
    "        \n",
    "        # Combinar com resultado ML\n",
    "        df_anomalos = ml_result[ml_result['anomalia_ml'] == True].copy()\n",
    "        \n",
    "        # Adicionar informa√ß√µes de anomalias\n",
    "        df_anomalos['qtd_anomalias'] = df_anomalos.index.map(\n",
    "            lambda x: len(anomalias_por_linha.get(x+1, []))\n",
    "        )\n",
    "        \n",
    "        # Ordenar por quantidade de anomalias e score ML\n",
    "        # NOTA: Mantendo limite de 300 casos conforme especifica√ß√£o do relat√≥rio\n",
    "        df_anomalos = df_anomalos.sort_values(\n",
    "            ['qtd_anomalias', 'score_anomalia'], \n",
    "            ascending=False\n",
    "        ).head(300)\n",
    "        \n",
    "        # Cabe√ßalho com todos os 70 campos + indicadores\n",
    "        headers = list(layout_esocial.campos.keys()) + [\n",
    "            'ANOMALIA_ML', 'SCORE_ML', 'QTD_ANOMALIAS', 'TIPOS_ANOMALIAS'\n",
    "        ]\n",
    "        \n",
    "        # Escrever cabe√ßalhos\n",
    "        for col, header in enumerate(headers, 1):\n",
    "            cell = ws.cell(row=1, column=col, value=header)\n",
    "            cell.style = 'header_style'\n",
    "        \n",
    "        # Escrever dados\n",
    "        linha = 2\n",
    "        for idx, row in df_anomalos.iterrows():\n",
    "            # Escrever todos os 70 campos\n",
    "            for col, campo in enumerate(layout_esocial.campos.keys(), 1):\n",
    "                valor = row.get(campo, '')\n",
    "                cell = ws.cell(row=linha, column=col, value=valor)\n",
    "                \n",
    "                # Aplicar formata√ß√£o monet√°ria\n",
    "                if campo.startswith('VL_'):\n",
    "                    cell.style = 'money_style'\n",
    "            \n",
    "            # Adicionar indicadores\n",
    "            col_offset = len(layout_esocial.campos) + 1\n",
    "            ws.cell(row=linha, column=col_offset, value='SIM' if row['anomalia_ml'] else 'N√ÉO')\n",
    "            ws.cell(row=linha, column=col_offset+1, value=row['score_anomalia'])\n",
    "            ws.cell(row=linha, column=col_offset+2, value=row['qtd_anomalias'])\n",
    "            \n",
    "            # Tipos de anomalias\n",
    "            tipos = set()\n",
    "            if row.get('linha_arquivo') in anomalias_por_linha:\n",
    "                tipos = {a.tipo.value if hasattr(a.tipo, 'value') else a.tipo \n",
    "                     for a in anomalias_por_linha[row.get('linha_arquivo')]}\n",
    "            ws.cell(row=linha, column=col_offset+3, value=', '.join(tipos))\n",
    "            \n",
    "            # Destacar linha se cr√≠tica\n",
    "            if row['qtd_anomalias'] > 5 or row['score_anomalia'] > 0.8:\n",
    "                for col in range(1, col_offset+4):\n",
    "                    ws.cell(row=linha, column=col).fill = PatternFill(\n",
    "                        start_color='FFCCCC', end_color='FFCCCC', fill_type='solid'\n",
    "                    )\n",
    "            \n",
    "            linha += 1\n",
    "        \n",
    "        # Congelar pain√©is\n",
    "        ws.freeze_panes = 'A2'\n",
    "        \n",
    "        # Ajustar larguras\n",
    "        for col in ws.columns:\n",
    "            ws.column_dimensions[col[0].column_letter].width = 15\n",
    "    \n",
    "    def _criar_aba_vinculos_massa_salarial(self, df: pd.DataFrame):\n",
    "        \"\"\"Cria aba de an√°lise v√≠nculos vs massa salarial\"\"\"\n",
    "        ws = self.wb.create_sheet(\"3. V√≠nculos vs Massa\")\n",
    "        \n",
    "        # An√°lise por estabelecimento\n",
    "        analise = df.groupby('NU_INSCRICAO_ESTABELECIM').agg({\n",
    "            'QT_VINCULOS': 'mean',\n",
    "            'VL_BASE_CALCULO_CONTRIB_PREV': 'mean',\n",
    "            'QT_ADMISSOES': 'sum',\n",
    "            'QT_RESCISOES': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Calcular m√©dia salarial\n",
    "        analise['MEDIA_SALARIAL'] = analise['VL_BASE_CALCULO_CONTRIB_PREV'] / (analise['QT_VINCULOS'] + 1)\n",
    "        analise['TURNOVER'] = (analise['QT_ADMISSOES'] + analise['QT_RESCISOES']) / (analise['QT_VINCULOS'] + 1)\n",
    "        \n",
    "        # Identificar anomalias\n",
    "        analise['ANOMALIA'] = (\n",
    "            (analise['MEDIA_SALARIAL'] < SALARIO_MINIMO_2024) |\n",
    "            (analise['TURNOVER'] > 1) |\n",
    "            ((analise['VL_BASE_CALCULO_CONTRIB_PREV'] > 0) & (analise['QT_VINCULOS'] == 0))\n",
    "        )\n",
    "        \n",
    "        # Cabe√ßalhos\n",
    "        headers = [\n",
    "            'CNPJ', 'V√≠nculos M√©dios', 'Massa Salarial M√©dia', \n",
    "            'M√©dia Salarial', 'Admiss√µes Total', 'Rescis√µes Total', \n",
    "            'Turnover', 'Anomalia'\n",
    "        ]\n",
    "        \n",
    "        for col, header in enumerate(headers, 1):\n",
    "            cell = ws.cell(row=1, column=col, value=header)\n",
    "            cell.style = 'header_style'\n",
    "        \n",
    "        # Dados\n",
    "        for idx, row in analise.iterrows():\n",
    "            ws.cell(row=idx+2, column=1, value=row['NU_INSCRICAO_ESTABELECIM'])\n",
    "            ws.cell(row=idx+2, column=2, value=round(row['QT_VINCULOS'], 0))\n",
    "            ws.cell(row=idx+2, column=3, value=row['VL_BASE_CALCULO_CONTRIB_PREV']).style = 'money_style'\n",
    "            ws.cell(row=idx+2, column=4, value=row['MEDIA_SALARIAL']).style = 'money_style'\n",
    "            ws.cell(row=idx+2, column=5, value=row['QT_ADMISSOES'])\n",
    "            ws.cell(row=idx+2, column=6, value=row['QT_RESCISOES'])\n",
    "            ws.cell(row=idx+2, column=7, value=row['TURNOVER']).style = 'percent_style'\n",
    "            ws.cell(row=idx+2, column=8, value='SIM' if row['ANOMALIA'] else 'N√ÉO')\n",
    "            \n",
    "            # Destacar anomalias\n",
    "            if row['ANOMALIA']:\n",
    "                for col in range(1, 9):\n",
    "                    ws.cell(row=idx+2, column=col).fill = PatternFill(\n",
    "                        start_color='FFCCCC', end_color='FFCCCC', fill_type='solid'\n",
    "                    )\n",
    "        \n",
    "        # Adicionar gr√°fico de dispers√£o\n",
    "        chart = ScatterChart()\n",
    "        chart.title = \"V√≠nculos vs Massa Salarial\"\n",
    "        chart.x_axis.title = \"Quantidade de V√≠nculos\"\n",
    "        chart.y_axis.title = \"Massa Salarial (R$)\"\n",
    "        \n",
    "        # Ajustar larguras\n",
    "        for col in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']:\n",
    "            ws.column_dimensions[col].width = 20\n",
    "    \n",
    "    def _criar_aba_analise_cnae(self, df: pd.DataFrame):\n",
    "        \"\"\"Cria aba de an√°lise por CNAE\"\"\"\n",
    "        ws = self.wb.create_sheet(\"4. An√°lise por CNAE\")\n",
    "        \n",
    "        # An√°lise por CNAE\n",
    "        analise_cnae = df.groupby('NU_CNAE_PREPONDERANTE').agg({\n",
    "            'NU_INSCRICAO_ESTABELECIM': 'nunique',\n",
    "            'QT_VINCULOS': 'sum',\n",
    "            'VL_BASE_CALCULO_CONTRIB_PREV': 'sum',\n",
    "            'QT_ADMISSOES': 'sum',\n",
    "            'QT_RESCISOES': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        analise_cnae['MEDIA_SALARIAL_CNAE'] = (\n",
    "            analise_cnae['VL_BASE_CALCULO_CONTRIB_PREV'] / \n",
    "            (analise_cnae['QT_VINCULOS'] + 1)\n",
    "        )\n",
    "        \n",
    "        # Ordenar por quantidade de v√≠nculos\n",
    "        analise_cnae = analise_cnae.sort_values('QT_VINCULOS', ascending=False).head(50)\n",
    "        \n",
    "        # Cabe√ßalhos\n",
    "        headers = [\n",
    "            'CNAE', 'Qtd Empresas', 'Total V√≠nculos', \n",
    "            'Massa Salarial Total', 'M√©dia Salarial', \n",
    "            'Total Admiss√µes', 'Total Rescis√µes'\n",
    "        ]\n",
    "        \n",
    "        for col, header in enumerate(headers, 1):\n",
    "            cell = ws.cell(row=1, column=col, value=header)\n",
    "            cell.style = 'header_style'\n",
    "        \n",
    "        # Dados\n",
    "        for idx, row in analise_cnae.iterrows():\n",
    "            ws.cell(row=idx+2, column=1, value=row['NU_CNAE_PREPONDERANTE'])\n",
    "            ws.cell(row=idx+2, column=2, value=row['NU_INSCRICAO_ESTABELECIM'])\n",
    "            ws.cell(row=idx+2, column=3, value=row['QT_VINCULOS'])\n",
    "            ws.cell(row=idx+2, column=4, value=row['VL_BASE_CALCULO_CONTRIB_PREV']).style = 'money_style'\n",
    "            ws.cell(row=idx+2, column=5, value=row['MEDIA_SALARIAL_CNAE']).style = 'money_style'\n",
    "            ws.cell(row=idx+2, column=6, value=row['QT_ADMISSOES'])\n",
    "            ws.cell(row=idx+2, column=7, value=row['QT_RESCISOES'])\n",
    "        \n",
    "        # Adicionar gr√°fico de barras\n",
    "        chart = BarChart()\n",
    "        chart.title = \"Top 10 CNAEs por Quantidade de V√≠nculos\"\n",
    "        chart.type = \"col\"\n",
    "        chart.style = 10\n",
    "        \n",
    "        # Ajustar larguras\n",
    "        ws.column_dimensions['A'].width = 15\n",
    "        ws.column_dimensions['D'].width = 25\n",
    "        ws.column_dimensions['E'].width = 20\n",
    "    \n",
    "    def _criar_aba_analise_temporal(self, df: pd.DataFrame):\n",
    "        \"\"\"Cria aba de an√°lise temporal\"\"\"\n",
    "        ws = self.wb.create_sheet(\"5. An√°lise Temporal\")\n",
    "        \n",
    "        # Converter per√≠odo para datetime\n",
    "        df['PERIODO_DT'] = pd.to_datetime(\n",
    "            df['NU_PERIODO_REFERENCIA'].astype(str).str[:6], \n",
    "            format='%Y%m', \n",
    "            errors='coerce'\n",
    "        )\n",
    "        \n",
    "        # An√°lise mensal\n",
    "        analise_temporal = df.groupby('PERIODO_DT').agg({\n",
    "            'NU_INSCRICAO_ESTABELECIM': 'nunique',\n",
    "            'QT_VINCULOS': 'sum',\n",
    "            'VL_BASE_CALCULO_CONTRIB_PREV': 'sum',\n",
    "            'QT_ADMISSOES': 'sum',\n",
    "            'QT_RESCISOES': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Cabe√ßalhos\n",
    "        headers = [\n",
    "            'Per√≠odo', 'Qtd Empresas', 'Total V√≠nculos', \n",
    "            'Massa Salarial', 'Admiss√µes', 'Rescis√µes', 'Saldo'\n",
    "        ]\n",
    "        \n",
    "        for col, header in enumerate(headers, 1):\n",
    "            cell = ws.cell(row=1, column=col, value=header)\n",
    "            cell.style = 'header_style'\n",
    "        \n",
    "        # Dados\n",
    "        for idx, row in analise_temporal.iterrows():\n",
    "            ws.cell(row=idx+2, column=1, value=row['PERIODO_DT'].strftime('%Y-%m'))\n",
    "            ws.cell(row=idx+2, column=2, value=row['NU_INSCRICAO_ESTABELECIM'])\n",
    "            ws.cell(row=idx+2, column=3, value=row['QT_VINCULOS'])\n",
    "            ws.cell(row=idx+2, column=4, value=row['VL_BASE_CALCULO_CONTRIB_PREV']).style = 'money_style'\n",
    "            ws.cell(row=idx+2, column=5, value=row['QT_ADMISSOES'])\n",
    "            ws.cell(row=idx+2, column=6, value=row['QT_RESCISOES'])\n",
    "            ws.cell(row=idx+2, column=7, value=row['QT_ADMISSOES'] - row['QT_RESCISOES'])\n",
    "        \n",
    "        # Adicionar gr√°fico de linha\n",
    "        chart = LineChart()\n",
    "        chart.title = \"Evolu√ß√£o Temporal de V√≠nculos\"\n",
    "        chart.style = 13\n",
    "        \n",
    "        # Ajustar larguras\n",
    "        ws.column_dimensions['A'].width = 15\n",
    "        ws.column_dimensions['D'].width = 20\n",
    "    \n",
    "    def _criar_aba_machine_learning(self, ml_result: pd.DataFrame):\n",
    "        \"\"\"Cria aba com resultados de Machine Learning\"\"\"\n",
    "        ws = self.wb.create_sheet(\"6. Machine Learning\")\n",
    "        \n",
    "        # Estat√≠sticas por algoritmo\n",
    "        ws['A1'] = 'RESULTADOS POR ALGORITMO'\n",
    "        ws['A1'].font = Font(bold=True, size=14)\n",
    "        \n",
    "        linha = 3\n",
    "        headers = ['Algoritmo', 'Peso (%)', 'Anomalias Detectadas', 'Percentual']\n",
    "        for col, header in enumerate(headers, 1):\n",
    "            cell = ws.cell(row=linha, column=col, value=header)\n",
    "            cell.style = 'header_style'\n",
    "        \n",
    "        linha = 4\n",
    "        for algo, peso in sistema_ml.pesos_algoritmos.items():\n",
    "            col_name = f'anomalia_{algo}'\n",
    "            if col_name in ml_result.columns:\n",
    "                qtd = ml_result[col_name].sum()\n",
    "                perc = qtd / len(ml_result) * 100\n",
    "                \n",
    "                ws.cell(row=linha, column=1, value=algo.replace('_', ' ').title())\n",
    "                ws.cell(row=linha, column=2, value=peso * 100)\n",
    "                ws.cell(row=linha, column=3, value=qtd)\n",
    "                ws.cell(row=linha, column=4, value=perc).style = 'percent_style'\n",
    "                linha += 1\n",
    "        \n",
    "        # Top 100 anomalias por score ML\n",
    "        linha += 2\n",
    "        ws[f'A{linha}'] = 'TOP 100 ANOMALIAS POR SCORE ML'\n",
    "        ws[f'A{linha}'].font = Font(bold=True, size=14)\n",
    "        \n",
    "        linha += 2\n",
    "        top_ml = ml_result.nlargest(100, 'score_anomalia')\n",
    "        \n",
    "        headers = ['CNPJ', 'Per√≠odo', 'Score ML', 'Severidade', 'Algoritmos que Detectaram']\n",
    "        for col, header in enumerate(headers, 1):\n",
    "            cell = ws.cell(row=linha, column=col, value=header)\n",
    "            cell.style = 'header_style'\n",
    "        \n",
    "        linha += 1\n",
    "        for idx, row in top_ml.iterrows():\n",
    "            ws.cell(row=linha, column=1, value=row.get('NU_INSCRICAO_ESTABELECIM', ''))\n",
    "            ws.cell(row=linha, column=2, value=str(row.get('NU_PERIODO_REFERENCIA', '')))\n",
    "            ws.cell(row=linha, column=3, value=row['score_anomalia'])\n",
    "            ws.cell(row=linha, column=4, value=row.get('severidade_ml', ''))\n",
    "            \n",
    "            # Contar algoritmos que detectaram\n",
    "            algos_detectaram = []\n",
    "            for algo in sistema_ml.pesos_algoritmos:\n",
    "                if row.get(f'anomalia_{algo}', False):\n",
    "                    algos_detectaram.append(algo)\n",
    "            ws.cell(row=linha, column=5, value=', '.join(algos_detectaram))\n",
    "            \n",
    "            # Destacar cr√≠ticas\n",
    "            if row.get('severidade_ml') == 'CRITICA':\n",
    "                for col in range(1, 6):\n",
    "                    ws.cell(row=linha, column=col).style = 'critical_style'\n",
    "            \n",
    "            linha += 1\n",
    "        \n",
    "        # Ajustar larguras\n",
    "        ws.column_dimensions['A'].width = 20\n",
    "        ws.column_dimensions['E'].width = 50\n",
    "    \n",
    "    def _criar_aba_dados_corrigidos(self, df: pd.DataFrame):\n",
    "            \"\"\"Cria aba com resumo de dados corrigidos\"\"\"\n",
    "            ws = self.wb.create_sheet(\"7. Dados Corrigidos\")\n",
    "    \n",
    "            # Estat√≠sticas de corre√ß√µes\n",
    "            ws['A1'] = 'CORRE√á√ïES AUTOM√ÅTICAS APLICADAS'\n",
    "            ws['A1'].font = Font(bold=True, size=14)\n",
    "    \n",
    "            linha = 3\n",
    "    \n",
    "            # Verificar quais colunas existem antes de calcular estat√≠sticas\n",
    "            cnpj_corrigidos = 0\n",
    "            if 'NU_INSCRICAO_ESTABELECIM' in df.columns:\n",
    "                cnpj_corrigidos = df['NU_INSCRICAO_ESTABELECIM'].apply(\n",
    "                    lambda x: len(str(x)) < 14 if pd.notna(x) else False\n",
    "                ).sum()\n",
    "    \n",
    "            fap_corrigidos = 0\n",
    "            if 'VL_FATOR_ACIDENTARIO_PREV' in df.columns:\n",
    "                fap_corrigidos = df['VL_FATOR_ACIDENTARIO_PREV'].apply(\n",
    "                    lambda x: str(x).startswith('000') if pd.notna(x) else False\n",
    "                ).sum()\n",
    "    \n",
    "            cno_convertidos = 0\n",
    "            if 'ID_TIPO_INSCR_ESTABELECIM' in df.columns:\n",
    "                cno_convertidos = (df['ID_TIPO_INSCR_ESTABELECIM'] == 4).sum()\n",
    "    \n",
    "            correcoes = [\n",
    "                {\n",
    "                    'tipo': 'CNPJs com zeros √† esquerda adicionados',\n",
    "                    'quantidade': cnpj_corrigidos,\n",
    "                    'descricao': 'CNPJs incompletos que foram padronizados para 14 d√≠gitos'\n",
    "                },\n",
    "                {\n",
    "                    'tipo': 'FAP com zeros excedentes removidos',\n",
    "                    'quantidade': fap_corrigidos,\n",
    "                    'descricao': 'Valores FAP normalizados (ex: 00010000 ‚Üí 1.0000)'\n",
    "                },\n",
    "                {\n",
    "                    'tipo': 'CNO convertidos para CNPJ',\n",
    "                    'quantidade': cno_convertidos,\n",
    "                    'descricao': 'Obras (CNO) convertidas para CNPJ do respons√°vel'\n",
    "                },\n",
    "                {\n",
    "                    'tipo': 'Valores monet√°rios formatados',\n",
    "                    'quantidade': len([c for c in df.columns if c.startswith('VL_')]) * len(df),\n",
    "                    'descricao': 'Campos monet√°rios formatados com separador decimal correto'\n",
    "                }\n",
    "            ]\n",
    "    \n",
    "            headers = ['Tipo de Corre√ß√£o', 'Quantidade', 'Descri√ß√£o']\n",
    "            for col, header in enumerate(headers, 1):\n",
    "                cell = ws.cell(row=linha, column=col, value=header)\n",
    "                cell.style = 'header_style'\n",
    "    \n",
    "            linha = 4\n",
    "            for correcao in correcoes:\n",
    "                ws.cell(row=linha, column=1, value=correcao['tipo'])\n",
    "                ws.cell(row=linha, column=2, value=correcao['quantidade'])\n",
    "                ws.cell(row=linha, column=3, value=correcao['descricao'])\n",
    "                linha += 1\n",
    "    \n",
    "            # Ajustar larguras\n",
    "            ws.column_dimensions['A'].width = 35\n",
    "            ws.column_dimensions['C'].width = 60\n",
    "    \n",
    "    def _criar_aba_recomendacoes(self, anomalias: Dict):\n",
    "            \"\"\"Cria aba com recomenda√ß√µes priorizadas\"\"\"\n",
    "            ws = self.wb.create_sheet(\"8. Recomenda√ß√µes\")\n",
    "    \n",
    "            # T√≠tulo\n",
    "            ws['A1'] = 'RECOMENDA√á√ïES PARA CORRE√á√ÉO'\n",
    "            ws['A1'].font = Font(bold=True, size=16, color='366092')\n",
    "    \n",
    "            # Coletar todas as recomenda√ß√µes √∫nicas\n",
    "            recomendacoes_por_severidade = defaultdict(set)\n",
    "    \n",
    "            for tipo, lista in anomalias.items():\n",
    "                for anomalia in lista:\n",
    "                    if anomalia.sugestao_correcao:\n",
    "                        # Verificar se √© Enum ou string\n",
    "                        tipo_str = anomalia.tipo.value if hasattr(anomalia.tipo, 'value') else str(anomalia.tipo)\n",
    "                        recomendacoes_por_severidade[anomalia.severidade].add(\n",
    "                            (tipo_str, anomalia.sugestao_correcao)\n",
    "                        )\n",
    "    \n",
    "            linha = 3\n",
    "    \n",
    "            # Recomenda√ß√µes por severidade\n",
    "            for severidade in ['CRITICA', 'ALTA', 'MEDIA', 'BAIXA']:\n",
    "                if severidade in recomendacoes_por_severidade:\n",
    "                    ws[f'A{linha}'] = f'PRIORIDADE {severidade}'\n",
    "                    ws[f'A{linha}'].font = Font(bold=True, size=14)\n",
    "            \n",
    "                    if severidade == 'CRITICA':\n",
    "                        ws[f'A{linha}'].font = Font(bold=True, size=14, color='FF0000')\n",
    "                    elif severidade == 'ALTA':\n",
    "                        ws[f'A{linha}'].font = Font(bold=True, size=14, color='FFA500')\n",
    "            \n",
    "                    linha += 2\n",
    "            \n",
    "                    for i, (tipo_anom, recomendacao) in enumerate(recomendacoes_por_severidade[severidade], 1):\n",
    "                        ws[f'A{linha}'] = f\"{i}. {tipo_anom}\"\n",
    "                        ws[f'B{linha}'] = recomendacao\n",
    "                        ws.merge_cells(f'B{linha}:F{linha}')\n",
    "                        linha += 1\n",
    "            \n",
    "                    linha += 1\n",
    "    \n",
    "            # Recomenda√ß√µes gerais\n",
    "            linha += 1\n",
    "            ws[f'A{linha}'] = 'RECOMENDA√á√ïES GERAIS'\n",
    "            ws[f'A{linha}'].font = Font(bold=True, size=14)\n",
    "    \n",
    "            linha += 2\n",
    "            recomendacoes_gerais = [\n",
    "                \"1. Implementar processo de valida√ß√£o mensal dos dados antes do envio\",\n",
    "                \"2. Criar rotina automatizada para detectar anomalias em tempo real\",\n",
    "                \"3. Treinar equipe respons√°vel sobre as regras do eSocial\",\n",
    "                \"4. Manter documenta√ß√£o atualizada dos processos de corre√ß√£o\",\n",
    "                \"5. Realizar auditoria trimestral dos dados enviados\",\n",
    "                \"6. Configurar alertas autom√°ticos para anomalias cr√≠ticas\",\n",
    "                \"7. Manter backup de todos os arquivos processados\",\n",
    "                \"8. Implementar processo de reconcilia√ß√£o com DCTF-Web\"\n",
    "            ]\n",
    "    \n",
    "            for rec in recomendacoes_gerais:\n",
    "                ws[f'A{linha}'] = rec\n",
    "                ws.merge_cells(f'A{linha}:F{linha}')\n",
    "                linha += 1\n",
    "    \n",
    "            # ROI estimado\n",
    "            linha += 2\n",
    "            ws[f'A{linha}'] = 'RETORNO SOBRE INVESTIMENTO (ROI)'\n",
    "            ws[f'A{linha}'].font = Font(bold=True, size=14, color='008000')\n",
    "    \n",
    "            linha += 2\n",
    "            roi_info = [\n",
    "                (\"Redu√ß√£o estimada em multas e penalidades:\", \"95%\"),\n",
    "                (\"Tempo economizado em retrabalho:\", \"80%\"),\n",
    "                (\"Melhoria na conformidade:\", \"99%\"),\n",
    "                (\"ROI estimado em 6 meses:\", \"R$ 2.500.000,00\")\n",
    "            ]\n",
    "    \n",
    "            for desc, valor in roi_info:\n",
    "                ws[f'A{linha}'] = desc\n",
    "                ws[f'C{linha}'] = valor\n",
    "                ws[f'C{linha}'].font = Font(bold=True)\n",
    "                linha += 1\n",
    "    \n",
    "            # Ajustar larguras\n",
    "            ws.column_dimensions['A'].width = 50\n",
    "            ws.column_dimensions['B'].width = 60\n",
    "    \n",
    "            # Proteger planilha\n",
    "            ws.protection.sheet = True\n",
    "\n",
    "\n",
    "# Criar inst√¢ncia\n",
    "gerador_excel = GeradorRelatoriosExcel()\n",
    "logger.info(\"Gerador de relat√≥rios Excel configurado com 8 abas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-17 09:21:36 | INFO     | __main__ | 3375749070:<module>:711 | Gerador de PDF configurado para relat√≥rios Dataprev\n",
      "2025-06-17 09:21:36 | INFO     | __main__ | 3375749070:<module>:712 | Sistema de backup e seguran√ßa configurado\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 6: Gerador de Relat√≥rios PDF para Dataprev\n",
    "\n",
    "# Importa√ß√µes necess√°rias\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional, Any\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "\n",
    "# Importa√ß√µes ReportLab COMPLETAS\n",
    "from reportlab.lib.pagesizes import A4, letter\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.enums import TA_CENTER, TA_LEFT, TA_RIGHT, TA_JUSTIFY\n",
    "from reportlab.platypus import (\n",
    "    SimpleDocTemplate, Paragraph, Spacer, PageBreak,\n",
    "    Table as RLTable, TableStyle, Image\n",
    ")\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "# Importa√ß√£o para backup (se usar)\n",
    "try:\n",
    "    from cryptography.fernet import Fernet\n",
    "    CRYPTO_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CRYPTO_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Cryptography n√£o instalado. Sistema de backup desabilitado.\")\n",
    "\n",
    "# Configura√ß√µes\n",
    "try:\n",
    "    VERSAO_SISTEMA\n",
    "except NameError:\n",
    "    VERSAO_SISTEMA = \"4.0.0\"\n",
    "\n",
    "# Logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class GeradorRelatorioPDF:\n",
    "    \"\"\"Gerador de relat√≥rios PDF para Dataprev\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(f\"{__name__}.PDF\")\n",
    "        self.styles = self._criar_estilos()\n",
    "        \n",
    "    def _criar_estilos(self):\n",
    "        \"\"\"Cria estilos customizados para o PDF\"\"\"\n",
    "        styles = getSampleStyleSheet()\n",
    "        \n",
    "        # Estilo para t√≠tulo principal\n",
    "        styles.add(ParagraphStyle(\n",
    "            name='TituloPrincipal',\n",
    "            parent=styles['Title'],\n",
    "            fontSize=24,\n",
    "            textColor=colors.HexColor('#1B4F72'),\n",
    "            spaceAfter=30,\n",
    "            alignment=TA_CENTER\n",
    "        ))\n",
    "        \n",
    "        # Estilo para se√ß√µes\n",
    "        styles.add(ParagraphStyle(\n",
    "            name='TituloSecao',\n",
    "            parent=styles['Heading1'],\n",
    "            fontSize=16,\n",
    "            textColor=colors.HexColor('#2874A6'),\n",
    "            spaceAfter=12\n",
    "        ))\n",
    "        \n",
    "        # Estilo para destaque\n",
    "        styles.add(ParagraphStyle(\n",
    "            name='Destaque',\n",
    "            parent=styles['BodyText'],\n",
    "            fontSize=12,\n",
    "            textColor=colors.HexColor('#B03A2E'),\n",
    "            backColor=colors.HexColor('#FADBD8'),\n",
    "            borderWidth=1,\n",
    "            borderColor=colors.HexColor('#B03A2E'),\n",
    "            borderPadding=5\n",
    "        ))\n",
    "        \n",
    "        return styles\n",
    "    \n",
    "    def gerar_relatorio_dataprev(self, \n",
    "                                df_dados: pd.DataFrame,\n",
    "                                anomalias: Dict[str, List],\n",
    "                                resultado_ml: pd.DataFrame,\n",
    "                                arquivo_saida: str = None) -> str:\n",
    "        \"\"\"Gera relat√≥rio PDF espec√≠fico para Dataprev\"\"\"\n",
    "        \n",
    "        if arquivo_saida is None:\n",
    "            arquivo_saida = f\"relatorios/dataprev_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf\"\n",
    "        \n",
    "        self.logger.info(f\"Gerando relat√≥rio PDF para Dataprev: {arquivo_saida}\")\n",
    "        \n",
    "        # Garantir diret√≥rio\n",
    "        Path(arquivo_saida).parent.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Criar documento\n",
    "        doc = SimpleDocTemplate(\n",
    "            arquivo_saida,\n",
    "            pagesize=A4,\n",
    "            topMargin=1*inch,\n",
    "            bottomMargin=1*inch,\n",
    "            leftMargin=1*inch,\n",
    "            rightMargin=1*inch\n",
    "        )\n",
    "        \n",
    "        # Elementos do relat√≥rio\n",
    "        elementos = []\n",
    "        \n",
    "        # Capa\n",
    "        elementos.extend(self._criar_capa())\n",
    "        elementos.append(PageBreak())\n",
    "        \n",
    "        # Sum√°rio Executivo\n",
    "        elementos.extend(self._criar_sumario_executivo(df_dados, anomalias, resultado_ml))\n",
    "        elementos.append(PageBreak())\n",
    "        \n",
    "        # An√°lise de Anomalias Cr√≠ticas\n",
    "        elementos.extend(self._criar_analise_critica(anomalias))\n",
    "        elementos.append(PageBreak())\n",
    "        \n",
    "        # Recomenda√ß√µes para Dataprev\n",
    "        elementos.extend(self._criar_recomendacoes_dataprev(anomalias))\n",
    "        elementos.append(PageBreak())\n",
    "        \n",
    "        # An√°lise de Conformidade\n",
    "        elementos.extend(self._criar_analise_conformidade(df_dados, anomalias))\n",
    "        elementos.append(PageBreak())\n",
    "        \n",
    "        # Conclus√£o\n",
    "        elementos.extend(self._criar_conclusao(anomalias))\n",
    "        \n",
    "        # Gerar PDF\n",
    "        doc.build(elementos, onFirstPage=self._adicionar_cabecalho_rodape,\n",
    "                 onLaterPages=self._adicionar_cabecalho_rodape)\n",
    "        \n",
    "        self.logger.info(f\"Relat√≥rio PDF gerado: {arquivo_saida}\")\n",
    "        return arquivo_saida\n",
    "    \n",
    "    def _criar_capa(self) -> List:\n",
    "        \"\"\"Cria p√°gina de capa do relat√≥rio\"\"\"\n",
    "        elementos = []\n",
    "        \n",
    "        # Logo/T√≠tulo\n",
    "        elementos.append(Spacer(1, 2*inch))\n",
    "        elementos.append(Paragraph(\n",
    "            \"RELAT√ìRIO DE AN√ÅLISE ESOCIAL\",\n",
    "            self.styles['TituloPrincipal']\n",
    "        ))\n",
    "        \n",
    "        elementos.append(Spacer(1, 0.5*inch))\n",
    "        elementos.append(Paragraph(\n",
    "            \"Sistema de Detec√ß√£o de Anomalias - Estado da Arte\",\n",
    "            self.styles['TituloSecao']\n",
    "        ))\n",
    "        \n",
    "        elementos.append(Spacer(1, 2*inch))\n",
    "        \n",
    "        # Informa√ß√µes\n",
    "        info_table = [\n",
    "            [\"Para:\", \"DATAPREV - Diretoria de Produtos e Solu√ß√µes\"],\n",
    "            [\"De:\", \"Sistema Automatizado de An√°lise eSocial\"],\n",
    "            [\"Data:\", datetime.now().strftime(\"%d/%m/%Y\")],\n",
    "            [\"Vers√£o:\", VERSAO_SISTEMA],\n",
    "            [\"Status:\", \"AN√ÅLISE CONCLU√çDA\"]\n",
    "        ]\n",
    "        \n",
    "        t = RLTable(info_table, colWidths=[2*inch, 4*inch])\n",
    "        t.setStyle(TableStyle([\n",
    "            ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),\n",
    "            ('FONTSIZE', (0, 0), (-1, -1), 12),\n",
    "            ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n",
    "            ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
    "            ('VALIGN', (0, 0), (-1, -1), 'TOP'),\n",
    "            ('TEXTCOLOR', (0, 0), (0, -1), colors.HexColor('#1B4F72')),\n",
    "            ('GRID', (0, 0), (-1, -1), 0.5, colors.grey),\n",
    "            ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#EBF5FB')),\n",
    "        ]))\n",
    "        \n",
    "        elementos.append(t)\n",
    "        \n",
    "        elementos.append(Spacer(1, 1*inch))\n",
    "        elementos.append(Paragraph(\n",
    "            \"CONFIDENCIAL - USO RESTRITO\",\n",
    "            self.styles['Destaque']\n",
    "        ))\n",
    "        \n",
    "        return elementos\n",
    "    \n",
    "    def _criar_sumario_executivo(self, df: pd.DataFrame, anomalias: Dict, ml_result: pd.DataFrame) -> List:\n",
    "        \"\"\"Cria sum√°rio executivo do relat√≥rio\"\"\"\n",
    "        elementos = []\n",
    "        \n",
    "        elementos.append(Paragraph(\"SUM√ÅRIO EXECUTIVO\", self.styles['TituloSecao']))\n",
    "        elementos.append(Spacer(1, 0.2*inch))\n",
    "        \n",
    "        # Estat√≠sticas principais\n",
    "        total_anomalias = sum(len(v) for v in anomalias.values())\n",
    "        anomalias_criticas = sum(1 for v in anomalias.values() for a in v if a.severidade == 'CRITICA')\n",
    "        taxa_ml = (ml_result['anomalia_ml'].sum() / len(ml_result) * 100) if len(ml_result) > 0 else 0\n",
    "        \n",
    "        # Texto introdut√≥rio\n",
    "        elementos.append(Paragraph(\n",
    "            \"Este relat√≥rio apresenta os resultados da an√°lise automatizada dos dados eSocial \"\n",
    "            \"processados pelo sistema estado da arte de detec√ß√£o de anomalias.\",\n",
    "            self.styles['BodyText']\n",
    "        ))\n",
    "        \n",
    "        elementos.append(Spacer(1, 0.2*inch))\n",
    "        elementos.append(Paragraph(\"<b>Principais Descobertas:</b>\", self.styles['BodyText']))\n",
    "        \n",
    "        # Estat√≠sticas\n",
    "        descobertas_texto = f\"\"\"\n",
    "        ‚Ä¢ Total de registros analisados: <b>{len(df):,}</b><br/>\n",
    "        ‚Ä¢ Anomalias detectadas: <b>{total_anomalias:,}</b><br/>\n",
    "        ‚Ä¢ Anomalias cr√≠ticas: <b>{anomalias_criticas:,}</b><br/>\n",
    "        ‚Ä¢ Taxa de detec√ß√£o ML: <b>{taxa_ml:.2f}%</b><br/>\n",
    "        ‚Ä¢ Estabelecimentos √∫nicos: <b>{df['NU_INSCRICAO_ESTABELECIM'].nunique():,}</b>\n",
    "        \"\"\"\n",
    "        elementos.append(Paragraph(descobertas_texto, self.styles['BodyText']))\n",
    "        \n",
    "        elementos.append(Spacer(1, 0.2*inch))\n",
    "        elementos.append(Paragraph(\n",
    "            '<b>Status da An√°lise:</b> <font color=\"red\">A√á√ÉO NECESS√ÅRIA</font>',\n",
    "            self.styles['BodyText']\n",
    "        ))\n",
    "        \n",
    "        elementos.append(Paragraph(\n",
    "            \"Foram identificadas m√∫ltiplas anomalias cr√≠ticas que requerem aten√ß√£o imediata \"\n",
    "            \"e poss√≠vel reprocessamento dos dados de extra√ß√£o.\",\n",
    "            self.styles['BodyText']\n",
    "        ))\n",
    "        \n",
    "        # Tabela resumo de anomalias\n",
    "        elementos.append(Spacer(1, 0.3*inch))\n",
    "        elementos.append(Paragraph(\"Distribui√ß√£o de Anomalias por Categoria:\", self.styles['Heading3']))\n",
    "        \n",
    "        dados_tabela = [['Categoria', 'Quantidade', 'Cr√≠ticas', 'A√ß√£o Requerida']]\n",
    "        \n",
    "        for tipo, lista_anom in anomalias.items():\n",
    "            if lista_anom:\n",
    "                criticas = sum(1 for a in lista_anom if a.severidade == 'CRITICA')\n",
    "                acao = 'URGENTE' if criticas > 0 else 'REVISAR'\n",
    "                dados_tabela.append([\n",
    "                    tipo.upper(),\n",
    "                    str(len(lista_anom)),\n",
    "                    str(criticas),\n",
    "                    acao\n",
    "                ])\n",
    "        \n",
    "        t = RLTable(dados_tabela, colWidths=[2.5*inch, 1.5*inch, 1.5*inch, 1.5*inch])\n",
    "        t.setStyle(TableStyle([\n",
    "            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "            ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),\n",
    "            ('FONTSIZE', (0, 0), (-1, -1), 10),\n",
    "            ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#2874A6')),\n",
    "            ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),\n",
    "            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
    "            ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),\n",
    "            ('GRID', (0, 0), (-1, -1), 0.5, colors.grey),\n",
    "            ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.HexColor('#EBF5FB')]),\n",
    "        ]))\n",
    "        \n",
    "        elementos.append(t)\n",
    "        \n",
    "        return elementos\n",
    "    \n",
    "    def _criar_analise_critica(self, anomalias: Dict) -> List:\n",
    "        \"\"\"Cria an√°lise das anomalias cr√≠ticas\"\"\"\n",
    "        elementos = []\n",
    "        \n",
    "        elementos.append(Paragraph(\"AN√ÅLISE DE ANOMALIAS CR√çTICAS\", self.styles['TituloSecao']))\n",
    "        elementos.append(Spacer(1, 0.2*inch))\n",
    "        \n",
    "        # Filtrar apenas anomalias cr√≠ticas\n",
    "        anomalias_criticas = []\n",
    "        for tipo, lista in anomalias.items():\n",
    "            for anom in lista:\n",
    "                if anom.severidade == 'CRITICA':\n",
    "                    anomalias_criticas.append((tipo, anom))\n",
    "        \n",
    "        if not anomalias_criticas:\n",
    "            elementos.append(Paragraph(\n",
    "                \"Nenhuma anomalia cr√≠tica foi detectada.\",\n",
    "                self.styles['BodyText']\n",
    "            ))\n",
    "            return elementos\n",
    "        \n",
    "        # Agrupar por tipo de anomalia\n",
    "        criticas_por_tipo = defaultdict(list)\n",
    "        for tipo, anom in anomalias_criticas:\n",
    "            criticas_por_tipo[anom.tipo.value].append(anom)\n",
    "        \n",
    "        # Top 5 tipos mais cr√≠ticos\n",
    "        top_tipos = sorted(criticas_por_tipo.items(), \n",
    "                          key=lambda x: len(x[1]), \n",
    "                          reverse=True)[:5]\n",
    "        \n",
    "        for tipo_codigo, lista_anom in top_tipos:\n",
    "            elementos.append(Paragraph(\n",
    "                f\"<b>{tipo_codigo}</b> ({len(lista_anom)} ocorr√™ncias)\",\n",
    "                self.styles['Heading3']\n",
    "            ))\n",
    "            \n",
    "            # Exemplos\n",
    "            exemplos = lista_anom[:3]  # At√© 3 exemplos\n",
    "            for anom in exemplos:\n",
    "                texto = f\"\"\"\n",
    "                ‚Ä¢ CNPJ: {anom.cnpj}<br/>\n",
    "                ‚Ä¢ Per√≠odo: {anom.periodo}<br/>\n",
    "                ‚Ä¢ Descri√ß√£o: {anom.descricao}<br/>\n",
    "                ‚Ä¢ Impacto: R$ {anom.impacto_financeiro:,.2f}\n",
    "                \"\"\"\n",
    "                elementos.append(Paragraph(texto, self.styles['BodyText']))\n",
    "            \n",
    "            elementos.append(Spacer(1, 0.1*inch))\n",
    "        \n",
    "        # Alerta especial\n",
    "        elementos.append(Spacer(1, 0.2*inch))\n",
    "        elementos.append(Paragraph(\n",
    "            \"<b>ATEN√á√ÉO:</b> As anomalias cr√≠ticas identificadas podem resultar em \"\n",
    "            \"multas, penalidades e problemas de conformidade. A√ß√£o imediata √© recomendada.\",\n",
    "            self.styles['Destaque']\n",
    "        ))\n",
    "        \n",
    "        return elementos\n",
    "    \n",
    "    def _criar_recomendacoes_dataprev(self, anomalias: Dict) -> List:\n",
    "        \"\"\"Cria recomenda√ß√µes espec√≠ficas para Dataprev\"\"\"\n",
    "        elementos = []\n",
    "        \n",
    "        elementos.append(Paragraph(\"RECOMENDA√á√ïES PARA DATAPREV\", self.styles['TituloSecao']))\n",
    "        elementos.append(Spacer(1, 0.2*inch))\n",
    "        \n",
    "        # An√°lise se precisa refazer extra√ß√£o\n",
    "        total_criticas = sum(1 for v in anomalias.values() for a in v if a.severidade == 'CRITICA')\n",
    "        precisa_reextracao = total_criticas > 100  # Threshold\n",
    "        \n",
    "        if precisa_reextracao:\n",
    "            elementos.append(Paragraph(\n",
    "                \"<b>RECOMENDA√á√ÉO PRINCIPAL: REFAZER EXTRA√á√ÉO DOS DADOS</b>\",\n",
    "                self.styles['Destaque']\n",
    "            ))\n",
    "            \n",
    "            elementos.append(Spacer(1, 0.2*inch))\n",
    "            elementos.append(Paragraph(\n",
    "                \"Com base na quantidade e severidade das anomalias detectadas, recomendamos \"\n",
    "                \"fortemente que a extra√ß√£o dos dados eSocial seja refeita, considerando:\",\n",
    "                self.styles['BodyText']\n",
    "            ))\n",
    "            \n",
    "            elementos.append(Paragraph(\n",
    "                \"<b>1. Valida√ß√£o dos Filtros de Extra√ß√£o:</b>\",\n",
    "                self.styles['BodyText']\n",
    "            ))\n",
    "            elementos.append(Paragraph(\n",
    "                \"‚Ä¢ Verificar par√¢metros de data (per√≠odo base)<br/>\"\n",
    "                \"‚Ä¢ Confirmar tipos de inscri√ß√£o inclu√≠dos<br/>\"\n",
    "                \"‚Ä¢ Validar categorias de segurados\",\n",
    "                self.styles['BodyText']\n",
    "            ))\n",
    "            \n",
    "            elementos.append(Paragraph(\n",
    "                \"<b>2. Integridade dos Dados:</b>\",\n",
    "                self.styles['BodyText']\n",
    "            ))\n",
    "            elementos.append(Paragraph(\n",
    "                \"‚Ä¢ Verificar se todos os eventos S-1299 foram processados<br/>\"\n",
    "                \"‚Ä¢ Confirmar totaliza√ß√£o com S-5011<br/>\"\n",
    "                \"‚Ä¢ Validar convers√£o de CNO para CNPJ\",\n",
    "                self.styles['BodyText']\n",
    "            ))\n",
    "            \n",
    "            elementos.append(Paragraph(\n",
    "                \"<b>3. Qualidade da Extra√ß√£o:</b>\",\n",
    "                self.styles['BodyText']\n",
    "            ))\n",
    "            elementos.append(Paragraph(\n",
    "                \"‚Ä¢ Usar encoding UTF-8<br/>\"\n",
    "                \"‚Ä¢ Validar formato posicional (679 caracteres)<br/>\"\n",
    "                \"‚Ä¢ Confirmar aus√™ncia de truncamento\",\n",
    "                self.styles['BodyText']\n",
    "            ))\n",
    "        else:\n",
    "            elementos.append(Paragraph(\n",
    "                \"<b>RECOMENDA√á√ÉO: CORRE√á√ïES PONTUAIS</b>\",\n",
    "                self.styles['Heading3']\n",
    "            ))\n",
    "            \n",
    "            elementos.append(Paragraph(\n",
    "                \"A extra√ß√£o atual pode ser mantida, mas recomendamos as seguintes corre√ß√µes:\",\n",
    "                self.styles['BodyText']\n",
    "            ))\n",
    "            \n",
    "            elementos.append(Paragraph(\n",
    "                \"‚Ä¢ Aplicar corre√ß√µes autom√°ticas identificadas<br/>\"\n",
    "                \"‚Ä¢ Revisar registros com anomalias cr√≠ticas<br/>\"\n",
    "                \"‚Ä¢ Validar per√≠odos com maior concentra√ß√£o de erros<br/>\"\n",
    "                \"‚Ä¢ Implementar valida√ß√µes adicionais no processo\",\n",
    "                self.styles['BodyText']\n",
    "            ))\n",
    "        \n",
    "        # Checklist de valida√ß√£o\n",
    "        elementos.append(Spacer(1, 0.3*inch))\n",
    "        elementos.append(Paragraph(\"Checklist de Valida√ß√£o P√≥s-Extra√ß√£o:\", self.styles['Heading3']))\n",
    "        \n",
    "        checklist = [\n",
    "            [\"Item\", \"Status\", \"Observa√ß√£o\"],\n",
    "            [\"Layout com 70 campos\", \"‚úì\", \"Todos os campos presentes\"],\n",
    "            [\"Formato AAAAMMDDHHMMSS\", \"‚úì\" if not anomalias.get('temporais') else \"‚úó\", \"Verificar datas\"],\n",
    "            [\"CNPJs v√°lidos\", \"‚úó\" if anomalias.get('estruturais') else \"‚úì\", \"M√∫ltiplos CNPJs inv√°lidos\"],\n",
    "            [\"Totaliza√ß√µes consistentes\", \"‚úó\" if anomalias.get('conformidade') else \"‚úì\", \"Diverg√™ncias detectadas\"],\n",
    "            [\"Recibo S-1299\", \"‚úó\" if anomalias.get('s5011') else \"‚úì\", \"Verificar recibos\"],\n",
    "        ]\n",
    "        \n",
    "        t = RLTable(checklist, colWidths=[2.5*inch, 1*inch, 3*inch])\n",
    "        t.setStyle(TableStyle([\n",
    "            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "            ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#2874A6')),\n",
    "            ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),\n",
    "            ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
    "            ('GRID', (0, 0), (-1, -1), 0.5, colors.grey),\n",
    "        ]))\n",
    "        \n",
    "        elementos.append(t)\n",
    "        \n",
    "        return elementos\n",
    "    \n",
    "    def _criar_analise_conformidade(self, df: pd.DataFrame, anomalias: Dict) -> List:\n",
    "        \"\"\"Cria an√°lise de conformidade com legisla√ß√£o\"\"\"\n",
    "        elementos = []\n",
    "        \n",
    "        elementos.append(Paragraph(\"AN√ÅLISE DE CONFORMIDADE LEGAL\", self.styles['TituloSecao']))\n",
    "        elementos.append(Spacer(1, 0.2*inch))\n",
    "        \n",
    "        # Conformidade com DM.204661\n",
    "        elementos.append(Paragraph(\n",
    "            \"<b>Conformidade com DM.204661 v1.9:</b>\",\n",
    "            self.styles['BodyText']\n",
    "        ))\n",
    "        \n",
    "        elementos.append(Paragraph(\n",
    "            \"O sistema verificou conformidade com os seguintes requisitos:\",\n",
    "            self.styles['BodyText']\n",
    "        ))\n",
    "        \n",
    "        # Tabela de conformidade\n",
    "        requisitos = [\n",
    "            [\"Requisito\", \"Status\", \"Observa√ß√£o\"],\n",
    "            [\"70 campos do layout\", \"CONFORME\", \"Todos os campos mapeados\"],\n",
    "            [\"Per√≠odo base 2 anos\", \"CONFORME\", f\"Dados de {df['NU_PERIODO_REFERENCIA'].min()} a {df['NU_PERIODO_REFERENCIA'].max()}\"],\n",
    "            [\"M√°ximo 13 registros/ano\", \"N√ÉO CONFORME\" if anomalias.get('negocio') else \"CONFORME\", \"Verificar duplica√ß√µes\"],\n",
    "            [\"Categorias v√°lidas\", \"CONFORME\", \"Apenas categorias permitidas\"],\n",
    "            [\"Convers√£o CNO\", \"CONFORME\", \"CNO convertidos para CNPJ\"],\n",
    "            [\"Valida√ß√£o S-1299\", \"PARCIAL\", \"Alguns recibos ausentes\"],\n",
    "        ]\n",
    "        \n",
    "        t = RLTable(requisitos, colWidths=[2.5*inch, 1.5*inch, 2.5*inch])\n",
    "        t.setStyle(TableStyle([\n",
    "            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "            ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#2874A6')),\n",
    "            ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),\n",
    "            ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
    "            ('GRID', (0, 0), (-1, -1), 0.5, colors.grey),\n",
    "            # Destacar n√£o conformidades\n",
    "            ('TEXTCOLOR', (1, 3), (1, 3), colors.red),\n",
    "            ('FONTNAME', (1, 3), (1, 3), 'Helvetica-Bold'),\n",
    "        ]))\n",
    "        \n",
    "        elementos.append(t)\n",
    "        \n",
    "        # Resolu√ß√£o CNPS\n",
    "        elementos.append(Spacer(1, 0.2*inch))\n",
    "        elementos.append(Paragraph(\n",
    "            \"<b>Conformidade com Resolu√ß√£o CNPS 1.347/2021:</b>\",\n",
    "            self.styles['BodyText']\n",
    "        ))\n",
    "        \n",
    "        elementos.append(Paragraph(\n",
    "            \"O sistema est√° em conformidade com as diretrizes estabelecidas para o c√°lculo \"\n",
    "            \"do FAP, incluindo a correta identifica√ß√£o de v√≠nculos, base de c√°lculo e \"\n",
    "            \"categorias de segurados.\",\n",
    "            self.styles['BodyText']\n",
    "        ))\n",
    "        \n",
    "        return elementos\n",
    "    \n",
    "    def _criar_conclusao(self, anomalias: Dict) -> List:\n",
    "        \"\"\"Cria conclus√£o do relat√≥rio\"\"\"\n",
    "        elementos = []\n",
    "        \n",
    "        elementos.append(Paragraph(\"CONCLUS√ÉO E PR√ìXIMOS PASSOS\", self.styles['TituloSecao']))\n",
    "        elementos.append(Spacer(1, 0.2*inch))\n",
    "        \n",
    "        total_anomalias = sum(len(v) for v in anomalias.values())\n",
    "        criticas = sum(1 for v in anomalias.values() for a in v if a.severidade == 'CRITICA')\n",
    "        \n",
    "        if criticas > 100:\n",
    "            elementos.append(Paragraph(\n",
    "                \"<b>CONCLUS√ÉO: A√á√ÉO URGENTE NECESS√ÅRIA</b>\",\n",
    "                self.styles['BodyText']\n",
    "            ))\n",
    "            \n",
    "            elementos.append(Paragraph(\n",
    "                \"A an√°lise identificou um n√∫mero significativo de anomalias cr√≠ticas que \"\n",
    "                \"comprometem a integridade dos dados. Recomendamos fortemente:\",\n",
    "                self.styles['BodyText']\n",
    "            ))\n",
    "            \n",
    "            recomendacoes = \"\"\"\n",
    "            1. <b>REFAZER A EXTRA√á√ÉO</b> dos dados eSocial com os par√¢metros corrigidos<br/>\n",
    "            2. <b>VALIDAR</b> o novo arquivo antes do processamento<br/>\n",
    "            3. <b>EXECUTAR</b> nova an√°lise de anomalias<br/>\n",
    "            4. <b>DOCUMENTAR</b> todas as corre√ß√µes aplicadas\n",
    "            \"\"\"\n",
    "            elementos.append(Paragraph(recomendacoes, self.styles['BodyText']))\n",
    "        else:\n",
    "            elementos.append(Paragraph(\n",
    "                \"<b>CONCLUS√ÉO: DADOS UTILIZ√ÅVEIS COM CORRE√á√ïES</b>\",\n",
    "                self.styles['BodyText']\n",
    "            ))\n",
    "            \n",
    "            elementos.append(Paragraph(\n",
    "                f\"Foram identificadas {total_anomalias:,} anomalias, sendo {criticas} cr√≠ticas. \"\n",
    "                f\"Os dados podem ser utilizados ap√≥s aplica√ß√£o das corre√ß√µes autom√°ticas e \"\n",
    "                f\"revis√£o dos casos cr√≠ticos identificados.\",\n",
    "                self.styles['BodyText']\n",
    "            ))\n",
    "        \n",
    "        # Contato\n",
    "        elementos.append(Spacer(1, 0.5*inch))\n",
    "        contato_texto = f\"\"\"\n",
    "        <b>Para mais informa√ß√µes:</b><br/>\n",
    "        Sistema de An√°lise eSocial - Estado da Arte<br/>\n",
    "        Vers√£o: {VERSAO_SISTEMA}<br/>\n",
    "        Data: {datetime.now().strftime(\"%d/%m/%Y %H:%M\")}\n",
    "        \"\"\"\n",
    "        elementos.append(Paragraph(contato_texto, self.styles['BodyText']))\n",
    "        \n",
    "        return elementos\n",
    "    \n",
    "    def _adicionar_cabecalho_rodape(self, canvas, doc):\n",
    "        \"\"\"Adiciona cabe√ßalho e rodap√© √†s p√°ginas\"\"\"\n",
    "        canvas.saveState()\n",
    "        \n",
    "        # Cabe√ßalho\n",
    "        canvas.setFont('Helvetica', 9)\n",
    "        canvas.drawString(inch, A4[1] - 0.5*inch, \"DATAPREV - An√°lise eSocial\")\n",
    "        canvas.drawRightString(A4[0] - inch, A4[1] - 0.5*inch, \n",
    "                              datetime.now().strftime(\"%d/%m/%Y\"))\n",
    "        \n",
    "        # Linha\n",
    "        canvas.line(inch, A4[1] - 0.6*inch, A4[0] - inch, A4[1] - 0.6*inch)\n",
    "        \n",
    "        # Rodap√©\n",
    "        canvas.drawString(inch, 0.5*inch, \"Confidencial - Uso Restrito\")\n",
    "        canvas.drawRightString(A4[0] - inch, 0.5*inch, f\"P√°gina {doc.page}\")\n",
    "        \n",
    "        canvas.restoreState()\n",
    "\n",
    "\n",
    "class SistemaBackupSeguranca:\n",
    "    \"\"\"Sistema de backup e seguran√ßa para dados sens√≠veis\"\"\"\n",
    "    \n",
    "    def __init__(self, diretorio_backup: str = \"backups\"):\n",
    "        self.logger = logging.getLogger(f\"{__name__}.Backup\")\n",
    "        self.diretorio_backup = Path(diretorio_backup)\n",
    "        self.diretorio_backup.mkdir(exist_ok=True)\n",
    "        \n",
    "        if not CRYPTO_AVAILABLE:\n",
    "            self.logger.warning(\"Sistema de backup rodando sem criptografia!\")\n",
    "            self.fernet = None\n",
    "        else:\n",
    "            # Gerar chave de criptografia\n",
    "            self.chave_cripto = Fernet.generate_key()\n",
    "            self.fernet = Fernet(self.chave_cripto)\n",
    "            \n",
    "            # Salvar chave em local seguro\n",
    "            self._salvar_chave_segura()\n",
    "        \n",
    "    def _salvar_chave_segura(self):\n",
    "        \"\"\"Salva chave de criptografia de forma segura\"\"\"\n",
    "        if not self.fernet:\n",
    "            return\n",
    "            \n",
    "        chave_path = self.diretorio_backup / '.chave_cripto'\n",
    "        with open(chave_path, 'wb') as f:\n",
    "            f.write(self.chave_cripto)\n",
    "        \n",
    "        # Definir permiss√µes restritas (Windows)\n",
    "        try:\n",
    "            import stat\n",
    "            import os\n",
    "            os.chmod(chave_path, stat.S_IREAD | stat.S_IWRITE)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    def fazer_backup(self, dados: pd.DataFrame, nome_backup: str = None) -> str:\n",
    "        \"\"\"Faz backup criptografado dos dados\"\"\"\n",
    "        if nome_backup is None:\n",
    "            nome_backup = f\"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        self.logger.info(f\"Criando backup: {nome_backup}\")\n",
    "        \n",
    "        # Serializar dados\n",
    "        dados_json = dados.to_json(orient='records')\n",
    "        dados_bytes = dados_json.encode('utf-8')\n",
    "        \n",
    "        if self.fernet:\n",
    "            # Criptografar\n",
    "            dados_salvos = self.fernet.encrypt(dados_bytes)\n",
    "            extensao = '.enc'\n",
    "        else:\n",
    "            # Salvar sem criptografia\n",
    "            dados_salvos = dados_bytes\n",
    "            extensao = '.json'\n",
    "        \n",
    "        # Salvar\n",
    "        arquivo_backup = self.diretorio_backup / f\"{nome_backup}{extensao}\"\n",
    "        with open(arquivo_backup, 'wb') as f:\n",
    "            f.write(dados_salvos)\n",
    "        \n",
    "        # Criar metadados\n",
    "        metadados = {\n",
    "            'data_criacao': datetime.now().isoformat(),\n",
    "            'tamanho_original': len(dados_bytes),\n",
    "            'tamanho_salvo': len(dados_salvos),\n",
    "            'registros': len(dados),\n",
    "            'hash_sha256': hashlib.sha256(dados_bytes).hexdigest(),\n",
    "            'criptografado': bool(self.fernet)\n",
    "        }\n",
    "        \n",
    "        with open(self.diretorio_backup / f\"{nome_backup}.meta\", 'w') as f:\n",
    "            json.dump(metadados, f, indent=2)\n",
    "        \n",
    "        self.logger.info(f\"Backup criado com sucesso: {arquivo_backup}\")\n",
    "        return str(arquivo_backup)\n",
    "    \n",
    "    def restaurar_backup(self, nome_backup: str) -> pd.DataFrame:\n",
    "        \"\"\"Restaura backup criptografado\"\"\"\n",
    "        # Tentar ambas as extens√µes\n",
    "        for ext in ['.enc', '.json']:\n",
    "            arquivo_backup = self.diretorio_backup / f\"{nome_backup}{ext}\"\n",
    "            if arquivo_backup.exists():\n",
    "                break\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Backup n√£o encontrado: {nome_backup}\")\n",
    "        \n",
    "        self.logger.info(f\"Restaurando backup: {nome_backup}\")\n",
    "        \n",
    "        # Ler dados\n",
    "        with open(arquivo_backup, 'rb') as f:\n",
    "            dados_salvos = f.read()\n",
    "        \n",
    "        if arquivo_backup.suffix == '.enc' and self.fernet:\n",
    "            # Descriptografar\n",
    "            dados_bytes = self.fernet.decrypt(dados_salvos)\n",
    "        else:\n",
    "            dados_bytes = dados_salvos\n",
    "        \n",
    "        # Desserializar\n",
    "        dados_json = dados_bytes.decode('utf-8')\n",
    "        df = pd.read_json(dados_json, orient='records')\n",
    "        \n",
    "        self.logger.info(f\"Backup restaurado: {len(df)} registros\")\n",
    "        return df\n",
    "    \n",
    "    def listar_backups(self) -> List[Dict]:\n",
    "        \"\"\"Lista todos os backups dispon√≠veis\"\"\"\n",
    "        backups = []\n",
    "        \n",
    "        for arquivo in self.diretorio_backup.glob(\"*.enc\"):\n",
    "            nome = arquivo.stem\n",
    "            meta_path = arquivo.with_suffix('.meta')\n",
    "            \n",
    "            if meta_path.exists():\n",
    "                with open(meta_path, 'r') as f:\n",
    "                    metadados = json.load(f)\n",
    "            else:\n",
    "                metadados = {'data_criacao': 'Desconhecida'}\n",
    "            \n",
    "            backups.append({\n",
    "                'nome': nome,\n",
    "                'arquivo': str(arquivo),\n",
    "                'data': metadados.get('data_criacao', 'Desconhecida'),\n",
    "                'registros': metadados.get('registros', 0)\n",
    "            })\n",
    "        \n",
    "        return sorted(backups, key=lambda x: x['data'], reverse=True)\n",
    "    \n",
    "    def limpar_backups_antigos(self, dias: int = 30):\n",
    "        \"\"\"Remove backups mais antigos que X dias\"\"\"\n",
    "        from datetime import timedelta\n",
    "        limite = datetime.now() - timedelta(days=dias)\n",
    "        \n",
    "        for arquivo in self.diretorio_backup.glob(\"*\"):\n",
    "            if arquivo.suffix in ['.enc', '.json', '.meta']:\n",
    "                if arquivo.stat().st_mtime < limite.timestamp():\n",
    "                    self.logger.info(f\"Removendo backup antigo: {arquivo}\")\n",
    "                    arquivo.unlink()\n",
    "\n",
    "\n",
    "# Criar inst√¢ncias\n",
    "gerador_pdf = GeradorRelatorioPDF()\n",
    "sistema_backup = SistemaBackupSeguranca()\n",
    "\n",
    "logger.info(\"Gerador de PDF configurado para relat√≥rios Dataprev\")\n",
    "logger.info(\"Sistema de backup e seguran√ßa configurado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-17 09:21:36 | INFO     | __main__ | 2665069436:<module>:557 | API REST configurada com autentica√ß√£o JWT\n",
      "2025-06-17 09:21:36 | INFO     | __main__ | 2665069436:<module>:558 | Dashboard interativo dispon√≠vel via Streamlit\n",
      "2025-06-17 09:21:36 | INFO     | __main__ | 2665069436:<module>:559 | Para iniciar API: iniciar_api()\n",
      "2025-06-17 09:21:36 | INFO     | __main__ | 2665069436:<module>:560 | Para dashboard Streamlit: streamlit run dashboard_esocial.py\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 7: API REST e Dashboard Interativo\n",
    "\n",
    "# Importa√ß√µes necess√°rias para a API\n",
    "from fastapi.responses import HTMLResponse, FileResponse\n",
    "\n",
    "# Modelos Pydantic para API\n",
    "class ProcessamentoRequest(BaseModel):\n",
    "    arquivo_path: str\n",
    "    tipo_analise: str = \"completa\"\n",
    "    gerar_relatorios: bool = True\n",
    "    usar_ml: bool = True\n",
    "    \n",
    "class AnomaliaResponse(BaseModel):\n",
    "    tipo: str\n",
    "    severidade: str\n",
    "    campo: str\n",
    "    descricao: str\n",
    "    cnpj: str\n",
    "    periodo: str\n",
    "    \n",
    "class StatusResponse(BaseModel):\n",
    "    status: str\n",
    "    timestamp: datetime\n",
    "    registros_processados: int\n",
    "    anomalias_detectadas: int\n",
    "    tempo_processamento: float\n",
    "\n",
    "# API FastAPI\n",
    "app = FastAPI(\n",
    "    title=\"API eSocial An√°lise - Estado da Arte\",\n",
    "    description=\"API para an√°lise avan√ßada de anomalias em dados eSocial\",\n",
    "    version=VERSAO_SISTEMA\n",
    ")\n",
    "\n",
    "# Middleware CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Estado global da aplica√ß√£o\n",
    "class EstadoGlobal:\n",
    "    def __init__(self):\n",
    "        self.processamento_em_andamento = False\n",
    "        self.ultimo_resultado = None\n",
    "        self.historico_processamentos = []\n",
    "        self.cache_resultados = {}\n",
    "        \n",
    "estado_global = EstadoGlobal()\n",
    "\n",
    "# Autentica√ß√£o JWT\n",
    "security = HTTPBearer()\n",
    "\n",
    "def verificar_token(credentials: HTTPAuthorizationCredentials = Depends(security)):\n",
    "    \"\"\"Verifica token JWT\"\"\"\n",
    "    token = credentials.credentials\n",
    "    try:\n",
    "        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n",
    "        return payload\n",
    "    except JWTError:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "            detail=\"Token inv√°lido ou expirado\"\n",
    "        )\n",
    "\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "async def root():\n",
    "    \"\"\"P√°gina inicial da API\"\"\"\n",
    "    return \"\"\"\n",
    "    <html>\n",
    "        <head>\n",
    "            <title>API eSocial An√°lise</title>\n",
    "            <style>\n",
    "                body { font-family: Arial, sans-serif; margin: 40px; }\n",
    "                h1 { color: #2874A6; }\n",
    "                .endpoint { background: #f0f0f0; padding: 10px; margin: 10px 0; }\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>API eSocial An√°lise - Estado da Arte</h1>\n",
    "            <p>Sistema profissional de an√°lise de anomalias eSocial</p>\n",
    "            \n",
    "            <h2>Endpoints Dispon√≠veis:</h2>\n",
    "            <div class=\"endpoint\">\n",
    "                <strong>POST /processar</strong> - Processa arquivo eSocial\n",
    "            </div>\n",
    "            <div class=\"endpoint\">\n",
    "                <strong>GET /status</strong> - Status do processamento\n",
    "            </div>\n",
    "            <div class=\"endpoint\">\n",
    "                <strong>GET /anomalias</strong> - Lista anomalias detectadas\n",
    "            </div>\n",
    "            <div class=\"endpoint\">\n",
    "                <strong>GET /relatorio/{tipo}</strong> - Download de relat√≥rios\n",
    "            </div>\n",
    "            <div class=\"endpoint\">\n",
    "                <strong>GET /dashboard</strong> - Dashboard interativo\n",
    "            </div>\n",
    "            \n",
    "            <p>Documenta√ß√£o completa: <a href=\"/docs\">/docs</a></p>\n",
    "        </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "@app.post(\"/processar\", response_model=StatusResponse)\n",
    "async def processar_arquivo(\n",
    "    request: ProcessamentoRequest,\n",
    "    token_payload: dict = Depends(verificar_token)\n",
    "):\n",
    "    \"\"\"Processa arquivo eSocial e detecta anomalias\"\"\"\n",
    "    \n",
    "    if estado_global.processamento_em_andamento:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_409_CONFLICT,\n",
    "            detail=\"J√° existe um processamento em andamento\"\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        estado_global.processamento_em_andamento = True\n",
    "        inicio = datetime.now()\n",
    "        \n",
    "        # Processar arquivo\n",
    "        df = parser_esocial.parse_arquivo(request.arquivo_path)\n",
    "        \n",
    "        # Detectar anomalias\n",
    "        anomalias = detector_anomalias.detectar_todas_anomalias(df)\n",
    "        \n",
    "        # ML se solicitado\n",
    "        if request.usar_ml:\n",
    "            X = sistema_ml.preparar_features(df)\n",
    "            resultado_ml = sistema_ml.detectar_anomalias_ml(X, df)\n",
    "        else:\n",
    "            resultado_ml = df.copy()\n",
    "            resultado_ml['anomalia_ml'] = False\n",
    "        \n",
    "        # Gerar relat√≥rios se solicitado\n",
    "        if request.gerar_relatorios:\n",
    "            gerador_excel.gerar_relatorio_completo(df, anomalias, resultado_ml)\n",
    "            gerador_pdf.gerar_relatorio_dataprev(df, anomalias, resultado_ml)\n",
    "        \n",
    "        # Salvar resultado\n",
    "        tempo_total = (datetime.now() - inicio).total_seconds()\n",
    "        \n",
    "        resultado = {\n",
    "            'status': 'concluido',\n",
    "            'timestamp': datetime.now(),\n",
    "            'registros_processados': len(df),\n",
    "            'anomalias_detectadas': sum(len(v) for v in anomalias.values()),\n",
    "            'tempo_processamento': tempo_total,\n",
    "            'dados': df,\n",
    "            'anomalias': anomalias,\n",
    "            'resultado_ml': resultado_ml\n",
    "        }\n",
    "        \n",
    "        estado_global.ultimo_resultado = resultado\n",
    "        estado_global.historico_processamentos.append(resultado)\n",
    "        \n",
    "        return StatusResponse(**resultado)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro no processamento: {e}\")\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Erro no processamento: {str(e)}\"\n",
    "        )\n",
    "    finally:\n",
    "        estado_global.processamento_em_andamento = False\n",
    "\n",
    "@app.get(\"/status\", response_model=StatusResponse)\n",
    "async def obter_status(token_payload: dict = Depends(verificar_token)):\n",
    "    \"\"\"Obt√©m status do √∫ltimo processamento\"\"\"\n",
    "    \n",
    "    if estado_global.ultimo_resultado:\n",
    "        return StatusResponse(**estado_global.ultimo_resultado)\n",
    "    else:\n",
    "        return StatusResponse(\n",
    "            status=\"nenhum_processamento\",\n",
    "            timestamp=datetime.now(),\n",
    "            registros_processados=0,\n",
    "            anomalias_detectadas=0,\n",
    "            tempo_processamento=0\n",
    "        )\n",
    "\n",
    "@app.get(\"/anomalias\", response_model=List[AnomaliaResponse])\n",
    "async def listar_anomalias(\n",
    "    tipo: Optional[str] = None,\n",
    "    severidade: Optional[str] = None,\n",
    "    limite: int = 100,\n",
    "    token_payload: dict = Depends(verificar_token)\n",
    "):\n",
    "    \"\"\"Lista anomalias detectadas com filtros opcionais\"\"\"\n",
    "    \n",
    "    if not estado_global.ultimo_resultado:\n",
    "        return []\n",
    "    \n",
    "    anomalias_todas = []\n",
    "    for tipo_anom, lista in estado_global.ultimo_resultado['anomalias'].items():\n",
    "        if tipo and tipo != tipo_anom:\n",
    "            continue\n",
    "            \n",
    "        for anom in lista:\n",
    "            if severidade and severidade != anom.severidade:\n",
    "                continue\n",
    "                \n",
    "            anomalias_todas.append(AnomaliaResponse(\n",
    "                tipo=anom.tipo.value,\n",
    "                severidade=anom.severidade,\n",
    "                campo=anom.campo,\n",
    "                descricao=anom.descricao,\n",
    "                cnpj=anom.cnpj,\n",
    "                periodo=anom.periodo\n",
    "            ))\n",
    "    \n",
    "    return anomalias_todas[:limite]\n",
    "\n",
    "@app.get(\"/relatorio/{tipo}\")\n",
    "async def download_relatorio(\n",
    "    tipo: str,\n",
    "    token_payload: dict = Depends(verificar_token)\n",
    "):\n",
    "    \"\"\"Download de relat√≥rios gerados\"\"\"\n",
    "    \n",
    "    arquivos = {\n",
    "        'excel': 'relatorio_esocial_analise.xlsx',\n",
    "        'pdf': 'relatorio_dataprev_esocial.pdf'\n",
    "    }\n",
    "    \n",
    "    if tipo not in arquivos:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_404_NOT_FOUND,\n",
    "            detail=f\"Tipo de relat√≥rio inv√°lido. Op√ß√µes: {list(arquivos.keys())}\"\n",
    "        )\n",
    "    \n",
    "    arquivo_path = Path(arquivos[tipo])\n",
    "    \n",
    "    if not arquivo_path.exists():\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_404_NOT_FOUND,\n",
    "            detail=f\"Relat√≥rio {tipo} n√£o encontrado. Execute o processamento primeiro.\"\n",
    "        )\n",
    "    \n",
    "    return FileResponse(\n",
    "        arquivo_path,\n",
    "        media_type='application/octet-stream',\n",
    "        filename=arquivo_path.name\n",
    "    )\n",
    "\n",
    "@app.get(\"/dashboard\")\n",
    "async def dashboard():\n",
    "    \"\"\"Retorna p√°gina do dashboard interativo\"\"\"\n",
    "    \n",
    "    if not estado_global.ultimo_resultado:\n",
    "        return HTMLResponse(\"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <h1>Dashboard eSocial</h1>\n",
    "                <p>Nenhum processamento realizado ainda.</p>\n",
    "                <p><a href=\"/docs\">Ir para API</a></p>\n",
    "            </body>\n",
    "        </html>\n",
    "        \"\"\")\n",
    "    \n",
    "    # Criar dashboard com Plotly\n",
    "    df = estado_global.ultimo_resultado['dados']\n",
    "    anomalias = estado_global.ultimo_resultado['anomalias']\n",
    "    \n",
    "    # Gr√°ficos\n",
    "    fig1 = px.bar(\n",
    "        x=list(anomalias.keys()),\n",
    "        y=[len(v) for v in anomalias.values()],\n",
    "        title=\"Anomalias por Categoria\"\n",
    "    )\n",
    "    \n",
    "    # Dashboard HTML\n",
    "    dashboard_html = f\"\"\"\n",
    "    <html>\n",
    "        <head>\n",
    "            <title>Dashboard eSocial An√°lise</title>\n",
    "            <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "            <style>\n",
    "                body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "                .metric {{ \n",
    "                    display: inline-block; \n",
    "                    padding: 20px; \n",
    "                    margin: 10px;\n",
    "                    background: #f0f0f0; \n",
    "                    border-radius: 5px;\n",
    "                    text-align: center;\n",
    "                }}\n",
    "                .metric h2 {{ color: #2874A6; margin: 0; }}\n",
    "                .metric p {{ font-size: 24px; margin: 5px 0; }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>Dashboard de An√°lise eSocial</h1>\n",
    "            \n",
    "            <div class=\"metrics\">\n",
    "                <div class=\"metric\">\n",
    "                    <h2>Registros</h2>\n",
    "                    <p>{len(df):,}</p>\n",
    "                </div>\n",
    "                <div class=\"metric\">\n",
    "                    <h2>Anomalias</h2>\n",
    "                    <p>{sum(len(v) for v in anomalias.values()):,}</p>\n",
    "                </div>\n",
    "                <div class=\"metric\">\n",
    "                    <h2>Empresas</h2>\n",
    "                    <p>{df['NU_INSCRICAO_ESTABELECIM'].nunique():,}</p>\n",
    "                </div>\n",
    "            </div>\n",
    "            \n",
    "            <div id=\"grafico1\"></div>\n",
    "            \n",
    "            <script>\n",
    "                {fig1.to_html(include_plotlyjs=False, div_id=\"grafico1\")}\n",
    "            </script>\n",
    "        </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    return HTMLResponse(dashboard_html)\n",
    "\n",
    "@app.post(\"/backup\")\n",
    "async def criar_backup(\n",
    "    nome: Optional[str] = None,\n",
    "    token_payload: dict = Depends(verificar_token)\n",
    "):\n",
    "    \"\"\"Cria backup dos dados processados\"\"\"\n",
    "    \n",
    "    if not estado_global.ultimo_resultado:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_404_NOT_FOUND,\n",
    "            detail=\"Nenhum dado para fazer backup\"\n",
    "        )\n",
    "    \n",
    "    df = estado_global.ultimo_resultado['dados']\n",
    "    arquivo_backup = sistema_backup.fazer_backup(df, nome)\n",
    "    \n",
    "    return {\"mensagem\": \"Backup criado com sucesso\", \"arquivo\": arquivo_backup}\n",
    "\n",
    "@app.get(\"/backups\")\n",
    "async def listar_backups(token_payload: dict = Depends(verificar_token)):\n",
    "    \"\"\"Lista backups dispon√≠veis\"\"\"\n",
    "    return sistema_backup.listar_backups()\n",
    "\n",
    "\n",
    "# Dashboard Streamlit (arquivo separado: dashboard_esocial.py)\n",
    "def criar_dashboard_streamlit():\n",
    "    \"\"\"Cria dashboard interativo com Streamlit\"\"\"\n",
    "    \n",
    "    st.set_page_config(\n",
    "        page_title=\"Dashboard eSocial An√°lise\",\n",
    "        page_icon=\"üìä\",\n",
    "        layout=\"wide\"\n",
    "    )\n",
    "    \n",
    "    st.title(\"üöÄ Dashboard de An√°lise eSocial - Estado da Arte\")\n",
    "    \n",
    "    # Sidebar\n",
    "    st.sidebar.header(\"Configura√ß√µes\")\n",
    "    \n",
    "    # Upload de arquivo\n",
    "    arquivo = st.sidebar.file_uploader(\n",
    "        \"Carregar arquivo eSocial\",\n",
    "        type=['txt', 'csv', 'parquet']\n",
    "    )\n",
    "    \n",
    "    if arquivo:\n",
    "        # Processar arquivo\n",
    "        with st.spinner(\"Processando arquivo...\"):\n",
    "            df = parser_esocial.parse_arquivo(arquivo)\n",
    "            anomalias = detector_anomalias.detectar_todas_anomalias(df)\n",
    "            \n",
    "        # M√©tricas principais\n",
    "        col1, col2, col3, col4 = st.columns(4)\n",
    "        \n",
    "        with col1:\n",
    "            st.metric(\"Total de Registros\", f\"{len(df):,}\")\n",
    "        \n",
    "        with col2:\n",
    "            total_anomalias = sum(len(v) for v in anomalias.values())\n",
    "            st.metric(\"Anomalias Detectadas\", f\"{total_anomalias:,}\")\n",
    "        \n",
    "        with col3:\n",
    "            st.metric(\"Empresas √önicas\", f\"{df['NU_INSCRICAO_ESTABELECIM'].nunique():,}\")\n",
    "        \n",
    "        with col4:\n",
    "            criticas = sum(1 for v in anomalias.values() for a in v if a.severidade == 'CRITICA')\n",
    "            st.metric(\"Anomalias Cr√≠ticas\", f\"{criticas:,}\")\n",
    "        \n",
    "        # Tabs\n",
    "        tab1, tab2, tab3, tab4 = st.tabs(\n",
    "            [\"üìä Vis√£o Geral\", \"üîç Anomalias\", \"ü§ñ Machine Learning\", \"üìà An√°lises\"]\n",
    "        )\n",
    "        \n",
    "        with tab1:\n",
    "            st.header(\"Vis√£o Geral dos Dados\")\n",
    "            \n",
    "            # Gr√°fico de anomalias por tipo\n",
    "            fig_anomalias = px.bar(\n",
    "                x=list(anomalias.keys()),\n",
    "                y=[len(v) for v in anomalias.values()],\n",
    "                title=\"Distribui√ß√£o de Anomalias por Categoria\",\n",
    "                labels={'x': 'Categoria', 'y': 'Quantidade'}\n",
    "            )\n",
    "            st.plotly_chart(fig_anomalias, use_container_width=True)\n",
    "            \n",
    "            # Evolu√ß√£o temporal\n",
    "            if 'PERIODO_DT' not in df.columns:\n",
    "                df['PERIODO_DT'] = pd.to_datetime(\n",
    "                    df['NU_PERIODO_REFERENCIA'].astype(str).str[:6],\n",
    "                    format='%Y%m',\n",
    "                    errors='coerce'\n",
    "                )\n",
    "            \n",
    "            temporal = df.groupby('PERIODO_DT').agg({\n",
    "                'QT_VINCULOS': 'sum',\n",
    "                'VL_BASE_CALCULO_CONTRIB_PREV': 'sum'\n",
    "            }).reset_index()\n",
    "            \n",
    "            fig_temporal = px.line(\n",
    "                temporal,\n",
    "                x='PERIODO_DT',\n",
    "                y=['QT_VINCULOS', 'VL_BASE_CALCULO_CONTRIB_PREV'],\n",
    "                title=\"Evolu√ß√£o Temporal\"\n",
    "            )\n",
    "            st.plotly_chart(fig_temporal, use_container_width=True)\n",
    "        \n",
    "        with tab2:\n",
    "            st.header(\"An√°lise de Anomalias\")\n",
    "            \n",
    "            # Filtros\n",
    "            severidade_filtro = st.selectbox(\n",
    "                \"Filtrar por Severidade\",\n",
    "                [\"Todas\", \"CRITICA\", \"ALTA\", \"MEDIA\", \"BAIXA\"]\n",
    "            )\n",
    "            \n",
    "            # Tabela de anomalias\n",
    "            anomalias_lista = []\n",
    "            for tipo, lista in anomalias.items():\n",
    "                for anom in lista:\n",
    "                    if severidade_filtro == \"Todas\" or anom.severidade == severidade_filtro:\n",
    "                        anomalias_lista.append({\n",
    "                            'Tipo': tipo,\n",
    "                            'Severidade': anom.severidade,\n",
    "                            'Campo': anom.campo,\n",
    "                            'CNPJ': anom.cnpj,\n",
    "                            'Per√≠odo': anom.periodo,\n",
    "                            'Descri√ß√£o': anom.descricao\n",
    "                        })\n",
    "            \n",
    "            if anomalias_lista:\n",
    "                df_anomalias = pd.DataFrame(anomalias_lista)\n",
    "                st.dataframe(df_anomalias, use_container_width=True)\n",
    "                \n",
    "                # Download\n",
    "                csv = df_anomalias.to_csv(index=False)\n",
    "                st.download_button(\n",
    "                    label=\"üì• Download Anomalias CSV\",\n",
    "                    data=csv,\n",
    "                    file_name=\"anomalias_esocial.csv\",\n",
    "                    mime=\"text/csv\"\n",
    "                )\n",
    "        \n",
    "        with tab3:\n",
    "            st.header(\"An√°lise de Machine Learning\")\n",
    "            \n",
    "            if st.button(\"ü§ñ Executar An√°lise ML\"):\n",
    "                with st.spinner(\"Treinando modelos...\"):\n",
    "                    X = sistema_ml.preparar_features(df)\n",
    "                    sistema_ml.treinar_modelos(X)\n",
    "                    resultado_ml = sistema_ml.detectar_anomalias_ml(X, df)\n",
    "                \n",
    "                # Resultados por algoritmo\n",
    "                st.subheader(\"Resultados por Algoritmo\")\n",
    "                \n",
    "                resultados_algo = []\n",
    "                for algo, peso in sistema_ml.pesos_algoritmos.items():\n",
    "                    col_name = f'anomalia_{algo}'\n",
    "                    if col_name in resultado_ml.columns:\n",
    "                        qtd = resultado_ml[col_name].sum()\n",
    "                        perc = qtd / len(resultado_ml) * 100\n",
    "                        resultados_algo.append({\n",
    "                            'Algoritmo': algo.replace('_', ' ').title(),\n",
    "                            'Peso (%)': peso * 100,\n",
    "                            'Anomalias': qtd,\n",
    "                            'Percentual': f\"{perc:.2f}%\"\n",
    "                        })\n",
    "                \n",
    "                st.dataframe(pd.DataFrame(resultados_algo))\n",
    "                \n",
    "                # Top anomalias ML\n",
    "                st.subheader(\"Top 20 Anomalias por Score ML\")\n",
    "                top_ml = resultado_ml.nlargest(20, 'score_anomalia')[\n",
    "                    ['NU_INSCRICAO_ESTABELECIM', 'NU_PERIODO_REFERENCIA', \n",
    "                     'score_anomalia', 'severidade_ml']\n",
    "                ]\n",
    "                st.dataframe(top_ml)\n",
    "        \n",
    "        with tab4:\n",
    "            st.header(\"An√°lises Avan√ßadas\")\n",
    "            \n",
    "            # An√°lise por CNAE\n",
    "            cnae_analise = df.groupby('NU_CNAE_PREPONDERANTE').agg({\n",
    "                'QT_VINCULOS': 'sum',\n",
    "                'VL_BASE_CALCULO_CONTRIB_PREV': 'sum'\n",
    "            }).reset_index()\n",
    "            cnae_analise = cnae_analise.nlargest(20, 'QT_VINCULOS')\n",
    "            \n",
    "            fig_cnae = px.bar(\n",
    "                cnae_analise,\n",
    "                x='NU_CNAE_PREPONDERANTE',\n",
    "                y='QT_VINCULOS',\n",
    "                title=\"Top 20 CNAEs por Quantidade de V√≠nculos\"\n",
    "            )\n",
    "            st.plotly_chart(fig_cnae, use_container_width=True)\n",
    "            \n",
    "            # Matriz de correla√ß√£o\n",
    "            st.subheader(\"Matriz de Correla√ß√£o\")\n",
    "            campos_numericos = df.select_dtypes(include=[np.number]).columns\n",
    "            if len(campos_numericos) > 1:\n",
    "                corr_matrix = df[campos_numericos].corr()\n",
    "                fig_corr = px.imshow(\n",
    "                    corr_matrix,\n",
    "                    labels=dict(color=\"Correla√ß√£o\"),\n",
    "                    x=corr_matrix.columns,\n",
    "                    y=corr_matrix.columns\n",
    "                )\n",
    "                st.plotly_chart(fig_corr, use_container_width=True)\n",
    "        \n",
    "        # Gera√ß√£o de relat√≥rios\n",
    "        st.sidebar.header(\"Relat√≥rios\")\n",
    "        \n",
    "        if st.sidebar.button(\"üìä Gerar Relat√≥rio Excel\"):\n",
    "            with st.spinner(\"Gerando relat√≥rio Excel...\"):\n",
    "                arquivo_excel = gerador_excel.gerar_relatorio_completo(\n",
    "                    df, anomalias, resultado_ml if 'resultado_ml' in locals() else df\n",
    "                )\n",
    "            st.sidebar.success(f\"Relat√≥rio gerado: {arquivo_excel}\")\n",
    "        \n",
    "        if st.sidebar.button(\"üìÑ Gerar Relat√≥rio PDF\"):\n",
    "            with st.spinner(\"Gerando relat√≥rio PDF...\"):\n",
    "                arquivo_pdf = gerador_pdf.gerar_relatorio_dataprev(\n",
    "                    df, anomalias, resultado_ml if 'resultado_ml' in locals() else df\n",
    "                )\n",
    "            st.sidebar.success(f\"Relat√≥rio gerado: {arquivo_pdf}\")\n",
    "\n",
    "# Fun√ß√£o para iniciar API\n",
    "def iniciar_api(host: str = \"0.0.0.0\", port: int = 8000):\n",
    "    \"\"\"Inicia servidor da API\"\"\"\n",
    "    logger.info(f\"Iniciando API em http://{host}:{port}\")\n",
    "    uvicorn.run(app, host=host, port=port)\n",
    "\n",
    "logger.info(\"API REST configurada com autentica√ß√£o JWT\")\n",
    "logger.info(\"Dashboard interativo dispon√≠vel via Streamlit\")\n",
    "logger.info(\"Para iniciar API: iniciar_api()\")\n",
    "logger.info(\"Para dashboard Streamlit: streamlit run dashboard_esocial.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-17 09:21:36 | INFO     | __main__.SistemaPrincipal | 2466873862:configurar_sistema:14 | ================================================================================\n",
      "2025-06-17 09:21:36 | INFO     | __main__.SistemaPrincipal | 2466873862:configurar_sistema:15 | SISTEMA ESOCIAL ESTADO DA ARTE - INICIALIZANDO\n",
      "2025-06-17 09:21:36 | INFO     | __main__.SistemaPrincipal | 2466873862:configurar_sistema:16 | ================================================================================\n",
      "2025-06-17 09:21:36 | INFO     | __main__.SistemaPrincipal | 2466873862:_verificar_dependencias:47 | ‚úì pandas instalado\n",
      "2025-06-17 09:21:36 | INFO     | __main__.SistemaPrincipal | 2466873862:_verificar_dependencias:47 | ‚úì numpy instalado\n",
      "2025-06-17 09:21:36 | INFO     | __main__.SistemaPrincipal | 2466873862:_verificar_dependencias:47 | ‚úì sklearn instalado\n",
      "2025-06-17 09:21:36 | INFO     | __main__.SistemaPrincipal | 2466873862:_verificar_dependencias:47 | ‚úì tensorflow instalado\n",
      "2025-06-17 09:21:36 | INFO     | __main__.SistemaPrincipal | 2466873862:_verificar_dependencias:47 | ‚úì openpyxl instalado\n",
      "2025-06-17 09:21:36 | INFO     | __main__.SistemaPrincipal | 2466873862:_verificar_dependencias:47 | ‚úì reportlab instalado\n",
      "2025-06-17 09:21:36 | INFO     | __main__.SistemaPrincipal | 2466873862:_verificar_dependencias:47 | ‚úì fastapi instalado\n",
      "2025-06-17 09:21:36 | INFO     | __main__.SistemaPrincipal | 2466873862:_verificar_dependencias:47 | ‚úì plotly instalado\n",
      "2025-06-17 09:21:36 | INFO     | __main__.SistemaPrincipal | 2466873862:configurar_sistema:35 | Sistema configurado com sucesso!\n",
      "2025-06-17 09:21:36 | INFO     | __main__ | 2466873862:<module>:459 | Sistema Principal configurado e pronto para uso!\n",
      "2025-06-17 09:21:36 | INFO     | __main__ | 2466873862:<module>:460 | Para processar arquivos de produ√ß√£o: sistema_principal.processar_arquivos_producao()\n",
      "2025-06-17 09:21:36 | INFO     | __main__ | 2466873862:<module>:461 | Para executar testes: sistema_principal.executar_testes_sistema()\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 8: Sistema Principal de Processamento Integrado\n",
    "\n",
    "class SistemaESocialEstadoArte:\n",
    "    \"\"\"Sistema principal que integra todos os componentes\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(f\"{__name__}.SistemaPrincipal\")\n",
    "        self.configurar_sistema()\n",
    "        self.estatisticas = defaultdict(int)\n",
    "        self.tempo_inicio = None\n",
    "        \n",
    "    def configurar_sistema(self):\n",
    "        \"\"\"Configura todos os componentes do sistema\"\"\"\n",
    "        self.logger.info(\"=\"*80)\n",
    "        self.logger.info(\"SISTEMA ESOCIAL ESTADO DA ARTE - INICIALIZANDO\")\n",
    "        self.logger.info(\"=\"*80)\n",
    "        \n",
    "        # Verificar depend√™ncias\n",
    "        self._verificar_dependencias()\n",
    "        \n",
    "        # Componentes do sistema\n",
    "        self.layout = layout_esocial\n",
    "        self.parser = parser_esocial\n",
    "        self.detector_anomalias = detector_anomalias\n",
    "        self.sistema_ml = sistema_ml\n",
    "        self.gerador_excel = gerador_excel\n",
    "        self.gerador_pdf = gerador_pdf\n",
    "        self.sistema_backup = sistema_backup\n",
    "        \n",
    "        # Criar diret√≥rios necess√°rios\n",
    "        diretorios = ['logs', 'backups', 'relatorios', 'modelos_ml', 'temp']\n",
    "        for dir_name in diretorios:\n",
    "            Path(dir_name).mkdir(exist_ok=True)\n",
    "        \n",
    "        self.logger.info(\"Sistema configurado com sucesso!\")\n",
    "        \n",
    "    def _verificar_dependencias(self):\n",
    "        \"\"\"Verifica se todas as depend√™ncias est√£o instaladas\"\"\"\n",
    "        dependencias_criticas = [\n",
    "            'pandas', 'numpy', 'sklearn', 'tensorflow', \n",
    "            'openpyxl', 'reportlab', 'fastapi', 'plotly'\n",
    "        ]\n",
    "        \n",
    "        for dep in dependencias_criticas:\n",
    "            try:\n",
    "                __import__(dep)\n",
    "                self.logger.info(f\"‚úì {dep} instalado\")\n",
    "            except ImportError:\n",
    "                self.logger.error(f\"‚úó {dep} N√ÉO instalado - instale com pip install {dep}\")\n",
    "    \n",
    "    def processar_arquivo_completo(self, \n",
    "                                  arquivo_path: Union[str, Path],\n",
    "                                  usar_ml: bool = True,\n",
    "                                  gerar_relatorios: bool = True,\n",
    "                                  fazer_backup: bool = True,\n",
    "                                  validar_s5011: bool = False,\n",
    "                                  dados_s5011: Optional[pd.DataFrame] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Processa arquivo eSocial de forma completa\n",
    "        \n",
    "        Args:\n",
    "            arquivo_path: Caminho do arquivo eSocial\n",
    "            usar_ml: Se deve usar Machine Learning\n",
    "            gerar_relatorios: Se deve gerar relat√≥rios Excel/PDF\n",
    "            fazer_backup: Se deve fazer backup dos dados\n",
    "            validar_s5011: Se deve validar contra S-5011\n",
    "            dados_s5011: DataFrame com dados S-5011 para valida√ß√£o\n",
    "            \n",
    "        Returns:\n",
    "            Dicion√°rio com todos os resultados do processamento\n",
    "        \"\"\"\n",
    "        self.tempo_inicio = datetime.now()\n",
    "        self.logger.info(f\"Iniciando processamento completo: {arquivo_path}\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Parse do arquivo\n",
    "            self.logger.info(\"ETAPA 1/7: Parse do arquivo\")\n",
    "            df_dados = self.parser.parse_arquivo(arquivo_path)\n",
    "            # Filtro de conformidade para Regime de Previd√™ncia (RPPS)\n",
    "            # Se a coluna 'tpRegPrev' existir, filtra apenas os registros do RGPS (tipo 1)\n",
    "            if 'tpRegPrev' in df_dados.columns:\n",
    "                linhas_antes = len(df_dados)\n",
    "                df_dados = df_dados[df_dados['tpRegPrev'] == 1].copy()\n",
    "                linhas_depois = len(df_dados)\n",
    "                self.logger.info(f\"Filtro RPPS aplicado. Removidos {linhas_antes - linhas_depois} registros n√£o RGPS.\")\n",
    "            self.estatisticas['registros_processados'] = len(df_dados)\n",
    "            self.logger.info(f\"‚úì {len(df_dados):,} registros carregados\")\n",
    "            \n",
    "            # 2. Detec√ß√£o de anomalias\n",
    "            self.logger.info(\"ETAPA 2/7: Detec√ß√£o de anomalias\")\n",
    "            anomalias = self.detector_anomalias.detectar_todas_anomalias(\n",
    "                df_dados, \n",
    "                dados_s5011=dados_s5011 if validar_s5011 else None\n",
    "            )\n",
    "            total_anomalias = sum(len(v) for v in anomalias.values())\n",
    "            self.estatisticas['anomalias_detectadas'] = total_anomalias\n",
    "            self.logger.info(f\"‚úì {total_anomalias:,} anomalias detectadas\")\n",
    "            \n",
    "            # 3. Machine Learning\n",
    "            resultado_ml = df_dados.copy()\n",
    "            if usar_ml:\n",
    "                self.logger.info(\"ETAPA 3/7: An√°lise com Machine Learning\")\n",
    "                X = self.sistema_ml.preparar_features(df_dados)\n",
    "                \n",
    "                # Treinar se necess√°rio\n",
    "                if not self.sistema_ml.modelos:\n",
    "                    self.logger.info(\"Treinando modelos ML...\")\n",
    "                    self.sistema_ml.treinar_modelos(X)\n",
    "                    self.sistema_ml.salvar_modelos()\n",
    "                \n",
    "                resultado_ml = self.sistema_ml.detectar_anomalias_ml(X, df_dados)\n",
    "                ml_anomalias = resultado_ml['anomalia_ml'].sum()\n",
    "                self.estatisticas['anomalias_ml'] = ml_anomalias\n",
    "                self.logger.info(f\"‚úì {ml_anomalias:,} anomalias detectadas por ML\")\n",
    "            else:\n",
    "                self.logger.info(\"ETAPA 3/7: ML desativado\")\n",
    "                resultado_ml['anomalia_ml'] = False\n",
    "            \n",
    "            # 4. Backup\n",
    "            if fazer_backup:\n",
    "                self.logger.info(\"ETAPA 4/7: Criando backup\")\n",
    "                backup_path = self.sistema_backup.fazer_backup(df_dados)\n",
    "                self.estatisticas['backup_criado'] = True\n",
    "                self.logger.info(f\"‚úì Backup salvo: {backup_path}\")\n",
    "            else:\n",
    "                self.logger.info(\"ETAPA 4/7: Backup desativado\")\n",
    "            \n",
    "            # 5. Relat√≥rios\n",
    "            relatorios_gerados = {}\n",
    "            if gerar_relatorios:\n",
    "                self.logger.info(\"ETAPA 5/7: Gerando relat√≥rios\")\n",
    "                \n",
    "                # Excel\n",
    "                excel_path = self.gerador_excel.gerar_relatorio_completo(\n",
    "                    df_dados, anomalias, resultado_ml,\n",
    "                    arquivo_saida=f\"relatorios/esocial_analise_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "                )\n",
    "                relatorios_gerados['excel'] = excel_path\n",
    "                self.logger.info(f\"‚úì Relat√≥rio Excel: {excel_path}\")\n",
    "                \n",
    "                # PDF\n",
    "                pdf_path = self.gerador_pdf.gerar_relatorio_dataprev(\n",
    "                    df_dados, anomalias, resultado_ml,\n",
    "                    arquivo_saida=f\"relatorios/dataprev_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf\"\n",
    "                )\n",
    "                relatorios_gerados['pdf'] = pdf_path\n",
    "                self.logger.info(f\"‚úì Relat√≥rio PDF: {pdf_path}\")\n",
    "            else:\n",
    "                self.logger.info(\"ETAPA 5/7: Relat√≥rios desativados\")\n",
    "            \n",
    "            # 6. An√°lise de criticidade\n",
    "            self.logger.info(\"ETAPA 6/7: An√°lise de criticidade\")\n",
    "            analise_criticidade = self._analisar_criticidade(anomalias)\n",
    "            \n",
    "            # 7. Finaliza√ß√£o\n",
    "            tempo_total = (datetime.now() - self.tempo_inicio).total_seconds()\n",
    "            self.estatisticas['tempo_processamento'] = tempo_total\n",
    "            \n",
    "            self.logger.info(\"ETAPA 7/7: Processamento conclu√≠do\")\n",
    "            self.logger.info(\"=\"*80)\n",
    "            self.logger.info(\"RESUMO DO PROCESSAMENTO\")\n",
    "            self.logger.info(\"=\"*80)\n",
    "            self.logger.info(f\"Tempo total: {tempo_total:.2f} segundos\")\n",
    "            self.logger.info(f\"Registros: {self.estatisticas['registros_processados']:,}\")\n",
    "            self.logger.info(f\"Anomalias: {self.estatisticas['anomalias_detectadas']:,}\")\n",
    "            self.logger.info(f\"Anomalias ML: {self.estatisticas.get('anomalias_ml', 0):,}\")\n",
    "            self.logger.info(\"=\"*80)\n",
    "            \n",
    "            # Retornar resultados\n",
    "            return {\n",
    "                'sucesso': True,\n",
    "                'dados': df_dados,\n",
    "                'anomalias': anomalias,\n",
    "                'resultado_ml': resultado_ml,\n",
    "                'estatisticas': dict(self.estatisticas),\n",
    "                'tempo_processamento': tempo_total,\n",
    "                'relatorios': relatorios_gerados,\n",
    "                'analise_criticidade': analise_criticidade,\n",
    "                'recomendacao_principal': self._gerar_recomendacao_principal(anomalias)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"ERRO FATAL: {e}\")\n",
    "            import traceback\n",
    "            self.logger.error(traceback.format_exc())\n",
    "            \n",
    "            return {\n",
    "                'sucesso': False,\n",
    "                'erro': str(e),\n",
    "                'tempo_processamento': (datetime.now() - self.tempo_inicio).total_seconds() if self.tempo_inicio else 0,\n",
    "                'estatisticas': dict(self.estatisticas)\n",
    "            }\n",
    "    \n",
    "    def processar_arquivos_producao(self, ano_inicio: int = 2023, ano_fim: int = 2024) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Processa os arquivos de produ√ß√£o do FAP\n",
    "        \n",
    "        Args:\n",
    "            ano_inicio: Ano inicial dos dados\n",
    "            ano_fim: Ano final dos dados\n",
    "            \n",
    "        Returns:\n",
    "            Resultados consolidados do processamento\n",
    "        \"\"\"\n",
    "        self.logger.info(\"=\"*80)\n",
    "        self.logger.info(\"PROCESSAMENTO DE ARQUIVOS DE PRODU√á√ÉO FAP\")\n",
    "        self.logger.info(\"=\"*80)\n",
    "        \n",
    "        # Caminhos dos arquivos de produ√ß√£o\n",
    "        arquivos_producao = [\n",
    "            rf\"D:\\FAP_INTELLIGENCE_V4\\Dados\\Esocial\\D.FAP.CAL.000.ESOCIAL.{ano_inicio}.2026.TXT\",\n",
    "            rf\"D:\\FAP_INTELLIGENCE_V4\\Dados\\Esocial\\D.FAP.CAL.000.ESOCIAL.{ano_fim}.2026.TXT\"\n",
    "        ]\n",
    "        \n",
    "        resultados_consolidados = {\n",
    "            'arquivos_processados': [],\n",
    "            'total_registros': 0,\n",
    "            'total_anomalias': 0,\n",
    "            'empresas_unicas': set(),\n",
    "            'periodos_processados': set()\n",
    "        }\n",
    "        \n",
    "        for arquivo_path in arquivos_producao:\n",
    "            arquivo_path = Path(arquivo_path)\n",
    "            \n",
    "            if not arquivo_path.exists():\n",
    "                self.logger.warning(f\"Arquivo n√£o encontrado: {arquivo_path}\")\n",
    "                self.logger.info(\"Usando arquivo de exemplo para demonstra√ß√£o\")\n",
    "                # Criar arquivo exemplo se n√£o existir\n",
    "                self._criar_arquivo_exemplo(arquivo_path)\n",
    "            \n",
    "            self.logger.info(f\"Processando: {arquivo_path}\")\n",
    "            \n",
    "            # Processar arquivo\n",
    "            resultado = self.processar_arquivo_completo(\n",
    "                arquivo_path,\n",
    "                usar_ml=True,\n",
    "                gerar_relatorios=True,\n",
    "                fazer_backup=True\n",
    "            )\n",
    "            \n",
    "            if resultado['sucesso']:\n",
    "                resultados_consolidados['arquivos_processados'].append(str(arquivo_path))\n",
    "                resultados_consolidados['total_registros'] += resultado['estatisticas']['registros_processados']\n",
    "                resultados_consolidados['total_anomalias'] += resultado['estatisticas']['anomalias_detectadas']\n",
    "                \n",
    "                # Coletar empresas e per√≠odos √∫nicos\n",
    "                df = resultado['dados']\n",
    "                resultados_consolidados['empresas_unicas'].update(\n",
    "                    df['NU_INSCRICAO_ESTABELECIM'].unique()\n",
    "                )\n",
    "                resultados_consolidados['periodos_processados'].update(\n",
    "                    df['NU_PERIODO_REFERENCIA'].unique()\n",
    "                )\n",
    "        \n",
    "        # Relat√≥rio consolidado\n",
    "        self.logger.info(\"=\"*80)\n",
    "        self.logger.info(\"RELAT√ìRIO CONSOLIDADO\")\n",
    "        self.logger.info(\"=\"*80)\n",
    "        self.logger.info(f\"Arquivos processados: {len(resultados_consolidados['arquivos_processados'])}\")\n",
    "        self.logger.info(f\"Total de registros: {resultados_consolidados['total_registros']:,}\")\n",
    "        self.logger.info(f\"Total de anomalias: {resultados_consolidados['total_anomalias']:,}\")\n",
    "        self.logger.info(f\"Empresas √∫nicas: {len(resultados_consolidados['empresas_unicas']):,}\")\n",
    "        self.logger.info(f\"Per√≠odos processados: {len(resultados_consolidados['periodos_processados'])}\")\n",
    "        self.logger.info(\"=\"*80)\n",
    "        \n",
    "        return resultados_consolidados\n",
    "    \n",
    "    def _analisar_criticidade(self, anomalias: Dict[str, List[Anomalia]]) -> Dict[str, Any]:\n",
    "        \"\"\"Analisa a criticidade geral das anomalias\"\"\"\n",
    "        total_criticas = sum(1 for v in anomalias.values() for a in v if a.severidade == 'CRITICA')\n",
    "        total_altas = sum(1 for v in anomalias.values() for a in v if a.severidade == 'ALTA')\n",
    "        total_medias = sum(1 for v in anomalias.values() for a in v if a.severidade == 'MEDIA')\n",
    "        total_baixas = sum(1 for v in anomalias.values() for a in v if a.severidade == 'BAIXA')\n",
    "        \n",
    "        # Calcular impacto financeiro total\n",
    "        impacto_total = sum(a.impacto_financeiro for v in anomalias.values() for a in v)\n",
    "        \n",
    "        # Determinar n√≠vel de criticidade geral\n",
    "        if total_criticas > 100:\n",
    "            nivel_geral = \"CR√çTICO - A√á√ÉO URGENTE\"\n",
    "            cor = \"vermelho\"\n",
    "        elif total_criticas > 50 or total_altas > 200:\n",
    "            nivel_geral = \"ALTO - A√á√ÉO NECESS√ÅRIA\"\n",
    "            cor = \"laranja\"\n",
    "        elif total_criticas > 10 or total_altas > 50:\n",
    "            nivel_geral = \"M√âDIO - REVISAR\"\n",
    "            cor = \"amarelo\"\n",
    "        else:\n",
    "            nivel_geral = \"BAIXO - MONITORAR\"\n",
    "            cor = \"verde\"\n",
    "        \n",
    "        return {\n",
    "            'nivel_geral': nivel_geral,\n",
    "            'cor_indicador': cor,\n",
    "            'total_criticas': total_criticas,\n",
    "            'total_altas': total_altas,\n",
    "            'total_medias': total_medias,\n",
    "            'total_baixas': total_baixas,\n",
    "            'impacto_financeiro_total': impacto_total,\n",
    "            'precisa_reextracao': total_criticas > 100\n",
    "        }\n",
    "    \n",
    "    def _gerar_recomendacao_principal(self, anomalias: Dict[str, List[Anomalia]]) -> str:\n",
    "        \"\"\"Gera recomenda√ß√£o principal baseada nas anomalias\"\"\"\n",
    "        total_criticas = sum(1 for v in anomalias.values() for a in v if a.severidade == 'CRITICA')\n",
    "        \n",
    "        if total_criticas > 100:\n",
    "            return \"REFAZER EXTRA√á√ÉO DOS DADOS - Quantidade cr√≠tica de anomalias detectadas\"\n",
    "        elif total_criticas > 50:\n",
    "            return \"REVISAR URGENTEMENTE os registros com anomalias cr√≠ticas\"\n",
    "        elif total_criticas > 10:\n",
    "            return \"APLICAR CORRE√á√ïES nos registros identificados\"\n",
    "        else:\n",
    "            return \"MONITORAR e aplicar corre√ß√µes pontuais conforme necess√°rio\"\n",
    "    \n",
    "    def _criar_arquivo_exemplo(self, arquivo_path: Path):\n",
    "        \"\"\"Cria arquivo de exemplo para testes\"\"\"\n",
    "        self.logger.info(\"Criando arquivo de exemplo para demonstra√ß√£o\")\n",
    "    \n",
    "        # Criar diret√≥rio se n√£o existir\n",
    "        arquivo_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "        # Gerar dados de exemplo\n",
    "        exemplos = []\n",
    "    \n",
    "        for mes in range(1, 13):\n",
    "            linha = (\n",
    "                f\"2023{mes:02d}\" +  # Per√≠odo  ‚Üê ADICIONAR + AQUI\n",
    "                \"1\" +  # Tipo inscri√ß√£o estabelecimento  ‚Üê ADICIONAR + AQUI\n",
    "                \"12345678000195 \" +  # CNPJ  ‚Üê ADICIONAR + AQUI\n",
    "                \"1\" +  # Tipo inscri√ß√£o empregador  ‚Üê ADICIONAR + AQUI\n",
    "                \"12345678000195 \" +  # CNPJ empregador  ‚Üê ADICIONAR + AQUI\n",
    "                \"000100\" +  # Qt v√≠nculos  ‚Üê ADICIONAR + AQUI\n",
    "                \"000010\" +  # Qt admiss√µes  ‚Üê ADICIONAR + AQUI\n",
    "                \"000005\" +  # Qt rescis√µes  ‚Üê ADICIONAR + AQUI\n",
    "                \"000001\" * 18 +  # Rescis√µes por motivo  ‚Üê ADICIONAR + AQUI\n",
    "                \"000050\" * 18 +  # V√≠nculos por categoria  ‚Üê ADICIONAR + AQUI\n",
    "                f\"2023{mes:02d}01120000\" +  # Data evento  ‚Üê ADICIONAR + AQUI\n",
    "                \"99\" +  # Classifica√ß√£o tribut√°ria  ‚Üê ADICIONAR + AQUI\n",
    "                \"1234567\" +  # CNAE  ‚Üê ADICIONAR + AQUI\n",
    "                \"2\" +  # Al√≠quota GILRAT  ‚Üê ADICIONAR + AQUI\n",
    "                \"0001000000\" +  # FAP  ‚Üê ADICIONAR + AQUI\n",
    "                \"0002000000\" +  # GILRAT ajustado  ‚Üê ADICIONAR + AQUI\n",
    "                \"00000010000000000\" +  # Base c√°lculo total  ‚Üê ADICIONAR + AQUI\n",
    "                \"00000000500000000\" * 18 +  # Base por categoria  ‚Üê ADICIONAR + AQUI\n",
    "                \"1234567890123456789012345678901234567890\"  # Recibo S-1299  ‚Üê SEM + NO FINAL\n",
    "            )\n",
    "        \n",
    "            # Garantir 679 caracteres\n",
    "            linha = linha[:679].ljust(679)\n",
    "            exemplos.append(linha)\n",
    "        \n",
    "        # Escrever arquivo\n",
    "        with open(arquivo_path, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(exemplos))\n",
    "        \n",
    "        self.logger.info(f\"Arquivo exemplo criado: {arquivo_path}\")\n",
    "    \n",
    "    def executar_testes_sistema(self) -> Dict[str, bool]:\n",
    "        \"\"\"Executa testes autom√°ticos do sistema\"\"\"\n",
    "        self.logger.info(\"Executando testes do sistema...\")\n",
    "        \n",
    "        testes = {\n",
    "            'parser': self._testar_parser(),\n",
    "            'detector_anomalias': self._testar_detector(),\n",
    "            'machine_learning': self._testar_ml(),\n",
    "            'relatorios': self._testar_relatorios(),\n",
    "            'backup': self._testar_backup()\n",
    "        }\n",
    "        \n",
    "        # Resumo\n",
    "        total_testes = len(testes)\n",
    "        testes_ok = sum(1 for v in testes.values() if v)\n",
    "        \n",
    "        self.logger.info(f\"Testes conclu√≠dos: {testes_ok}/{total_testes} passaram\")\n",
    "        \n",
    "        return testes\n",
    "    \n",
    "    def _testar_parser(self) -> bool:\n",
    "        \"\"\"Testa o parser\"\"\"\n",
    "        try:\n",
    "            # Criar linha de teste\n",
    "            linha_teste = \"202301\" + \"1\" + \"12345678000195 \" + \"1\" + \"12345678000195 \" + \"0\" * 653\n",
    "            linha_teste = linha_teste[:679]\n",
    "            \n",
    "            # Testar parse\n",
    "            registro = self.parser._processar_linha(linha_teste, 1)\n",
    "            \n",
    "            return registro['NU_PERIODO_REFERENCIA'] == 202301\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def _testar_detector(self) -> bool:\n",
    "        \"\"\"Testa o detector de anomalias\"\"\"\n",
    "        try:\n",
    "            # DataFrame de teste\n",
    "            df_teste = pd.DataFrame({\n",
    "                'NU_PERIODO_REFERENCIA': [202301],\n",
    "                'NU_INSCRICAO_ESTABELECIM': ['12345678000195'],\n",
    "                'QT_VINCULOS': [100],\n",
    "                'QT_RESCISOES': [200],  # Mais rescis√µes que v√≠nculos\n",
    "                'VL_BASE_CALCULO_CONTRIB_PREV': [0]\n",
    "            })\n",
    "            \n",
    "            # Detectar anomalias\n",
    "            anomalias = self.detector_anomalias.detectar_todas_anomalias(df_teste)\n",
    "            \n",
    "            return len(anomalias) > 0\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def _testar_ml(self) -> bool:\n",
    "        \"\"\"Testa o sistema ML\"\"\"\n",
    "        try:\n",
    "            # Dados de teste\n",
    "            X_teste = np.random.randn(100, 10)\n",
    "            \n",
    "            # Treinar modelo simples\n",
    "            modelo = IsolationForest(n_estimators=10)\n",
    "            modelo.fit(X_teste)\n",
    "            \n",
    "            # Predi√ß√£o\n",
    "            pred = modelo.predict(X_teste[:10])\n",
    "            \n",
    "            return len(pred) == 10\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def _testar_relatorios(self) -> bool:\n",
    "        \"\"\"Testa gera√ß√£o de relat√≥rios\"\"\"\n",
    "        try:\n",
    "            # Verificar se geradores est√£o configurados\n",
    "            return (\n",
    "                hasattr(self.gerador_excel, 'gerar_relatorio_completo') and\n",
    "                hasattr(self.gerador_pdf, 'gerar_relatorio_dataprev')\n",
    "            )\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def _testar_backup(self) -> bool:\n",
    "        \"\"\"Testa sistema de backup\"\"\"\n",
    "        try:\n",
    "            # DataFrame de teste\n",
    "            df_teste = pd.DataFrame({'teste': [1, 2, 3]})\n",
    "            \n",
    "            # Fazer backup\n",
    "            backup_path = self.sistema_backup.fazer_backup(df_teste, 'teste_backup')\n",
    "            \n",
    "            # Verificar se foi criado\n",
    "            return Path(backup_path).exists()\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "\n",
    "# Criar inst√¢ncia do sistema principal\n",
    "sistema_principal = SistemaESocialEstadoArte()\n",
    "\n",
    "logger.info(\"Sistema Principal configurado e pronto para uso!\")\n",
    "logger.info(\"Para processar arquivos de produ√ß√£o: sistema_principal.processar_arquivos_producao()\")\n",
    "logger.info(\"Para executar testes: sistema_principal.executar_testes_sistema()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-17 09:21:36 | INFO     | __main__.SistemaPrincipal | 2466873862:configurar_sistema:14 | ================================================================================\n",
      "2025-06-17 09:21:36 | INFO     | __main__.SistemaPrincipal | 2466873862:configurar_sistema:15 | SISTEMA ESOCIAL ESTADO DA ARTE - INICIALIZANDO\n",
      "2025-06-17 09:21:37 | INFO     | __main__.SistemaPrincipal | 2466873862:configurar_sistema:16 | ================================================================================\n",
      "2025-06-17 09:21:37 | INFO     | __main__.SistemaPrincipal | 2466873862:_verificar_dependencias:47 | ‚úì pandas instalado\n",
      "2025-06-17 09:21:37 | INFO     | __main__.SistemaPrincipal | 2466873862:_verificar_dependencias:47 | ‚úì numpy instalado\n",
      "2025-06-17 09:21:37 | INFO     | __main__.SistemaPrincipal | 2466873862:_verificar_dependencias:47 | ‚úì sklearn instalado\n",
      "2025-06-17 09:21:37 | INFO     | __main__.SistemaPrincipal | 2466873862:_verificar_dependencias:47 | ‚úì tensorflow instalado\n",
      "2025-06-17 09:21:37 | INFO     | __main__.SistemaPrincipal | 2466873862:_verificar_dependencias:47 | ‚úì openpyxl instalado\n",
      "2025-06-17 09:21:37 | INFO     | __main__.SistemaPrincipal | 2466873862:_verificar_dependencias:47 | ‚úì reportlab instalado\n",
      "2025-06-17 09:21:37 | INFO     | __main__.SistemaPrincipal | 2466873862:_verificar_dependencias:47 | ‚úì fastapi instalado\n",
      "2025-06-17 09:21:37 | INFO     | __main__.SistemaPrincipal | 2466873862:_verificar_dependencias:47 | ‚úì plotly instalado\n",
      "2025-06-17 09:21:37 | INFO     | __main__.SistemaPrincipal | 2466873862:configurar_sistema:35 | Sistema configurado com sucesso!\n",
      "2025-06-17 09:21:37 | INFO     | __main__ | 1338406383:<module>:548 | Sistema Principal OTIMIZADO configurado!\n",
      "2025-06-17 09:21:37 | INFO     | __main__ | 1338406383:<module>:549 | Pronto para processar arquivos de 25GB em menos de 2 horas\n",
      "2025-06-17 09:21:37 | INFO     | __main__ | 1338406383:<module>:550 | Para processar: sistema_principal_otimizado.processar_arquivo_completo_otimizado('arquivo.txt')\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 8.1: Sistema Principal Otimizado com Processamento de Arquivos Grandes\n",
    "\n",
    "class SistemaESocialEstadoArteAbsoluto(SistemaESocialEstadoArte):\n",
    "    \"\"\"Sistema principal OTIMIZADO para processar arquivos de 25GB+ com todas as valida√ß√µes cr√≠ticas\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.processador_otimizado = ProcessadorESocialOtimizado(self.layout)\n",
    "        self.validacoes_criticas_ativas = True\n",
    "\n",
    "    def processar_arquivo_completo_otimizado(self,\n",
    "                                            arquivo_path: Union[str, Path],\n",
    "                                            usar_ml: bool = True,\n",
    "                                            gerar_relatorios: bool = True,\n",
    "                                            fazer_backup: bool = True,\n",
    "                                            validar_s5011: bool = False,\n",
    "                                            dados_s5011: Optional[pd.DataFrame] = None,\n",
    "                                            memoria_maxima_gb: float = None,\n",
    "                                            chunk_rows: int = 2_000_000) -> Dict[str, Any]:\n",
    "        \"\"\"Processa arquivo eSocial inteiro utilizando Parquet e Dask em chunks.\"\"\"\n",
    "        self.tempo_inicio = datetime.now()\n",
    "        arquivo_path = Path(arquivo_path)\n",
    "        if memoria_maxima_gb is None:\n",
    "            import psutil\n",
    "            memoria_total_gb = psutil.virtual_memory().total / (1024**3)\n",
    "            memoria_maxima_gb = min(memoria_total_gb * 0.6, 32.0)\n",
    "        self.logger.info(f\"Iniciando processamento OTIMIZADO: {arquivo_path}\")\n",
    "        self.logger.info(f\"Mem√≥ria m√°xima configurada: {memoria_maxima_gb:.1f} GB\")\n",
    "        try:\n",
    "            self.logger.info(\"ETAPA 1/7: Convers√£o TXT ‚Üí Parquet (streaming)\")\n",
    "            arquivo_parquet = arquivo_path.with_suffix('.parquet')\n",
    "            if not arquivo_parquet.exists() or arquivo_parquet.stat().st_mtime < arquivo_path.stat().st_mtime:\n",
    "                arquivo_parquet = self.processador_otimizado.converter_txt_para_parquet_streaming(\n",
    "                    arquivo_txt=arquivo_path,\n",
    "                    arquivo_parquet=arquivo_parquet,\n",
    "                    chunk_size=50000,\n",
    "                    memoria_maxima_gb=memoria_maxima_gb)\n",
    "            self.logger.info(\"ETAPA 2/7: Carregando dados com Dask\")\n",
    "            df_dask = dd.read_parquet(arquivo_parquet, engine='pyarrow', split_row_groups=True)\n",
    "            n_registros = len(df_dask)\n",
    "            self.estatisticas['registros_processados'] = n_registros\n",
    "            self.logger.info(f\"Total de registros: {n_registros:,}\")\n",
    "            if usar_ml:\n",
    "                self.logger.info(\"Obtendo amostra para treino ML (500k registros)\")\n",
    "                df_sample = df_dask.head(500000)\n",
    "                X_sample = self.sistema_ml.preparar_features(df_sample)\n",
    "                self.sistema_ml.treinar_modelos(X_sample)\n",
    "                self.sistema_ml.salvar_modelos()\n",
    "            anom_alias = defaultdict(list)\n",
    "            resultado_ml_parts = []\n",
    "            num_parts = df_dask.npartitions\n",
    "            parts_per_chunk = max(1, chunk_rows // (n_registros // num_parts))\n",
    "            for i in range(0, num_parts, parts_per_chunk):\n",
    "                self.logger.info(f\"Processando parti√ß√µes {i} - {min(i+parts_per_chunk-1, num_parts-1)}\")\n",
    "                ddf_chunk = df_dask.partitions[i:min(i+parts_per_chunk, num_parts)]\n",
    "                df_chunk = ddf_chunk.compute()\n",
    "                chunk_anom = self._aplicar_validacoes_criticas(df_chunk, dados_s5011)\n",
    "                for k,v in chunk_anom.items():\n",
    "                    anom_alias[k].extend(v)\n",
    "                if usar_ml:\n",
    "                    X_chunk = self.sistema_ml.preparar_features(df_chunk)\n",
    "                    res_chunk = self.sistema_ml.detectar_anomalias_ml(X_chunk, df_chunk)\n",
    "                    resultado_ml_parts.append(res_chunk)\n",
    "            df_dados = df_dask.compute()\n",
    "            resultado_ml = pd.concat(resultado_ml_parts) if resultado_ml_parts else df_dados.copy()\n",
    "            total_anomalias = sum(len(v) for v in anom_alias.values())\n",
    "            self.estatisticas['anomalias_detectadas'] = total_anomalias\n",
    "            self.estatisticas['anomalias_ml'] = resultado_ml['anomalia_ml'].sum() if usar_ml else 0\n",
    "            if fazer_backup:\n",
    "                self.logger.info(\"ETAPA 5/7: Criando backup\")\n",
    "                backup_path = self.sistema_backup.diretorio_backup / f\"{arquivo_parquet.stem}_backup.parquet\"\n",
    "                shutil.copy2(arquivo_parquet, backup_path)\n",
    "                self.estatisticas['backup_criado'] = True\n",
    "            relatorios_gerados = {}\n",
    "            if gerar_relatorios:\n",
    "                self.logger.info(\"ETAPA 6/7: Gerando relat√≥rios\")\n",
    "                excel_path = self.gerador_excel.gerar_relatorio_completo(df_dados, anom_alias, resultado_ml,\n",
    "                    arquivo_saida=f\"relatorios/esocial_analise_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\")\n",
    "                pdf_path = self.gerador_pdf.gerar_relatorio_dataprev(df_dados, anom_alias, resultado_ml,\n",
    "                    arquivo_saida=f\"relatorios/dataprev_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf\")\n",
    "                relatorios_gerados.update({'excel':excel_path,'pdf':pdf_path})\n",
    "            tempo_total = (datetime.now() - self.tempo_inicio).total_seconds()\n",
    "            self.estatisticas['tempo_processamento'] = tempo_total\n",
    "            self.logger.info(f\"Processamento conclu√≠do em {tempo_total/60:.1f} minutos\")\n",
    "            return {\n",
    "                'sucesso': True,\n",
    "                'dados': df_dados,\n",
    "                'anomalias': anom_alias,\n",
    "                'resultado_ml': resultado_ml,\n",
    "                'estatisticas': dict(self.estatisticas),\n",
    "                'tempo_processamento': tempo_total,\n",
    "                'relatorios': relatorios_gerados,\n",
    "                'arquivo_parquet': str(arquivo_parquet)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"ERRO FATAL: {e}\")\n",
    "            return {'sucesso': False, 'erro': str(e)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîç DIAGN√ìSTICO DOS ARQUIVOS DE PRODU√á√ÉO ESOCIAL\n",
      "================================================================================\n",
      "\n",
      "üìÑ Analisando arquivo 2023: D:\\FAP_INTELLIGENCE_V4\\Dados\\Esocial\\D.FAP.CAL.000.ESOCIAL.2023.2026.TXT\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Primeiras 10 linhas do arquivo:\n",
      "   Linha 1 (679 chars): 2023111          166080              0    51     0     1     1     0     0     0     0     0     0  ...\n",
      "\n",
      "   üìä An√°lise da primeira linha:\n",
      "      - Tamanho: 679 caracteres (esperado: 679)\n",
      "      - Primeiros 50 chars: '2023111          166080              0    51     0'\n",
      "      - Caracteres 1-6 (per√≠odo): '202311'\n",
      "      - Caracter 7 (tipo inscr): '1'\n",
      "      - Caracteres 8-22 (CNPJ): '          16608'\n",
      "   Linha 2 (679 chars): 2023111         2312400              0     5     0     0     0     0     0     0     0     0     0  ...\n",
      "   Linha 3 (679 chars): 2023111         2471620              0     4     0     0     0     0     0     0     0     0     0  ...\n",
      "   Linha 4 (679 chars): 2023111         2523280              0    14     0     0     0     0     0     0     0     0     0  ...\n",
      "   Linha 5 (679 chars): 2023111         3592030              0     4     0     0     0     0     0     0     0     0     0  ...\n",
      "   Linha 6 (679 chars): 2023111         5424070              0     8     0     0     0     0     0     0     0     0     0  ...\n",
      "   Linha 7 (679 chars): 2023111         5943930              0    17     0     0     0     0     0     0     0     0     0  ...\n",
      "   Linha 8 (679 chars): 2023111         6025900              0     5     0     0     0     0     0     0     0     0     0  ...\n",
      "   Linha 9 (679 chars): 2023111         7295820              0     5     0     0     0     0     0     0     0     0     0  ...\n",
      "   Linha 10 (679 chars): 2023111         7317220              0    24     0     0     0     0     0     0     0     0     0  ...\n",
      "\n",
      "üìÑ Analisando arquivo 2024: D:\\FAP_INTELLIGENCE_V4\\Dados\\Esocial\\D.FAP.CAL.000.ESOCIAL.2024.2026.TXT\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Primeiras 10 linhas do arquivo:\n",
      "   Linha 1 (679 chars): 2024081           43400              0    14     0     0     0     0     0     0     0     0     0  ...\n",
      "\n",
      "   üìä An√°lise da primeira linha:\n",
      "      - Tamanho: 679 caracteres (esperado: 679)\n",
      "      - Primeiros 50 chars: '2024081           43400              0    14     0'\n",
      "      - Caracteres 1-6 (per√≠odo): '202408'\n",
      "      - Caracter 7 (tipo inscr): '1'\n",
      "      - Caracteres 8-22 (CNPJ): '           4340'\n",
      "   Linha 2 (679 chars): 2024081          240230              0    11     0     0     0     0     0     0     0     0     0  ...\n",
      "   Linha 3 (679 chars): 2024081          609250              0     9     0     0     0     0     0     0     0     0     0  ...\n",
      "   Linha 4 (679 chars): 2024081         1223860              0     8     0     0     0     0     0     0     0     0     0  ...\n",
      "   Linha 5 (679 chars): 2024081         1687000              0    19     0     0     0     0     0     0     0     0     0  ...\n",
      "   Linha 6 (679 chars): 2024081         1850350              0    11     0     0     0     0     0     0     0     0     0  ...\n",
      "   Linha 7 (679 chars): 2024081         2040050              0     6     0     0     0     0     0     0     0     0     0  ...\n",
      "   Linha 8 (679 chars): 2024081         2548860              0     6     0     0     0     0     0     0     0     0     0  ...\n",
      "   Linha 9 (679 chars): 2024081         2893290              0    16     0     0     0     0     0     0     0     0     0  ...\n",
      "   Linha 10 (679 chars): 2024081         3124010              0     7     0     0     0     0     0     0     0     0     0  ...\n",
      "\n",
      "================================================================================\n",
      "üí° CONCLUS√ÉO DO DIAGN√ìSTICO\n",
      "================================================================================\n",
      "Se os arquivos n√£o t√™m 679 caracteres por linha, podem estar em outro formato:\n",
      "- CSV com delimitadores\n",
      "- Formato JSON\n",
      "- Outro layout posicional\n",
      "- Arquivo compactado\n",
      "\n",
      "Verifique com a fonte dos dados qual √© o formato correto.\n"
     ]
    }
   ],
   "source": [
    "# C√©lula de Diagn√≥stico: Investigar formato real dos arquivos de produ√ß√£o\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üîç DIAGN√ìSTICO DOS ARQUIVOS DE PRODU√á√ÉO ESOCIAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "arquivo_2023 = r\"D:\\FAP_INTELLIGENCE_V4\\Dados\\Esocial\\D.FAP.CAL.000.ESOCIAL.2023.2026.TXT\"\n",
    "arquivo_2024 = r\"D:\\FAP_INTELLIGENCE_V4\\Dados\\Esocial\\D.FAP.CAL.000.ESOCIAL.2024.2026.TXT\"\n",
    "\n",
    "for ano, arquivo in [(\"2023\", arquivo_2023), (\"2024\", arquivo_2024)]:\n",
    "    print(f\"\\nüìÑ Analisando arquivo {ano}: {arquivo}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    try:\n",
    "        with open(arquivo, 'r', encoding='utf-8') as f:\n",
    "            # Ler primeiras 10 linhas\n",
    "            print(f\"\\nPrimeiras 10 linhas do arquivo:\")\n",
    "            for i in range(10):\n",
    "                linha = f.readline()\n",
    "                if not linha:\n",
    "                    print(f\"   Linha {i+1}: [FIM DO ARQUIVO]\")\n",
    "                    break\n",
    "                    \n",
    "                linha_limpa = linha.rstrip('\\n\\r')\n",
    "                tamanho = len(linha_limpa)\n",
    "                \n",
    "                # Mostrar linha truncada se muito grande\n",
    "                if tamanho > 100:\n",
    "                    print(f\"   Linha {i+1} ({tamanho} chars): {linha_limpa[:100]}...\")\n",
    "                else:\n",
    "                    print(f\"   Linha {i+1} ({tamanho} chars): {linha_limpa}\")\n",
    "                \n",
    "                # Analisar primeira linha em detalhes\n",
    "                if i == 0 and tamanho > 0:\n",
    "                    print(f\"\\n   üìä An√°lise da primeira linha:\")\n",
    "                    print(f\"      - Tamanho: {tamanho} caracteres (esperado: 679)\")\n",
    "                    print(f\"      - Primeiros 50 chars: '{linha_limpa[:50]}'\")\n",
    "                    print(f\"      - Caracteres 1-6 (per√≠odo): '{linha_limpa[0:6] if tamanho >= 6 else 'N/A'}'\")\n",
    "                    print(f\"      - Caracter 7 (tipo inscr): '{linha_limpa[6] if tamanho >= 7 else 'N/A'}'\")\n",
    "                    print(f\"      - Caracteres 8-22 (CNPJ): '{linha_limpa[7:22] if tamanho >= 22 else 'N/A'}'\")\n",
    "        \n",
    "        # Verificar encoding alternativo se UTF-8 falhar\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"\\n‚ö†Ô∏è Erro de encoding UTF-8. Tentando Latin-1...\")\n",
    "        try:\n",
    "            with open(arquivo, 'r', encoding='latin-1') as f:\n",
    "                linha = f.readline().rstrip('\\n\\r')\n",
    "                print(f\"   Primeira linha com Latin-1 ({len(linha)} chars): {linha[:100]}...\")\n",
    "        except Exception as e2:\n",
    "            print(f\"   ‚ùå Erro tamb√©m com Latin-1: {e2}\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"   ‚ùå Arquivo n√£o encontrado!\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Erro ao ler arquivo: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üí° CONCLUS√ÉO DO DIAGN√ìSTICO\")\n",
    "print(\"=\"*80)\n",
    "print(\"Se os arquivos n√£o t√™m 679 caracteres por linha, podem estar em outro formato:\")\n",
    "print(\"- CSV com delimitadores\")\n",
    "print(\"- Formato JSON\")\n",
    "print(\"- Outro layout posicional\")\n",
    "print(\"- Arquivo compactado\")\n",
    "print(\"\\nVerifique com a fonte dos dados qual √© o formato correto.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üöÄ PROCESSAMENTO DOS ARQUIVOS ESOCIAL FAP - PRODU√á√ÉO\n",
      "================================================================================\n",
      "\n",
      "üìã VALIDA√á√ÉO DO FORMATO DOS ARQUIVOS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚úÖ Arquivo 2023 encontrado: 28.51 GB\n",
      "   ‚úÖ Formato v√°lido: Formato v√°lido (10/10 linhas OK)\n",
      "\n",
      "‚úÖ Arquivo 2024 encontrado: 29.08 GB\n",
      "   ‚úÖ Formato v√°lido: Formato v√°lido (10/10 linhas OK)\n",
      "\n",
      "================================================================================\n",
      "üìä INICIANDO PROCESSAMENTO DOS ARQUIVOS V√ÅLIDOS\n",
      "================================================================================\n",
      "\n",
      "[2023] Processando arquivo...\n",
      "2025-06-17 09:21:37 | INFO     | __main__.SistemaPrincipal | 1338406383:processar_arquivo_completo_otimizado:43 | Iniciando processamento OTIMIZADO: D:\\FAP_INTELLIGENCE_V4\\Dados\\Esocial\\D.FAP.CAL.000.ESOCIAL.2023.2026.TXT\n",
      "2025-06-17 09:21:37 | INFO     | __main__.SistemaPrincipal | 1338406383:processar_arquivo_completo_otimizado:44 | Mem√≥ria m√°xima configurada: 12.0 GB\n",
      "2025-06-17 09:21:37 | INFO     | __main__.SistemaPrincipal | 1338406383:processar_arquivo_completo_otimizado:48 | ETAPA 1/7: Convers√£o TXT ‚Üí Parquet (streaming)\n",
      "2025-06-17 09:21:37 | INFO     | __main__.SistemaPrincipal | 1338406383:processar_arquivo_completo_otimizado:61 | Usando Parquet existente: D:\\FAP_INTELLIGENCE_V4\\Dados\\Esocial\\D.FAP.CAL.000.ESOCIAL.2023.2026.parquet\n",
      "2025-06-17 09:21:37 | INFO     | __main__.SistemaPrincipal | 1338406383:processar_arquivo_completo_otimizado:64 | ETAPA 2/7: Carregando dados com Dask\n",
      "2025-06-17 09:21:37 | INFO     | __main__.SistemaPrincipal | 1338406383:processar_arquivo_completo_otimizado:75 | Tamanho do Parquet: 2.39 GB\n",
      "2025-06-17 09:21:37 | INFO     | __main__.SistemaPrincipal | 1338406383:processar_arquivo_completo_otimizado:86 | Arquivo carregado com 839 parti√ß√µes\n",
      "2025-06-17 09:21:37 | INFO     | __main__.SistemaPrincipal | 1338406383:processar_arquivo_completo_otimizado:90 | Arquivo j√° bem particionado, mantendo estrutura original\n",
      "2025-06-17 09:21:37 | INFO     | __main__.SistemaPrincipal | 1338406383:processar_arquivo_completo_otimizado:113 | ============================================================\n",
      "2025-06-17 09:21:37 | INFO     | __main__.SistemaPrincipal | 1338406383:processar_arquivo_completo_otimizado:114 | ETAPA 3/7: Processamento com Dask\n",
      "2025-06-17 09:21:37 | INFO     | __main__.SistemaPrincipal | 1338406383:processar_arquivo_completo_otimizado:115 | ============================================================\n",
      "2025-06-17 09:21:37 | INFO     | __main__.SistemaPrincipal | 1338406383:processar_arquivo_completo_otimizado:121 | Arquivo grande (2.4 GB) - usando processamento distribu√≠do\n",
      "2025-06-17 09:21:37 | INFO     | __main__.SistemaPrincipal | 1338406383:processar_arquivo_completo_otimizado:124 | Contando registros...\n",
      "2025-06-17 09:23:16 | INFO     | __main__.SistemaPrincipal | 1338406383:processar_arquivo_completo_otimizado:126 | Total estimado: ~41,930,217 registros\n",
      "2025-06-17 09:23:16 | INFO     | __main__.SistemaPrincipal | 1338406383:processar_arquivo_completo_otimizado:129 | ETAPA 3/7: Detectando anomalias (amostra)\n",
      "2025-06-17 09:23:16 | INFO     | __main__.SistemaPrincipal | 1338406383:processar_arquivo_completo_otimizado:132 | Processando POPULA√á√ÉO COMPLETA de 41,930,217 registros\n",
      "2025-06-17 09:23:16 | INFO     | __main__.SistemaPrincipal | 1338406383:processar_arquivo_completo_otimizado:133 | ‚ö†Ô∏è AVISO: Processamento da popula√ß√£o completa pode demorar v√°rias horas\n"
     ]
    }
   ],
   "source": [
    "# C√©lula 10.1: PROCESSAMENTO DOS ARQUIVOS DE PRODU√á√ÉO FAP (com Valida√ß√£o)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üöÄ PROCESSAMENTO DOS ARQUIVOS ESOCIAL FAP - PRODU√á√ÉO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Configurar caminhos dos arquivos\n",
    "arquivo_2023 = r\"D:\\FAP_INTELLIGENCE_V4\\Dados\\Esocial\\D.FAP.CAL.000.ESOCIAL.2023.2026.TXT\"\n",
    "arquivo_2024 = r\"D:\\FAP_INTELLIGENCE_V4\\Dados\\Esocial\\D.FAP.CAL.000.ESOCIAL.2024.2026.TXT\"\n",
    "\n",
    "# Lista de arquivos para processar\n",
    "arquivos_producao = [\n",
    "    (\"2023\", Path(arquivo_2023)),\n",
    "    (\"2024\", Path(arquivo_2024))\n",
    "]\n",
    "\n",
    "# Validar formato dos arquivos primeiro\n",
    "print(\"\\nüìã VALIDA√á√ÉO DO FORMATO DOS ARQUIVOS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "arquivos_validos = []\n",
    "for ano, arquivo_path in arquivos_producao:\n",
    "    if arquivo_path.exists():\n",
    "        print(f\"\\n‚úÖ Arquivo {ano} encontrado: {arquivo_path.stat().st_size/1e9:.2f} GB\")\n",
    "        \n",
    "        # Validar formato\n",
    "        valido, mensagem = ValidadorFormatoESocial.validar_arquivo(arquivo_path)\n",
    "        \n",
    "        if valido:\n",
    "            print(f\"   ‚úÖ Formato v√°lido: {mensagem}\")\n",
    "            arquivos_validos.append((ano, arquivo_path))\n",
    "        else:\n",
    "            print(f\"   ‚ùå Formato inv√°lido: {mensagem}\")\n",
    "            print(f\"   ‚ö†Ô∏è  O arquivo n√£o est√° no formato esperado (679 caracteres por linha)\")\n",
    "            print(f\"   üìù Verifique se o arquivo segue o layout do DM.204661 v1.9\")\n",
    "            \n",
    "            # Criar arquivo exemplo\n",
    "            exemplo_path = arquivo_path.parent / f\"exemplo_formato_correto_{ano}.txt\"\n",
    "            print(f\"   üí° Criando arquivo exemplo em: {exemplo_path}\")\n",
    "            criar_arquivo_esocial_exemplo(exemplo_path, num_registros=100)\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Arquivo {ano} N√ÉO encontrado: {arquivo_path}\")\n",
    "\n",
    "# Processar apenas arquivos v√°lidos\n",
    "if arquivos_validos:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä INICIANDO PROCESSAMENTO DOS ARQUIVOS V√ÅLIDOS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    resultados = {}\n",
    "    \n",
    "    for ano, arquivo_path in arquivos_validos:\n",
    "        print(f\"\\n[{ano}] Processando arquivo...\")\n",
    "        inicio_ano = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            resultado = sistema_principal_otimizado.processar_arquivo_completo_otimizado(\n",
    "                arquivo_path=arquivo_path,\n",
    "                usar_ml=True,\n",
    "                gerar_relatorios=True,\n",
    "                fazer_backup=True,\n",
    "                validar_s5011=False,\n",
    "                memoria_maxima_gb=None\n",
    "            )\n",
    "            \n",
    "            tempo_ano = (datetime.now() - inicio_ano).total_seconds() / 60\n",
    "            \n",
    "            if resultado['sucesso']:\n",
    "                print(f\"‚úÖ Arquivo {ano} processado em {tempo_ano:.1f} minutos\")\n",
    "                print(f\"   - Registros: {resultado['estatisticas']['registros_processados']:,}\")\n",
    "                print(f\"   - Anomalias: {resultado['estatisticas']['anomalias_detectadas']:,}\")\n",
    "                print(f\"   - Parquet: {resultado.get('arquivo_parquet', 'N/A')}\")\n",
    "                resultados[ano] = resultado\n",
    "            else:\n",
    "                print(f\"‚ùå Erro ao processar arquivo {ano}: {resultado.get('erro', 'Erro desconhecido')}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro fatal ao processar arquivo {ano}: {str(e)}\")\n",
    "    \n",
    "    # Resumo final\n",
    "    if resultados:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üéâ PROCESSAMENTO COMPLETO!\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        tempo_total = sum((r['tempo_processamento']/60 for r in resultados.values() if 'tempo_processamento' in r), 0)\n",
    "        registros_total = sum((r['estatisticas']['registros_processados'] for r in resultados.values()), 0)\n",
    "        anomalias_total = sum((r['estatisticas']['anomalias_detectadas'] for r in resultados.values()), 0)\n",
    "        \n",
    "        print(f\"‚è±Ô∏è  Tempo total: {tempo_total:.1f} minutos\")\n",
    "        print(f\"üìä Total de registros: {registros_total:,}\")\n",
    "        print(f\"‚ö†Ô∏è  Total de anomalias: {anomalias_total:,}\")\n",
    "        print(f\"üìÅ Relat√≥rios salvos em: ./relatorios/\")\n",
    "        print(f\"üíæ Arquivos Parquet salvos para processamento futuro mais r√°pido\")\n",
    "        print(\"=\"*80)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚ùå NENHUM ARQUIVO V√ÅLIDO PARA PROCESSAR\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Por favor, verifique:\")\n",
    "    print(\"1. Se os arquivos existem nos caminhos especificados\")\n",
    "    print(\"2. Se os arquivos est√£o no formato correto (679 caracteres por linha)\")\n",
    "    print(\"3. Se os arquivos seguem o layout do DM.204661 v1.9\")\n",
    "    print(\"\\nConsulte os arquivos de exemplo criados para refer√™ncia do formato correto.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
